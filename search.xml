<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2018/07/01/01.hello-world/</url>
    <content><![CDATA[<p>Welcome to my blog. </p>
<a id="more"></a> 
<p>以前的博客是搭在阿里云上的，现在打算迁移，毕竟为了写博客专门用一台服务器有点儿太奢侈了。</p>
<p>重新写博客最大的目的，就是重振生活状态。最近很忙，准确说是这一年都很忙，忙到没有时间思考。而现在，我想找回以前的自己。</p>
<p>既然搭了新的博客，我就会一直坚持写下去。</p>
<hr>
<p>以下是从原来的博客上粘贴过来的内容：</p>
<p>时间：2016-11-9</p>
<p>终于弄好了这个网站，以后会陆续放出一些技术性的干货，主要是专业方向以及自己感兴趣的方面的，同时我还会放出许多别的内容来让我的网站更加有趣。这个Blog会记录下我成长过程的一点一滴。</p>
<p>最开始，我是打算仅仅做一个技术博客的，但是后来（也是因为某些原因）我觉得它应该要有——吸引人的地方，与众不同才好，所以为什么不把它真正经营好，成为我的门户呢？</p>
<p>这样，才算真正对得起成长的一点一滴。</p>
<hr>
<p>在邓俊辉老师的《计算几何》这门课程中，曾讲到：</p>
<p>假设有一棵树，树上有一只羽翼尚未丰满的雏鸟，或许将来它会成为搏击苍穹的雄鹰，但现在它还不会飞。那怎么办呢？它勇敢的张开翅膀，一次又一次地向树下滑行，因此古人云，学习的习就是“数飞也”。</p>
<p>屡次地，不断地尝试，在这一过程中，可能会经历挫折，可能会经历失败，但最终将得到成长。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>旋转表示法</title>
    <url>/2018/07/12/03.%E6%97%8B%E8%BD%AC%E8%A1%A8%E7%A4%BA%E6%B3%95/</url>
    <content><![CDATA[<h1 id="一、向量和坐标系"><a href="#一、向量和坐标系" class="headerlink" title="一、向量和坐标系"></a>一、向量和坐标系</h1><p>要想确定一个坐标，首先要建立坐标系。在三维空间中建立的坐标系通常都是两两垂直的，坐标系也叫做线性空间中的一组基 $(e<em>{1},e</em>{2},e_{3})$ ，向量 $a$ 在这组基的坐标就可以表示成：    </p>
<script type="math/tex; mode=display">
a=[e_{1},e_{2},e_{3}]\left[\begin{matrix}a_{1} \\ a_{2} \\ a_3\end{matrix}\right]=a_{1}e_{1}+a_{2}e_{2}+a_{3}e_{3}</script><p>向量有内积和外积之分，<br>内积表示为 $a^Tb$ ，可以描述为两个向量之间的投影关系。<br>外积表示为 $ab^T$ ，外积大小是两个向量张成四边形的有向面积，之所以有向，是因为外积方向垂直于这两个向量构成的平面，且满足右手定理， $ab^T$ 和 $ba^T$ 会得到大小相同，方向相反的结果。</p>
<a id="more"></a> 
<h1 id="二、旋转向量"><a href="#二、旋转向量" class="headerlink" title="二、旋转向量"></a>二、旋转向量</h1><p>向量的外积是可以表示旋转的，假设物体从 $a$ 旋转到 $b$ 转过的角度为 $\theta$ ,那么旋转的方向和大小分别可以用有向面积的正负和大小来表示，它们的外积方向垂直于它们构成的平面（右手定律），长度等于张成四边形的面积。<br>于是，我们可以使用一个向量，其方向与旋转轴一致，长度等于旋转角来描述旋转，表示为 $\theta n$ （$\theta$为旋转角， $n$ 为旋转轴）。在三维空间中，只需要三个这样的向量，并且它们可以构成线性空间中的一组基，就可以描述三维空间中任意的刚体旋转。<br>根据3条旋转轴建立空间坐标，可以略去对转轴的描述，因此，通过一个空间中的三维向量就可以描述旋转，这就是旋转向量。<br>旋转向量通过3个参数控制了6个自由度的旋转。其表示非常紧凑。</p>
<h1 id="三、旋转矩阵"><a href="#三、旋转矩阵" class="headerlink" title="三、旋转矩阵"></a>三、旋转矩阵</h1><p>旋转向量描述的是物体原地做旋转，给人感觉并不关心坐标系，在使用旋转向量的时候，就默认自己是坐标系的中心了。<br>设某个单位正交基 $(e<em>{1},e</em>{2},e<em>{3})$ 经过一次旋转变成了 $(e</em>{1}’,e<em>{2}’,e</em>{3}’)$ ，那么对于同一个向量 $a$ (注意向量并没有跟随坐标系的旋转而发生运动)，它在两个坐标系下的坐标为 $[a<em>{1},a</em>{2},a<em>{3}]^T$ 和 $[a</em>{1}’,a<em>{2}’,a</em>{3}’]^T$ 。根据坐标定义，有：</p>
<script type="math/tex; mode=display">[e_{1},e_{2},e_{3}]\left[\begin{matrix}a_{1} \\ a_{2}\\ a_3\end{matrix}\right]=[e_{1}',e_{2}',e_{3}']\left[\begin{matrix}a_{1}' \\ a_{2}' \\ a_{3}'\end{matrix}\right]</script><p>可以得到 $a=Ra’$ ，这里的 矩阵$R$ 称为旋转矩阵。可以把旋转矩阵的集合定义为</p>
<script type="math/tex; mode=display">SO(n)=\{ {\boldsymbol{R}\in\mathbb{R}^{n\times n}|RR^T=I,det(R)=1}\}</script><p>$R^T$ 刻画了一个相反的旋转，这算一个重要性质吧，求逆非常方便。<br>$SO(3)$ 的旋转矩阵有9个量，但一次旋转只有3个自由度，这种表示法是冗余的。<br>旋转矩阵自身带有约束太多：必须是一个单位正交矩阵，且行列式为1。</p>
<h1 id="四、四元数"><a href="#四、四元数" class="headerlink" title="四、四元数"></a>四、四元数</h1><p>复数由一个实部和一个虚部构成，两个参数可以表示二维平面内所有的旋转，直觉上推广到三个参数就能表示三维旋转，这是错误的，必须4个量才可以表示。<br>在表达三维空间的旋转时，有一种类似复数的代数：四元数。它和复数一样有实部和虚部，表示为</p>
<script type="math/tex; mode=display">q=[s,v],s=q_{0}\in\mathbb{R}, v=[q_{1},q_{2},q_{3}]^T\in \mathbb{R}^3</script><p>考虑到三维空间需要3个轴，四元数也有3个虚部。三维空间中的点是通过虚四元数来标识的，此时四元数的实部没有任何作用。而在旋转的过程中，这个实部才会起作用，某种意义上来讲，这个四元数的实部不是三维空间的量。<br>更详细的四元数内容可以参考<span class="exturl" data-url="aHR0cHM6Ly93d3cuM2RnZXAuY29tL3VuZGVyc3RhbmRpbmctcXVhdGVybmlvbnMv">https://www.3dgep.com/understanding-quaternions/<i class="fa fa-external-link-alt"></i></span><br>这里关心的只是旋转，旋转向量、旋转矩阵、四元数的相互转化，</p>
<script type="math/tex; mode=display">q=[cos\frac{\theta}{2},\boldsymbol{n}sin\frac{\theta}{2}]</script><p>这里的 $n$ 表示旋转轴，和旋转向量的轴是一根轴，而这跟轴所表示的向量正是旋转矩阵的特征向量。<br>从旋转向量到旋转矩阵由著名的<code>Rodrigues公式</code>表示。</p>
<h1 id="五、李群李代数"><a href="#五、李群李代数" class="headerlink" title="五、李群李代数"></a>五、李群李代数</h1><p>$R$ 是旋转矩阵，设$\phi$ 为 $R$ 的导数，反应了 $R$ 的导数性质，故它在 $SO(3)$ 原点附近的正切空间上。同时在 $t<em>{0}$ 附近，设 $\phi(t</em>{0})=\phi_{0}$ ，所以有</p>
<script type="math/tex; mode=display">R(t)=\exp(\phi_{0}^{\wedge}t)
\Phi=\phi^\wedge=\left[\begin{matrix}0&-\phi_{3}&\phi_{2}\\ \phi_{3}&0&-\phi_{1}\\ -\phi_{2}&\phi_{1}&0\end{matrix}\right]\in\mathbb{R}^{3\times3} （反对称矩阵）</script><script type="math/tex; mode=display">\mathfrak{s}\mathfrak{o}(3)=\{ {\phi\in\mathbb{R}^3,\Phi=\phi^\wedge\in\mathbb{R}^{3\times3}}\}</script><p>李代数对应的反对称矩阵正是李群的导数，而李群李代数存在指数映射，这意味这求导很方便。<br>李群李代数的关系，正是旋转矩阵与旋转向量的关系。旋转向量其实就是李代数，而旋转矩阵构成的集合就是李群。指数映射即是Rodrigues公式。</p>
<p>任意矩阵的指数映射可以写成一个泰勒展开，但是只有在收敛的情况下才会有结果，其结果仍是一个矩阵。<br>$\exp(\phi^\wedge)=\sum_{n=0}^{\infty}\frac{1}{n!}(\phi^\wedge)^n$ 公式展开，可以证明<code>Rodrigues公式</code>。</p>
<p>$\mathfrak{s}\mathfrak{o}(3)$ 是由一个三维向量组成的集合，每一个向量对应到一个反对成矩阵，可以表达旋转矩阵的倒导数。每个 $SO(3)$ 中的元素都可以对应一个 $\mathfrak{s}\mathfrak{o}(3)$ 的元素，但是可能存在多个 $\mathfrak{s}\mathfrak{o}(3)$ 中的元素对应到同一个 SO(3) 中的元素。</p>
<h1 id="六、李代数求导与扰动模型"><a href="#六、李代数求导与扰动模型" class="headerlink" title="六、李代数求导与扰动模型"></a>六、李代数求导与扰动模型</h1><p>BCH公式（Baker-Campbell-Hausdorff）给出</p>
<script type="math/tex; mode=display">\ln{(\exp(\phi_{1}^\wedge)\exp((\phi_{2}^\wedge))^\vee}= \begin{equation} \left\{ \begin{array}{lc} J_{l} (\phi_{2})^{-1}\phi_{1}+\phi_{2} \\ J_{l} (\phi_{1})^{-1}\phi_{2}+\phi_{1} \end{array} \right. \end{equation}</script><p>左乘与右乘情况不同。<br>其中</p>
<script type="math/tex; mode=display">\boldsymbol{J}_{l}=\boldsymbol{J}=\frac{sin\theta}{\theta}\boldsymbol{I}+(\frac{1-sin\theta}{\theta})\boldsymbol{a}\boldsymbol{a}^T+\frac{1-cos\theta}{\theta}\boldsymbol{a}^\wedge.</script><p>它的逆</p>
<script type="math/tex; mode=display">\boldsymbol{J}^{-1}=\frac{\theta}{2}cot\frac{\theta}{2}\boldsymbol{I}+(1-\frac{\theta}{2}cot\frac{\theta}{2})\boldsymbol{a}\boldsymbol{a}^T-\frac{\theta}{2}\boldsymbol{a}^\wedge.</script><p>右乘雅可比仅需要取负号</p>
<script type="math/tex; mode=display">\boldsymbol{J}_{r}(\phi)=\boldsymbol{J}_{l}(-\phi).</script><p>假设对某一个旋转 $\boldsymbol{R}$ 对应的李代数是 $\phi$ ，给它左乘一个微小的旋转，记作 $ \Delta\boldsymbol{R}$ ，对应的李代数为 $\Delta\phi$ 。那么在李群上得到的结果为 $\Delta\boldsymbol{R}·\boldsymbol{R}$ ，而在李代数上根据BCH近似，为 $\boldsymbol{J}^{-1}(\phi)\Delta\phi+\phi$ ，合并起来可以写成：</p>
<script type="math/tex; mode=display">\exp(\Delta\phi)\exp(\phi)=\exp\left( \left( \phi+\boldsymbol{J}^{-1}\left( \phi \right)\Delta\phi \right)^\wedge \right)</script><p>李代数求导，因为太菜了，照着书没推下来。还是直接上结论吧。</p>
<script type="math/tex; mode=display">\frac{\partial(\boldsymbol{R}p)}{\partial\boldsymbol{R}}=\frac{\partial(\exp(\phi^\wedge)p)}{\partial\phi}=-(\boldsymbol{R}p)^\wedge\boldsymbol{J}_{l}</script><p>扰动模型，左乘</p>
<script type="math/tex; mode=display">\begin{equation} \begin{split} \frac{\partial(\boldsymbol{R}p)}{\partial\boldsymbol{R}}&=\lim\limits_{\varphi\rightarrow0}\frac{\exp(\varphi^\wedge)\exp(\phi^\wedge)\boldsymbol{p}-\exp(\phi^\wedge)\boldsymbol{p}}{\varphi}\\\\ &=\lim\limits_{\varphi\rightarrow0}\frac{(1+\varphi^\wedge)\exp(\phi^\wedge)\boldsymbol{p}-\exp(\phi^\wedge)\boldsymbol{p}}{\varphi}\\\\ &=\lim\limits_{\varphi\rightarrow0}\frac{\varphi^\wedge\boldsymbol{Rp}}{\varphi}=\lim\limits_{\varphi\rightarrow0}\frac{-(\boldsymbol{Rp})^\wedge\varphi}{\varphi}=-(\boldsymbol{Rp})^\wedge. \end{split} \end{equation}</script><p>写在最后：<br>以前学习都是先学习理论，后学习实践。现在顺序倒过来才发现数学作为工具存在的意义。看高博的书更像看综述，能很快建立知识的闭环，对系统有个了解，然后情不自禁地查更多资料，看更多书来填充知识的框架。<br>最后推荐一本好书《旋量代数与李群、李代数》，系统学习一波再来补充这篇文章。</p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
        <tag>机器人</tag>
      </tags>
  </entry>
  <entry>
    <title>配置我的linux</title>
    <url>/2018/07/02/02.%E9%85%8D%E7%BD%AE%E6%88%91%E7%9A%84linux/</url>
    <content><![CDATA[<p>这篇博客从我以前的博客搬运过来，备份系统可以使用Remastersys，网上的教程一搜一大把。为了省心，我一直坚持使用ubuntu16.04，以下是系统环境的配置。</p>
<a id="more"></a> 
<h2 id="一些基础的安装和依赖"><a href="#一些基础的安装和依赖" class="headerlink" title="一些基础的安装和依赖"></a>一些基础的安装和依赖</h2><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install </span><br><span class="line">sudo snap install mathpix-snipping-tool</span><br><span class="line">sudo apt-get install build-essential</span><br><span class="line">sudo apt-get install git cmake cmake-qt-gui </span><br><span class="line"></span><br><span class="line">git config --global user.name &quot;yourname&quot;</span><br><span class="line">git config --global user.email &quot;youremail@example.com&quot;</span><br><span class="line"></span><br><span class="line">ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</span><br></pre></td></tr></table></figure></h2><h2 id="安装Opencv：Install-opencv-in-linux-—-opencv-org"><a href="#安装Opencv：Install-opencv-in-linux-—-opencv-org" class="headerlink" title="安装Opencv：Install opencv in linux — opencv.org"></a>安装Opencv：<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLm9wZW5jdi5vcmcvbWFzdGVyL2Q3L2Q5Zi90dXRvcmlhbF9saW51eF9pbnN0YWxsLmh0bWw=">Install opencv in linux — opencv.org<i class="fa fa-external-link-alt"></i></span></h2><p>以下是从网站上摘录的依赖，可能会更新，以官网为准。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev</span><br><span class="line"></span><br><span class="line">sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev</span><br></pre></td></tr></table></figure><br>opencv的依赖中，很推荐<span class="exturl" data-url="aHR0cDovL2VpZ2VuLnR1eGZhbWlseS5vcmcvaW5kZXgucGhwP3RpdGxlPU1haW5fUGFnZQ==">Eigen3<i class="fa fa-external-link-alt"></i></span>，特别适合在自己的项目中使用。</p>
<p>如果安装了opencv的再安装ros,原来的opencv系统变量什么的就会被ros自带的opencv覆盖掉，ros自带的opencv会随着ros更新而更新，挺方便的，而且也需要用到ros。</p>
<p>与ubuntu16.04对应的是ROS-kinetic.</p>
<hr>
<h2 id="安装ROS-kinetic：Install-ros-in-linux-—-wiki-ros-org"><a href="#安装ROS-kinetic：Install-ros-in-linux-—-wiki-ros-org" class="headerlink" title="安装ROS-kinetic：Install ros in linux — wiki.ros.org"></a>安装ROS-kinetic：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9raW5ldGljL0luc3RhbGxhdGlvbi9VYnVudHU=">Install ros in linux — wiki.ros.org<i class="fa fa-external-link-alt"></i></span></h2><p>以下是从网站上摘录的依赖，可能会更新，以官网为准。</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">## 添加 sources.list</span><br><span class="line">sudo sh -c &#39;echo &quot;deb http:&#x2F;&#x2F;packages.ros.org&#x2F;ros&#x2F;ubuntu $(lsb_release -sc) main&quot; &gt; &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;ros-latest.list&#39;</span><br><span class="line"></span><br><span class="line">## 添加 ksys</span><br><span class="line">sudo apt-key adv --keyserver hkp:&#x2F;&#x2F;ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install ros-kinetic-desktop-full</span><br><span class="line">sudo rosdep init</span><br><span class="line">rosdep update</span><br><span class="line"></span><br><span class="line">## 环境变量配置</span><br><span class="line">echo &quot;source &#x2F;opt&#x2F;ros&#x2F;kinetic&#x2F;setup.bash&quot; &gt;&gt; ~&#x2F;.bashrc</span><br><span class="line">source ~&#x2F;.bashrc</span><br><span class="line"></span><br><span class="line">## 构建工厂依赖</span><br><span class="line">sudo apt-get install python-rosinstall python-rosinstall-generator python-wstool build-essential</span><br></pre></td></tr></table></figure></h2><h2 id="Nvidia-相关的安装与配置"><a href="#Nvidia-相关的安装与配置" class="headerlink" title="Nvidia 相关的安装与配置"></a>Nvidia 相关的安装与配置</h2><h3 id="安装驱动"><a href="#安装驱动" class="headerlink" title="安装驱动"></a>安装驱动</h3><p>首先是显卡驱动，可以在NVIDIA官网在下载相应的驱动并且安装，有可能会遇到无限重启的Bug，其实直接使用系统自带的方法就能很好安装驱动：</p>
<p>Ubuntu System Settings — Software &amp;&amp; update — Others 选中Nvidia，安装完成以后重启。</p>
<h2 id="-2"><a href="#-2" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br><span class="line">## 查看GPU列表</span><br><span class="line"></span><br><span class="line">nvidia-settings</span><br><span class="line">## 调出Nvidia驱动程序面板，可以选择显卡</span><br><span class="line">## 在System Settings — More detail 可以查看当前使用的是哪个显卡。</span><br></pre></td></tr></table></figure></h2><h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>其次是CUDA，我现在使用的是CUDA9.0</p>
<p>下载：<span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9jdWRhLWRvd25sb2Fkcw==">https://developer.nvidia.com/cuda-downloads<i class="fa fa-external-link-alt"></i></span></p>
<p>推荐选择runfile. 这样避免出现很多问题。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo sh cuda_x.x_xxx_xxx_.run</span><br></pre></td></tr></table></figure><br>安装过程中，会有一系列选项，其中有两个选项要注意一下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">是否安装驱动（Y&#x2F;N）？N</span><br><span class="line">是否创建软链接（Y&#x2F;N）？N    </span><br></pre></td></tr></table></figure><br>软链接那里，会创建/usr/local/cuda/的链接，其实就是原本安装位置和文件的快捷方式，选择No，反而会方便很多，cuda原本的安装位置是有版本号的，比如cuda-9.0，如果后面的配置都是声明/usr/local/cuda/而不是/usr/local/cuda-x.0/的话，最后配置在了快捷方式上，而没有配置到原本的安装位置上，以后就会很多**问题。</p>
<p>其他选项的根据情况自己做选择。一般都是Yes。</p>
<hr>
<p>配置cuda之后要加上的环境变量声明，在文件~/.bashrc之后加上<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gedit ~&#x2F;.bashrc</span><br><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-x.0&#x2F;bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-x.0&#x2F;lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><br>然后设置环境变量和动态链接库，在命令行输入</p>
<p>sudo gedit /etc/profile<br>在打开的文件里面加上（注意等号两边不能有空格）<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-x.0&#x2F;bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda-x.0&#x2F;lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><br>保存之后，创建链接文件<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit &#x2F;etc&#x2F;ld.so.conf.d&#x2F;cuda.conf</span><br></pre></td></tr></table></figure><br>在打开的文件中添加如下语句：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64</span><br></pre></td></tr></table></figure><br>保存退出执行命令行：</p>
<h2 id="-3"><a href="#-3" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure></h2><p>网上的一些博客说需要编译器的升降级别，其实是不用的。关于编译器升降级我放到Blog的最后。</p>
<p>测试cuda的Samples<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;local&#x2F;cuda-8.0&#x2F;samples&#x2F;1_Utilities&#x2F;deviceQuery</span><br><span class="line">make</span><br><span class="line">sudo .&#x2F;deviceQuery</span><br><span class="line">....</span><br><span class="line">....</span><br><span class="line">....</span><br><span class="line">... &#x3D; PASS    ## 成功</span><br><span class="line">... &#x3D; FAILED  ## 失败</span><br></pre></td></tr></table></figure><br>如果显示的是一些关于GPU的信息，则说明安装成功了。</p>
<hr>
<h3 id="CuDnn"><a href="#CuDnn" class="headerlink" title="CuDnn"></a>CuDnn</h3><p>cudnn的官网下载好像瞎了，但是在网络资源丰富的今天不难找到，什么百度云啊等等。（这里我安装的是cudnn7.0）</p>
<p>cudnn不需要install，将cuDnn里面的include/ 和 lib64/ 下面所有的头文件和库文件拷贝到 /usr/local/cuda-x.0/ 下面即可。</p>
<p>首先是把cudnn/include目录下的cudnn.h头文件扔到cuda-x.0/include/里<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp cudnn.h &#x2F;usr&#x2F;local&#x2F;cuda-x.0&#x2F;include&#x2F; </span><br></pre></td></tr></table></figure><br>再把cudnn/lib64/目录下的所有的lib开头的动态文件扔到/usr/local/cuda-x.0/lib64/里<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo cp lib* &#x2F;usr&#x2F;local&#x2F;cuda-x.0&#x2F;lib64&#x2F;    </span><br><span class="line">#复制动态链接库</span><br></pre></td></tr></table></figure><br>在lib64/文件夹下，libcudnn.so 和 libcudnn.so.x 都是链接</p>
<p>经过复制以后，原有的链接就已经失效了，即快捷方式，因此就需要重新生成新的快捷方式</p>
<h2 id="-4"><a href="#-4" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;local&#x2F;cuda-x.0&#x2F;lib64&#x2F;</span><br><span class="line">sudo rm -rf libcudnn.so libcudnn.so.7       #删除原有动态文件</span><br><span class="line">sudo ln -s libcudnn.so.7.0.5 libcudnn.so.7  #生成软链接</span><br><span class="line">sudo ln -s libcudnn.so.7 libcudnn.so        #生成软链接</span><br></pre></td></tr></table></figure></h2><h2 id="安装Caffe"><a href="#安装Caffe" class="headerlink" title="安装Caffe"></a>安装Caffe</h2><p>按照官网的流程走，不会有大问题。官网地址：<span class="exturl" data-url="aHR0cDovL2NhZmZlLmJlcmtlbGV5dmlzaW9uLm9yZy9pbnN0YWxsYXRpb24uaHRtbA==">Caffe installation<i class="fa fa-external-link-alt"></i></span></p>
<p>配置universe仓库然后更新<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository universe  </span><br><span class="line">sudo apt-get update -y  </span><br></pre></td></tr></table></figure></p>
<h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><p>通用的依赖<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev \  </span><br><span class="line">libhdf5-serial-dev protobuf-compiler -y  </span><br><span class="line">sudo apt-get install --no-install-recommends libboost-all-dev -y  </span><br></pre></td></tr></table></figure><br>其中，<span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXJzLmdvb2dsZS5jb20vcHJvdG9jb2wtYnVmZmVycy8=">protobuf<i class="fa fa-external-link-alt"></i></span>我很推荐，建议自己的项目里积极使用。</p>
<hr>
<p>BLAS是Basic Linear Algebra Subprograms（基础线性代数程序集）。它还被用于创建更大的数值程序包（如LAPACK）。在高性能计算领域，BLAS被广泛使用。Caffe也需要BLAS，CUDA有自己的cuBALS。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libatlas-base-dev -y  </span><br></pre></td></tr></table></figure><br>以下是建议安装的依赖:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev -y  </span><br><span class="line">sudo apt-get install python-dev python-numpy –y  </span><br><span class="line">sudo apt-get install -y python-pip  </span><br><span class="line">sudo apt-get install -y python-dev  </span><br><span class="line">sudo apt-get install -y python-numpy python-scipy </span><br></pre></td></tr></table></figure><br>以上都推荐在自己的项目中积极使用。</p>
<hr>
<h3 id="配置编译"><a href="#配置编译" class="headerlink" title="配置编译"></a>配置编译</h3><p>下面开始源码编译 Caffe，目录下有一份config.example需要配置，把它复制并重命名一份出来，然后修改它就可以配置Caffe的编译选项。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;BVLC&#x2F;caffe.git  </span><br><span class="line">cd caffe  </span><br><span class="line">cp Makefile.config.example Makefile.config </span><br></pre></td></tr></table></figure><br>Makefile.config这个配置文件还是挺重要的，编译过不过，就看它了。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cuDNN acceleration switch (uncomment to build with cuDNN).</span><br><span class="line">USE_CUDNN :&#x3D; 1</span><br><span class="line"> </span><br><span class="line"># Uncomment if you&#39;re using OpenCV 3 如果用的是opencv3版本</span><br><span class="line">OPENCV_VERSION :&#x3D; 3 </span><br></pre></td></tr></table></figure><br>这里的OPENCV_VERSION其实可以不配，在用ros的opencv3的时候，勾选了这个会报很多错误。</p>
<p>另外ubuntu16.04的文件包含位置发生了变化，尤其是需要用到的hdf5的位置，所以需要更改这一路径<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INCLUDE_DIRS :&#x3D; $(PYTHON_INCLUDE) &#x2F;usr&#x2F;local&#x2F;include &#x2F;usr&#x2F;include&#x2F;hdf5&#x2F;serial </span><br><span class="line">LIBRARY_DIRS :&#x3D; $(PYTHON_LIB) &#x2F;usr&#x2F;local&#x2F;lib &#x2F;usr&#x2F;lib &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;hdf5&#x2F;serial</span><br></pre></td></tr></table></figure><br>打开makefile文件，将<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NVCCFLAGS +&#x3D;-ccbin&#x3D;$(CXX) -Xcompiler-fPIC $(COMMON_FLAGS)</span><br></pre></td></tr></table></figure><br>替换为<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NVCCFLAGS +&#x3D; -D_FORCE_INLINES -ccbin&#x3D;$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)</span><br></pre></td></tr></table></figure><br>编辑/usr/local/cuda-x.0/include/host_config.h（这个看你之前cuda是怎么配置的了）</p>
<p>把这一行注释掉：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#error-- unsupported GNU version! gcc versions later than 5 are not supported!</span><br></pre></td></tr></table></figure></p>
<h3 id="编译Caffe"><a href="#编译Caffe" class="headerlink" title="编译Caffe"></a>编译Caffe</h3><p>完成以上所有工作，就可以编译Caffe了<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make all -j8</span><br><span class="line">make runtest</span><br><span class="line">make pycaffe</span><br></pre></td></tr></table></figure><br>中间出任何问题，及时google。这块内容主要参考自这三个博客：</p>
<p><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDE2NzI2OS9hcnRpY2xlL2RldGFpbHMvNTA3MDM5MjM=">http://blog.csdn.net/u010167269/article/details/50703923<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGl0MjAxNXNwcmluZy9hcnRpY2xlL2RldGFpbHMvNTM1MTA5MDk=">http://blog.csdn.net/hit2015spring/article/details/53510909<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXV0b2N5ei9hcnRpY2xlL2RldGFpbHMvNTIyOTk4ODk=">http://blog.csdn.net/autocyz/article/details/52299889<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h2 id="Anaconda多版本配置"><a href="#Anaconda多版本配置" class="headerlink" title="Anaconda多版本配置"></a>Anaconda多版本配置</h2><p>Anaconda是python科学计算包，因为py2/3有区别，所有Anaconda也去分2和3，这里Ubuntu安装Anaconda版本共存，默认py2，需要py3环境时，用下面命令切换。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source activate py3</span><br></pre></td></tr></table></figure><br>系统配置Anaconda后，python就换成Anaconda的环境，和ros会有很多兼容问题，caktin_make编译无法通过。我暂时还没有解决。</p>
<hr>
<p>需要提前去网站下载 Anaconda的安装包，推荐在国内的镜像源下载，官网的下载实在是太慢了。</p>
<p>安装 Anaconda2就一行命令<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash Anaconda2-4.3.1-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><br>安装 Anaconda3不需要create新环境，直接运行以下代码，其中py3是新的环境名：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash Anaconda3-4.3.0-Linux-x86_64.sh -b -p $HOME&#x2F;anaconda2&#x2F;envs&#x2F;py3</span><br><span class="line">rm -f $HOME&#x2F;anaconda2&#x2F;envs&#x2F;py3&#x2F;bin&#x2F;conda*</span><br><span class="line">rm -f $HOME&#x2F;anaconda2&#x2F;envs&#x2F;py3&#x2F;conda-meta&#x2F;conda-*</span><br><span class="line">rm -f $HOME&#x2F;anaconda2&#x2F;envs&#x2F;py3&#x2F;bin&#x2F;activate</span><br><span class="line">rm -f $HOME&#x2F;anaconda2&#x2F;envs&#x2F;py3&#x2F;bin&#x2F;deactivate</span><br><span class="line">cd $HOME&#x2F;anaconda2&#x2F;envs&#x2F;py3&#x2F;bin</span><br><span class="line">ln -s ..&#x2F;..&#x2F;..&#x2F;bin&#x2F;conda .</span><br><span class="line">ln -s ..&#x2F;..&#x2F;..&#x2F;bin&#x2F;activate .</span><br><span class="line">ln -s ..&#x2F;..&#x2F;..&#x2F;bin&#x2F;deactivate .</span><br></pre></td></tr></table></figure><br>打开一个新终端，输入下面命令来查看它：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure><br>如果提示错误，则说明没有配置好，需要进行下面的步骤：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo gedit ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><br>打开文件后在末尾输入<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export PATH&#x3D;&quot;&#x2F;home&#x2F;yourname&#x2F;anaconda2&#x2F;bin:$PATH&quot;</span><br></pre></td></tr></table></figure><br>此处anaconda2的路径根据你自己的做相应的修改即可。</p>
<hr>
<p>由于墙的问题，需要设置国内的anaconda源<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br></pre></td></tr></table></figure><br>TUNA的help中镜像地址加有引号，需要去掉。</p>
<p>设置搜索时显示通道地址<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure><br>执行完上述命令后，会生成~/.condarc文件，记录着对conda的配置，直接手动创建、编辑该文件是相同的效果。</p>
<hr>
<h2 id="系统的主题"><a href="#系统的主题" class="headerlink" title="系统的主题"></a>系统的主题</h2><p>安装 Unity Tweak Tool<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install unity-tweak-tool</span><br></pre></td></tr></table></figure><br>我个人推荐Flatabulous这个主题</p>
<p>下载：<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FubW9samFnZXRpYS9GbGF0YWJ1bG91cy9yZWxlYXNlcw==">https://github.com/anmoljagetia/Flatabulous/releases<i class="fa fa-external-link-alt"></i></span></p>
<p>系统Icon我很喜欢这个风格，淡淡的蓝色<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:noobslab&#x2F;icons</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install ultra-flat-icons</span><br></pre></td></tr></table></figure><br>最后是ununtu安装shadowsocks-qt5，通过PPA源安装，如果需要的话。</p>
<h2 id="-5"><a href="#-5" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:hzwhuang&#x2F;ss-qt5</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install shadowsocks-qt5</span><br></pre></td></tr></table></figure></h2><h2 id="Linux串口通信"><a href="#Linux串口通信" class="headerlink" title="Linux串口通信"></a>Linux串口通信</h2><p>做机器人需要用到Linux的串口，但是16.04普通用户会没有权限访问ttyS设备<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ls -l &#x2F;dev&#x2F;ttyS0</span><br><span class="line">crw-rw---- 1 root dialout 4, 64  1月21 21:53 &#x2F;dev&#x2F;ttyS0</span><br></pre></td></tr></table></figure><br>ttyS设备的用户主是root，而所属的组是dialout，并且owner和group都是有相同的rw权限的，但others是没有任何权限的。</p>
<p>这个可以通过用户组设置的来解决，输入<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo usermod -a -G dialout user_name</span><br></pre></td></tr></table></figure><br>这样，重启系统后，把“user_name”就会加入dialout组了，之后就能自由访问ttyS设备了。</p>
<hr>
<h2 id="安装QT"><a href="#安装QT" class="headerlink" title="安装QT"></a>安装QT</h2><p>一种方法，直接一行命令搞定<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install qt4-dev-tools qt4-doc qt4-qtconfig qt4-demos qt4-designer qt5-default qtcreator -y</span><br></pre></td></tr></table></figure><br>另一种方法，安装.run包来安装，使用qt只是偶尔需要用到图形化的东西。很少做开发，只装一个qt4就够了。</p>
<hr>
<h2 id="编译器升降级、LLVM、Clang"><a href="#编译器升降级、LLVM、Clang" class="headerlink" title="编译器升降级、LLVM、Clang"></a>编译器升降级、LLVM、Clang</h2><h3 id="关于gcc-g"><a href="#关于gcc-g" class="headerlink" title="关于gcc/g++"></a>关于gcc/g++</h3><p>之前在安装CUDA的时候，有提到CUDA不支持gcc5以上编译器的问题，属于历史遗留。抛开CUDA，有的时候需要编译器升降级，以下是方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install gcc -4.9 gcc-5 g++-4.9 g++-5</span><br><span class="line"></span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;gcc gcc &#x2F;usr&#x2F;bin&#x2F;gcc-4.9 20</span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;gcc gcc &#x2F;usr&#x2F;bin&#x2F;gcc-5 10</span><br><span class="line"></span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;g++ g++ &#x2F;usr&#x2F;bin&#x2F;g++-4.9 20</span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;g++ g++ &#x2F;usr&#x2F;bin&#x2F;g++-5 10</span><br><span class="line"></span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;cc cc &#x2F;usr&#x2F;bin&#x2F;gcc 30</span><br><span class="line">sudo update-alternatives --set cc &#x2F;usr&#x2F;bin&#x2F;gcc</span><br><span class="line"></span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;c++ c++ &#x2F;usr&#x2F;bin&#x2F;g++ 30</span><br><span class="line">sudo update-alternatives --set c++ &#x2F;usr&#x2F;bin&#x2F;g++</span><br></pre></td></tr></table></figure><br>这一步好像不是必须的，但是如此配置以后，就可以自由切换gcc/g++的编译器版本，感觉非常方便。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --config gcc  ## 选择gcc编译器版本</span><br><span class="line">sudo update-alternatives --config g++  ## 选择g++编译器版本</span><br></pre></td></tr></table></figure></p>
<h3 id="关于LLVM-Clang"><a href="#关于LLVM-Clang" class="headerlink" title="关于LLVM / Clang"></a>关于LLVM / Clang</h3><p>最近转了LLVM+Clang编译C++，体验比以前上升不少。目前在使用llvm-4.0和clang-5.0，这是源上就有的版本，没有更新的版本需求，就不要给自己找源码编译的坑，能二进制安装就二进制。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get install llvm</span><br><span class="line">sudo apt-get install clang</span><br><span class="line">sudo apt-get install clang-5.0</span><br></pre></td></tr></table></figure><br>这里的配置方法和上面几乎一样。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;clang++ clang++ &#x2F;usr&#x2F;bin&#x2F;clang++-3.8 100</span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;clang++ clang++ &#x2F;usr&#x2F;bin&#x2F;clang++-5.0 1000</span><br><span class="line"></span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;clang clang &#x2F;usr&#x2F;bin&#x2F;clang-3.8 100</span><br><span class="line">sudo update-alternatives --install &#x2F;usr&#x2F;bin&#x2F;clang clang &#x2F;usr&#x2F;bin&#x2F;clang-5.0 1000</span><br></pre></td></tr></table></figure><br>如此配置，能自由切换clang/clang++的编译器版本。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --config clang</span><br><span class="line">sudo update-alternatives --config clang++</span><br></pre></td></tr></table></figure><br>关于C/C++编译器，可以选择。</p>
<h2 id="-6"><a href="#-6" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --config cc</span><br><span class="line">sudo update-alternatives --config c++</span><br></pre></td></tr></table></figure></h2><h2 id="google工具链"><a href="#google工具链" class="headerlink" title="google工具链"></a>google工具链</h2><h3 id="通过apt安装Ceres-Solver的依赖"><a href="#通过apt安装Ceres-Solver的依赖" class="headerlink" title="通过apt安装Ceres-Solver的依赖"></a>通过apt安装Ceres-Solver的依赖</h3><p>这部分已经包含了命令行解析gflags、日志系统glog<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># CMake</span><br><span class="line">sudo apt-get install cmake</span><br><span class="line"># google-glog + gflags</span><br><span class="line">sudo apt-get install libgoogle-glog-dev</span><br><span class="line"># BLAS &amp; LAPACK</span><br><span class="line">sudo apt-get install libatlas-base-dev</span><br><span class="line"># Eigen3</span><br><span class="line">sudo apt-get install libeigen3-dev</span><br><span class="line"># SuiteSparse and CXSparse (optional)</span><br><span class="line"># - If you want to build Ceres as a *static* library (the default)</span><br><span class="line">#   you can use the SuiteSparse package in the main Ubuntu package</span><br><span class="line">#   repository:</span><br><span class="line">sudo apt-get install libsuitesparse-dev</span><br><span class="line"># - However, if you want to build Ceres as a *shared* library, you must</span><br><span class="line">#   add the following PPA:</span><br><span class="line">sudo add-apt-repository ppa:bzindovic&#x2F;suitesparse-bugfix-1319687</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install libsuitesparse-dev</span><br></pre></td></tr></table></figure></p>
<h3 id="命令行解析gflags、日志系统glog、代码测试gtest、优化Ceres-Solver"><a href="#命令行解析gflags、日志系统glog、代码测试gtest、优化Ceres-Solver" class="headerlink" title="命令行解析gflags、日志系统glog、代码测试gtest、优化Ceres-Solver"></a>命令行解析gflags、日志系统glog、代码测试gtest、优化Ceres-Solver</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;gflags&#x2F;gflags.git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;glog.git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;googletest.git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;ceres-solver&#x2F;ceres-solver</span><br><span class="line">cd &lt;source-dir&gt;</span><br><span class="line">mkdir &lt;build-dir&gt;</span><br><span class="line">cd &lt;build-dir&gt;</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>参考：<br><span class="exturl" data-url="aHR0cDovL2NlcmVzLXNvbHZlci5vcmcvaW5zdGFsbGF0aW9uLmh0bWwjb3B0aW9ucy1jb250cm9sbGluZy1jZXJlcy1jb25maWd1cmF0aW9u">http://ceres-solver.org/installation.html#options-controlling-ceres-configuration<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3NlbmxpbnpoYW4uZ2l0aHViLmlvLzIwMTcvMTAvMDcvZ2xvZy8=">http://senlinzhan.github.io/2017/10/07/glog/<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title>图像的矩</title>
    <url>/2018/08/02/08.moments/</url>
    <content><![CDATA[<h1 id="数学中的矩"><a href="#数学中的矩" class="headerlink" title="数学中的矩"></a>数学中的矩</h1><p>矩的本质是数学期望，而期望的计算公式是</p>
<script type="math/tex; mode=display">E(x)=\int_{-\infty}^{+\infty} xf(x)dx</script><p>其中f(x)是x的概率密度，上面的公式默认了所有变量概率相等。<br><a id="more"></a><br>[定义]：设x，y是离散型随机变量，c为常数，k为正整数，如果E(|x-c|^2)存在，则称其为x关于点c的k阶矩。<br>c=0时，称为k阶原点矩<br>c=E(x)时，成为k阶中心距<br>如果E(|x-c1|^p·|x-c2|^q)存在，则称其为x，y关于点c的p+q阶矩<br>c1=c2=0时，称为p+q阶混合中心矩<br>c1=E(x), c2=E(y)时，称为p+q阶混合中心矩<br>如果x, y是连续型的，则下式称其为x，y关于点c的p+q阶矩</p>
<script type="math/tex; mode=display">\iint\limits_{p+y} {( x-x_{0})^p\cdot (y-y_{0})^q}dxdy</script><h1 id="图像的矩"><a href="#图像的矩" class="headerlink" title="图像的矩"></a>图像的矩</h1><p>一般的图象距，指的是原点矩。一阶矩和零阶矩就可以计算某个形状的重心，而二阶矩就可以拿来计算形状和方向。<br>(1)普通矩</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>0阶矩</th>
<th>目标区域的质量（轮廓的面积）</th>
</tr>
</thead>
<tbody>
<tr>
<td>1阶矩</td>
<td>目标区域的质心（轮廓的几何中心）</td>
</tr>
<tr>
<td>2阶矩</td>
<td>表示旋转半径</td>
</tr>
<tr>
<td>3阶矩</td>
<td>描述目标的方位、斜度、和扭曲程度</td>
</tr>
</tbody>
</table>
</div>
<p>(2)中心矩：构造平移不变性</p>
<script type="math/tex; mode=display">\bar{x}=\frac{m_{10}}{m_{00}}，\bar{y}=\frac{m_{01}}{m_{00}}</script><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Get Moments for all Contours and the mass centers</span></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;Moments&gt; <span class="title">mu</span><span class="params">(contours.size())</span></span>;</span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;Point2f&gt; <span class="title">mc</span><span class="params">(contours.size())</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>( <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; contours.size(); i++ )</span><br><span class="line">&#123;	mu[i] = moments( contours[i], <span class="literal">false</span> ); </span><br><span class="line">	mc[i] = Point2f( mu[i].m10/mu[i].m00 , mu[i].m01/mu[i].m00 );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>(3)归一化中心距：构造尺度不变性</p>
<script type="math/tex; mode=display">y_{py}=\frac{\mu_{pq}}{\mu_{00}}，r=\frac{p+q+2}{2}，p+q=2,3,...</script><p>(4)Hu矩：构造M1到M7来描述旋转不变性<br>待续。。</p>
<p>参考<br><span class="exturl" data-url="aHR0cDovL3d3dy5jbmJsb2dzLmNvbS9yb25ueS9wLzM5ODU4MTAuaHRtbA==">http://www.cnblogs.com/ronny/p/3985810.html<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3d3dy50azQ0NzkubmV0L0F1Z3VzZGkvYXJ0aWNsZS9kZXRhaWxzLzkwMDA4Mjk=">http://www.tk4479.net/Augusdi/article/details/9000829<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZmVuZ2JpbmdjaHVuL2FydGljbGUvZGV0YWlscy82MTk5NTYz">http://blog.csdn.net/fengbingchun/article/details/6199563<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>基于图的图像分割</title>
    <url>/2018/08/07/10.Graph-Based-Image-Segmentation/</url>
    <content><![CDATA[<p>Graph-Based Segmentation 是经典的图像分割算法。论文：<span class="exturl" data-url="aHR0cDovL2NzLmJyb3duLmVkdS9wZW9wbGUvcGZlbHplbnMvcGFwZXJzL3NlZy1pamN2LnBkZg==">Efficient Graph-Based Image Segmentation，IJCV 2004，MIT<i class="fa fa-external-link-alt"></i></span>，作者 Pedro F. Felzenszwalb.</p>
<a id="more"></a> 
<h1 id="图"><a href="#图" class="headerlink" title="图"></a>图</h1><p>图是由顶点集$V$（vertices）和边集$E$（edges）组成，表示为$G=(V,E)$，顶点$v \in V$，在本文中即为单个的像素点，连接一对顶点的边$(v_i,v_j)\in E$具有权重$w(v_i,v_j)$，本文中的意义为顶点之间的不相似度，所用的是无向图。</p>
<p><img src="/img/Graph-Based/g.png" alt="">树：特殊的图，图中任意两个顶点，都有路径相连接，但是没有回路。如上图中加粗的边所连接而成的图。如果看成一团乱连的珠子，只保留树中的珠子和连线，那么随便选个珠子，都能把这棵树中所有的珠子都提起来。如果，i和h这条边也保留下来，那么顶点h,i,c,f,g就构成了一个回路。</p>
<p>最小生成树（MST, minimum spanning tree）：特殊的树，给定需要连接的顶点，选择边权之和最小的树。上图即是一棵MST<br><img src="/img/Graph-Based/t.png" alt="">本文中，初始化时每一个像素点都是一个顶点，然后逐渐合并得到一个区域，确切地说是连接这个区域中的像素点的一个MST。如图，棕色圆圈为顶点，线段为边，合并棕色顶点所生成的MST，对应的就是一个分割区域。分割后的结果其实就是森林。</p>
<h1 id="相似性"><a href="#相似性" class="headerlink" title="相似性"></a>相似性</h1><p>既然是聚类算法，那应该依据何种规则判定何时该合二为一，何时该继续划清界限呢？<br>对于孤立的两个像素点，所不同的是颜色，自然就用颜色的距离来衡量两点的相似性，本文中是使用RGB的距离，即</p>
<script type="math/tex; mode=display">\sqrt{(r_1-r_2)^2+(g_1-g_2)^2+(b_1-b_2)^2}</script><p>当然也可以用perceptually uniform的Luv或者Lab色彩空间，对于灰度图像就只能使用亮度值了，此外，还可以先使用纹理特征滤波，再计算距离，比如，先做Census Transform再计算Hamming distance距离。</p>
<h1 id="自适应阈值"><a href="#自适应阈值" class="headerlink" title="自适应阈值"></a>自适应阈值</h1><p>上面提到应该用亮度值之差来衡量两个像素点之间的差异性。对于两个区域（子图）或者一个区域和一个像素点的相似性，最简单的方法即只考虑连接二者的边的不相似度。</p>
<p><img src="/img/Graph-Based/t2.png" alt="">已经形成了棕色和绿色两个区域，现在通过紫色边来判断这两个区域是否合并。那么我们就可以设定一个阈值，当两个像素之间的差异（即不相似度）小于该值时，合二为一。迭代合并，最终就会合并成一个个区域，效果类似于区域生长.</p>
<p><img src="/img/Graph-Based/pic1.png" alt="">显然，对上面这张图如果设置全局阈值并不合适，那么自然就得用自适应阈值。对于p区该阈值要特别小，s区稍大，h区巨大。</p>
<blockquote>
<p>In this section we define a predicate, D, for evaluating whether or not there is evidence for a boundary between two components in a segmentation (two regions of an image).This predicate is based on measuring the dissimilarity between elements along the boundary of the two components relative to a measure of the dissimilarity among neighboring elements within each of the two components. The resulting predicate compares the inter-component differences to the within component differences and is thereby adaptive with respect to the local characteristics of the data.<br>We define the internal difference of a component C ⊆ V to be the largest weight in the minimum spanning tree of the component, MST(C, E). That is,</p>
<script type="math/tex; mode=display">Int(C)=\max\limits_{e\in MST(C,E)}w(e)</script><p>One intuition underlying this measure is that a given component C only remains connected when edges of weight at least Int(C) are considered.<br>We define the difference between two components C1, C2 ⊆ V to be the minimum weight edge connecting the two components. That is,</p>
<script type="math/tex; mode=display">Diff(C_1,C_2)=\min\limits_{v_i\in C_1,v_j\in C_2,(v_i,v_j)\in E}w((v_i,v_j))</script><p>If there is no edge connecting C1 and C2 we let Dif(C1, C2) = ∞. This measure of difference could in principle be problematic, because it reflects only the smallest edge weight between two components. In practice we have found that the measure works quite well in spite of this apparent limitation. Moreover, changing the definition to use the median weight, or some other quantile, in order to make it more robust to outliers, makes the problem of finding a good segmentation NP-hard, as discussed in the Appendix. Thus a small change to the segmentation criterion vastly changes the difficulty of the problem.</p>
<p>The region comparison predicate evaluates if there is evidence for a boundary between a pair or components by checking if the difference between the components, Dif(C1, C2), is large relative to the internal difference within at least one of the components, Int(C1) and Int(C2). A threshold function is used to control the degree to which the difference between components must be larger than minimum internal difference. We define the pairwise comparison predicate as, </p>
<script type="math/tex; mode=display">D(C_1,C_2)=
\begin{equation}
\left\{
\begin{array}{lr}
true，diff(C_1,C_2)>MInt(C_1,C_2)
\\
flase，otherwise
\end{array}
\right.
\end{equation}</script><p>where the minimum internal difference, MInt, is defined as,</p>
<script type="math/tex; mode=display">MInt(C_1,C_2)=min(Int(C_1)+\tau(C_1),Int(C_2)+\tau(C_2))</script><p>The threshold function τ controls the degree to which the difference between two components must be greater than their internal differences in order for there to be evidence of a boundary between them (D to be true). For small components, Int(C) is not a good estimate of the local characteristics of the data. In the extreme case, when |C| = 1, Int(C) = 0. Therefore, we use a threshold function based on the size of the component, </p>
<script type="math/tex; mode=display">\tau (C)=k/|C|</script><p>where |C| denotes the size of C, and k is some constant parameter. That is, for small components we require stronger evidence for a boundary. In practice k sets a scale of observation, in that a larger k causes a preference for larger components. Note, however, that k is not a minimum component size. Smaller components are allowed when there is a sufficiently large difference between neighboring components.</p>
<p>Any non-negative function of a single component can be used for τ without changing the algorithmic results in Section 4. For instance, it is possible to have the segmentation method prefer components of certain shapes, by defining a τ which is large for components that do not fit some desired shape and small for ones that do. This would cause the segmentation algorithm to aggressively merge components that are not of the desired shape. Such a shape preference could be as weak as preferring components that are not long and thin (e.g., using a ratio of perimeter to area) or as strong as preferring components that match a particular shape model. Note that the result of this would not solely be components of the desired shape, however for any two neighboring components one of them would be of the desired shape. </p>
</blockquote>
<h1 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h1><p>Step 1: 计算每一个像素点与其8邻域或4邻域的不相似度。</p>
<p><img src="/img/Graph-Based/pic3.png" alt="">Step 2: 将边按照不相似度non-decreasing排列（从小到大）排序得到$e_1,e_2,……,e_N$。<br>Step 3: 选择$e_1$<br>Step 4: 对当前选择的边$e_n$进行合并判断。设其所连接的顶点为$(v_i,v_j)$。如果满足合并条件：</p>
<ol>
<li>$v_i,v_j$不属于同一个区域$Id(V_i)\ne Id(v_j)$</li>
<li>不相似度不大于二者内部的不相似度。$w_{i,j}\leq Mint(C_i,C_j)$则执行Step 5，否则执行Step 6。</li>
</ol>
<p>Step 5: 更新阈值以及类标号。<br>更新类标号：将$Id(v<em>i),Id(v_j)$的类标号统一为$Id(v_i)$的标号。<br>更新该类的不相似度阈值为：$w</em>{i,j}+\frac{k}{|c<em>i|+|c_j|}$。<br>注意：由于不相似度小的边先合并，所以，$w</em>{i,j}$即为当前合并后的区域的最大的边，即$Int(C<em>i\cup C_j)=w</em>{i,j}$。<br>Step 6: 如果$n\leq N$，则按照排好的顺序，选择下一条边转到Step 4，否则结束。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3R0cmFuc3Bvc2l0aW9uL2FydGljbGUvZGV0YWlscy8zODAyNDU1Nw==">https://blog.csdn.net/ttransposition/article/details/38024557<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>Opencv</tag>
      </tags>
  </entry>
  <entry>
    <title>Kalman Filter</title>
    <url>/2018/08/21/11.Kalman/</url>
    <content><![CDATA[<p>前段时间在知乎上看到的<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zODc0NTk1MA==">《卡尔曼滤波器及其在云台控制中的应用》<i class="fa fa-external-link-alt"></i></span>这篇文章，中间提到了Estimate的三个境界：smoothing-&gt;filtering-&gt;prediction。为了解决我在机器人比赛中的预测问题，也为了迈入Estimate的大门，就从Kalman Filter开始入手吧。</p>
<a id="more"></a>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>卡尔曼滤波器是在估计线性系统状态的过程中，以最小均方误差为目的而推导出的5个递推数学等式，即“根据当前的测量值和上一刻的预测量、误差，计算当前的最优量，再预测下一刻的量”。误差是不断变化而且独立存在, 始终不受测量数据的影响。<br>从概率论贝叶斯模型的观点来看，系统预测的结果是先验，系统测量的结果是后验。<br>核心思想：预测 + 测量反馈。</p>
<h1 id="状态方程"><a href="#状态方程" class="headerlink" title="状态方程"></a>状态方程</h1><script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
P(x_t|x_{t-1})&=N(Ax_{t-1}+Bu，Q)\\
x_t &=A_{t-1}+Bu+w ,&w\sim N(0，Q) 
\end{split} 
\end{equation}</script><ul>
<li>$A$ 为转换矩阵，大小 [n x n]</li>
<li>$x$ 是系统的状态向量，大小 [n x 1]</li>
<li>$B$ 是将输入转换为状态的矩阵，大小为 [n x k]</li>
<li>$u$ 为系统输入，大小为 [k x 1]</li>
<li>$w$ 是系统噪声</li>
</ul>
<h1 id="观测方程"><a href="#观测方程" class="headerlink" title="观测方程"></a>观测方程</h1><script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
P(y_t|x_{t})&=N(Hx_t+C，R)\\
y_t &=Hx_t+C+v，& v\sim N(0，R) 
\end{split} 
\end{equation}</script><ul>
<li>$Z$ 是测量值$y_t$，大小为 [m x 1]</li>
<li>$H$ 是状态变量到测量的转换矩阵，大小为 [m x n]</li>
<li>$v$ 是测量噪声</li>
</ul>
<h1 id="概率推导"><a href="#概率推导" class="headerlink" title="概率推导"></a>概率推导</h1><script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
P(x_t|y_1,...,y_t)&\propto P(x_t,y_1,...,y_t) \\
&= P(y_t|x_t,y_1,...,y_{t-1})\cdot P(x_t|y_1,...,y_{t-1})\cdot P(y_1,...,y_{t-1}) \\
&\propto P(y_t|x_t,y_1,...,y_{t-1})\cdot P(x_t|y_1,...,y_{t-1}) \\
&= P(y_t|x_t)\cdot P(x_t|y_1,...,y_{t-1})
\end{split} 
\end{equation}</script><p>其中，更新为 $P(x<em>t|y_1,…,y_t)$，预测为 $P(x_t|y_1,…,y</em>{t-1})$</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
P(x_t|y_1,...,y_{t-1}) 
&= \int_{x_{t-1}}P(x_t,x_{t-1}|y_1,...,y_{t-1})dx_{t-1} \\
&=\int_{}P(x_t|x_{t-1},y_1,...,y_{t-1}) \cdot P(x_{t-1}|y_1,...,y_{t-1})\cdot P(y_1,...,y_{t-1}) \\
&\propto\int_{}P(x_t|x_{t-1},y_1,...,y_{t-1}) \cdot P(x_{t-1}|y_1,...,y_{t-1}) \\
&=\int_{}P(x_t|x_{t-1})\cdot P(x_{t-1}|y_1,...,y_{t-1}) 
\end{split} 
\end{equation}</script><p>最后更新有了递归 $P(x<em>{t-1}|y_1,…,y</em>{t-1})$<br>重复描述这个过程：<br>$t=1$：<br>更新：$P(x_1|y_1)\sim N(\hat{\mu}_1,\hat{\Sigma}_1)$<br>$t=2$：<br>预测：$P(x_2|y_1)\sim N(\bar{\mu}_2,\bar{\Sigma}_2) $<br>更新：$P(x_2|y_1,y_2)\sim N(\hat{\mu}_2,\hat{\Sigma}_2) $</p>
<p>$t=t$：<br>预测：$P(x<em>t|y_1,…,y</em>{t-1})\sim N(\bar{\mu}_t,\bar{\Sigma}_t)$<br>更新：$P(x_t|y_1,…,y_t)\sim N(\hat{\mu}_t,\hat{\Sigma}_t) $</p>
<h1 id="由带噪声的预测和测量得到估计"><a href="#由带噪声的预测和测量得到估计" class="headerlink" title="由带噪声的预测和测量得到估计"></a>由带噪声的预测和测量得到估计</h1><p>每次更新状态是相互独立的，状态测量虽然是一个序列，但是由观测方程，每次测量也是相互独立的。对噪声计算协方差的平方矩阵就是Q和R。</p>
<p>预测理论值$x_k$（先验）有了，测量实际值$z_k$（后验）也有了，假设一个$K$，然后得到估计值</p>
<script type="math/tex; mode=display">\hat{x}_k=\hat{x}_k'+K_k(z_k-\hat{z}_k)=\hat{x}_k'+K_k(z_k-H\hat{x}_k')</script><ul>
<li>$\hat{x}_k’$ 是先验的预测</li>
<li>$\hat{z}_k$ 是后验的预测</li>
<li>$\hat{x}_k$ 是最后的估计值</li>
<li>$(z_k-H\hat{x}_k’)$ 是残差，即测量实际值$z_k$和预测理论值$\hat{z}_k$的差，即后验和先验的差</li>
</ul>
<h1 id="推导卡尔曼增益K"><a href="#推导卡尔曼增益K" class="headerlink" title="推导卡尔曼增益K"></a>推导卡尔曼增益K</h1><p>预测值$\hat{x}_k’$和真实值$x_k$之间有系统噪声带来的误差，计算协方差矩阵：</p>
<script type="math/tex; mode=display">P_k'=E\left[e_k'e_k'^T \right]=E\left[(x_k-\hat{x}_k')(x_k-\hat{x}_k')^T\right]</script><p>估计值$\hat{x}_k$和真实值$x_k$之间两种噪声都有，计算协方差矩阵：</p>
<script type="math/tex; mode=display">P_k=E\left[e_ke_k^T\right]=E\left[(x_k-\hat{x}_k)(x_k-\hat{x}_k)^T\right]</script><p>代入前面得到的估计值$\hat{x}_k$化简：</p>
<script type="math/tex; mode=display">P_k=E\left[[(I-K_kH)(x_k-\hat{x}_k')-K_kv_k][(I-K_kH)(x_k-\hat{x}_k')-K_kv_k]^T\right]</script><p>因为系统状态变量和测量噪声之间是相互独立的，所以继续推导：</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
P_k 
&= (I-K_kH)E\left[ (x_k-\hat{x}_k')(x_k-\hat{x}_k')^T \right](I-K_kH)+K_kE\left[ v_kv_k^T \right]K_k^T \\
&= (I-K_kH)P_k'(I-K_kH)^T+K_kRK_k^T \\
&= p_k'-K_kHP_k'-P_k'H^TK_k^T+K_k(HP_k'H^T+R)K_k^T
\end{split} 
\end{equation}</script><p>接下来求矩阵$P_k$的迹，即协方差矩阵的对角线元素之和，即均方差：</p>
<script type="math/tex; mode=display">T[P_k]=T[P_k']-2T[K_kHP_k']+T[K_k(HP_k'H^T+R)K_k^T]</script><p>寻找均方差的最小值，对未知量$K$求导，令导函数等于0得到$K$的计算式：</p>
<script type="math/tex; mode=display">
\frac{dT[P_k]}{dK_k}=-2(HP')^T+2K_k(HP_k'H^T+R) \\
K_k=P_k'H^T(HP_k'H^T+R)^{-1}</script><p>上面这个式子，转换矩阵$H$是常数，测量噪声协方差$R$也是常数。因此$K$的大小只和$P_k’$有关。<br>不妨进一步假设，上面式子中的矩阵大小都为 [1 x 1]，并假设$H=1$、$P_k’\ne0$。那么$K$可以写成：</p>
<script type="math/tex; mode=display">K_k=\frac{P_k'}{P_k'+R}=\frac{1}{1+R/P_k'}</script><p>所以$P_k’$越大，那么$K$就越大，权重将更加重视反馈。如果$P_k’=0$，即预测值和真实值相等，那么$K=0$，估计值就等于预测值。</p>
<p>将计算出的这个$K$反代入$P_k$中，就能简化$P_k$：</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
P_k
&= P_k'-P_k'H^T(HP_k'H^T+R)^{-1}HP_k' \\
&= P_k'-K_kHP_k' \\
&=(I-K_kH)P_k'
\end{split} 
\end{equation}</script><p>因此递推公式中每一步的K就计算出来了，同时每一步的估计协方差也能计算出来。最后一个未知量$P_k’$。它的递推计算如下：</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
P_{k+1}
&= E\left[e_{k+1}'e_{k+1}'^T \right] \\
&=E\left[ (x_{k+1}-\hat{x}'_{k+1})(x_{k+1}-\hat{x}'_{k+1})^T \right] \\
&= E\left[ (A(x_k-\hat{x}_k)+w_k)(A(x_k-\hat{x}_k)+w_{kk})^T \right] \\
&= E\left[(Ae_k)(Ae_k)^T\right]+E\left[ w_kw_k^T \right] \\
&= AP_kA^T+Q
\end{split} 
\end{equation}</script><p>由此也得到了$P_k’$的递推公式。因此我们只需设定最初的$P_k$，就能不断递推下去。</p>
<h1 id="总结-Kalman-计算流程"><a href="#总结-Kalman-计算流程" class="headerlink" title="总结 Kalman 计算流程"></a>总结 Kalman 计算流程</h1><ol>
<li>计算预测值 <script type="math/tex">x_k = Ax_{k-1}+Bu_{k-1}+w_{k-1}</script></li>
<li>计算系统误差 $P<em>k’ = AP</em>{k-1}A^T+Q$</li>
<li>计算卡尔曼增益 $K_k=P_k’H^T(HP_k’H^T+R)^{-1}$</li>
<li>计算估计值 $\hat{x}_k=\hat{x}_k’+K_k(z_k-H\hat{x}_k’)$</li>
<li>递推更新 $P_k = (I-K_kH)P_k’$</li>
</ol>
<p><img src="\img\kalman\kalman.png" alt=""></p>
<hr>
<h1 id="简化-Kalman-Filter"><a href="#简化-Kalman-Filter" class="headerlink" title="简化 Kalman Filter"></a>简化 Kalman Filter</h1><p>最简单的情况是一阶，全部是标量计算，调节Q/R的比值来控制滤波效果。当然可以是更复杂的，二阶三阶都可以。</p>
<ol>
<li>计算预测值 $\hat{x}^-<em>k=\hat{x}</em>{k-1}$</li>
<li>计算系统误差 $P^-<em>{k}=P</em>{k-1}+Q$</li>
<li>计算卡尔曼增益 $K_k=\frac{P^-_k}{P^-_k+R}$</li>
<li>计算估计值 $\hat{x}=\hat{x}^-_k+K_k(z_k-\hat{x}^-_k)$</li>
<li>递推更新 $P_k=(1-K_k)P^-_k$</li>
</ol>
<h1 id="坐标系和运动学模型的选择"><a href="#坐标系和运动学模型的选择" class="headerlink" title="坐标系和运动学模型的选择"></a>坐标系和运动学模型的选择</h1><p>参考自动驾驶中常用于车辆状态估计的车辆运动模型，主要包含以下几类：</p>
<ul>
<li>恒定速度模型（Constant Velocity, CV）</li>
<li>恒定加速度模型（Constant Acceleration, CA）</li>
<li>恒定转率和速度模型（Constant Turn Rate and Velocity，CTRV）</li>
<li>恒定转率和加速度模型（Constant Turn Rate and Acceleration，CTRA）</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hleWlqaWEwMzI3L2FydGljbGUvZGV0YWlscy8xNzQ4NzQ2Nw==">https://blog.csdn.net/heyijia0327/article/details/17487467<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hleWlqaWEwMzI3L2FydGljbGUvZGV0YWlscy8xNzY2NzM0MQ==">https://blog.csdn.net/heyijia0327/article/details/17667341<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zODc0NTk1MA==">https://zhuanlan.zhihu.com/p/38745950<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3d3dy5iemFyZy5jb20vcC9ob3ctYS1rYWxtYW4tZmlsdGVyLXdvcmtzLWluLXBpY3R1cmVzLyNtYXRoeWJpdHM=">http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/#mathybits<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>机器人</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>Kalman with eigen</title>
    <url>/2018/08/22/12.matrix-decomposition/</url>
    <content><![CDATA[<p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL21oZXJiL2thbG1hblQ=">https://github.com/mherb/kalmanT<i class="fa fa-external-link-alt"></i></span> 是一个基于eigin3的Kalman-Filter，包含了Extended Kalman Filter (EKF)、Square Root Extended Kalman Filter (SR-EKF)、Unscented Kalman Filter (UKF)、Square Root Unscented Kalman Filter (SR-UKF)，4种Kalman filter。</p>
<a id="more"></a>
<h1 id="Cholesky分解"><a href="#Cholesky分解" class="headerlink" title="Cholesky分解"></a>Cholesky分解</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlGJTgzJUU1JUIwJTk0JUU3JUIxJUIzJUU3JTg5JUI5JUU3JTlGJUE5JUU5JTk4JUI1">Hermitian matrix（埃尔米特矩阵，厄米矩阵，自伴随矩阵）<i class="fa fa-external-link-alt"></i></span>，是共轭对称的方阵。</p>
<script type="math/tex; mode=display">\left[\begin{matrix} 3 & 2+i \\ 2-i & 1 \end{matrix}\right]</script><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU2JUFEJUEzJUU1JUFFJTlBJUU3JTlGJUE5JUU5JTk4JUI1">正定矩阵<i class="fa fa-external-link-alt"></i></span>是Hermitian matrix的一种。<br>设A是一个n阶厄米特正定矩阵(Hermitian positive-definite matrix)。Cholesky分解的目标是：<br>设$A = LL^{T}$，得到：（其中$A<em>{21}$是一个列向量，$A</em>{22}$是一个n-1阶的方阵）</p>
<script type="math/tex; mode=display">\left[ \begin{matrix} a_{11}&A_{21}^{T}\\ A_{21}&A_{22}\\ \end{matrix} \right] =  \left[ \begin{matrix} l_{11}&0\\ L_{21}&L_{22}\\ \end{matrix} \right] \left[ \begin{matrix} l_{11}&L_{21}^{T}\\ 0&L_{22}^{T}\\ \end{matrix} \right]=  \left[ \begin{matrix} l_{11}^{2}&l_{11}L_{21}^{T}\\ l_{11}L_{21}&L_{21}L_{21}^{T}+L_{22}L_{22}^{T}\\ \end{matrix} \right]</script><p>其中，未知量$l<em>{11},L</em>{21},L_{22}$，这3个未知量的求解公式是：</p>
<script type="math/tex; mode=display">l_{11} = \sqrt {a_{11}}，L_{21} = \frac {1}{l_{11}}A_{21}，L_{22}L_{22}^{T} =  A_{22} - L_{21}L_{21}^{T}</script><p>设$A<em>{22}’ = A</em>{22} - L<em>{21}L</em>{21}^{T}$，则化简为$A<em>{22}’ = L</em>{22}L_{22}^{T}$，可以继续Cholesky分解，被分解的矩阵是A的右下角的n-1阶子方阵。所以这个算法具有递归性质。</p>
<p>举个例子：</p>
<script type="math/tex; mode=display">A =  \left[ \begin{matrix} 25&15&-5\\ 15&18&0\\ -5&0&11\\ \end{matrix} \right] =  \left[ \begin{matrix} l_{11}&0&0\\ l_{21}&l_{22}&0\\ l_{31}&l_{32}&l_{33}\\ \end{matrix} \right] \left[ \begin{matrix} l_{11}&l_{21}&l_{31}\\ 0&l_{22}&l_{32}\\ 0&0&l_{33}\\ \end{matrix} \right]</script><p>根据公式，有：</p>
<script type="math/tex; mode=display">l_{11} = \sqrt { a_{11} } = 5</script><script type="math/tex; mode=display">L_{21} = \frac {1}{l_{11}}A_{21} = \frac {1}{5} \left[ \begin{matrix} 15\\ -5\\ \end{matrix} \right] =  \left[ \begin{matrix} 3\\ -1\\ \end{matrix} \right]</script><script type="math/tex; mode=display">A_{22} - L_{21}L_{21}^{T}  =  L_{22}L_{22}^{T}</script><script type="math/tex; mode=display">A_{22} - L_{21}L_{21}^{T}  =  L_{22}L_{22}^{T}</script><script type="math/tex; mode=display">\left[ \begin{matrix} 18&0\\ 0&11\\ \end{matrix} \right] -  \left[ \begin{matrix} 3\\ -1\\ \end{matrix} \right] \left[ \begin{matrix} 3&-1\\ \end{matrix} \right] =   \left[ \begin{matrix} l_{22}&0\\ l_{32}&l_{33}\\ \end{matrix} \right]   \left[ \begin{matrix} l_{22}&l_{32}\\ 0&l_{33}\\ \end{matrix} \right]</script><script type="math/tex; mode=display">\left[ \begin{matrix} 9&3\\ 3&10\\ \end{matrix} \right] =  \left[ \begin{matrix} l_{22}&0\\ l_{32}&l_{33}\\ \end{matrix} \right]   \left[ \begin{matrix} l_{22}&l_{32}\\ 0&l_{33}\\ \end{matrix} \right]</script><p>(注意，这里已经是n-1阶的Cholesky分解)</p>
<script type="math/tex; mode=display">l_{22} = \sqrt { 9 } = 3</script><script type="math/tex; mode=display">l_{32} = \frac {1}{3}3 = 1</script><script type="math/tex; mode=display">10 = l_{32}^{2} + l_{33}^{2} = 1 + l_{33}^{2}</script><script type="math/tex; mode=display">l_{33} = \sqrt {10 - 1} = 3</script><p>综上</p>
<script type="math/tex; mode=display">A =  \left[ \begin{matrix} 25&15&-5\\ 15&18&0\\ -5&0&11\\ \end{matrix} \right] =  \left[ \begin{matrix} 5&0&0\\ 3&3&0\\ -1&1&3\\ \end{matrix} \right] \left[ \begin{matrix} 5&3&-1\\ 0&3&1\\ 0&0&3\\ \end{matrix} \right]</script><p>对矩阵的Cholesky分解，就像对实数的求平方根。根据协方差矩阵的定义，Cholesky分解可以反求期望。</p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>pods_cmake</title>
    <url>/2018/08/26/13.pods-cmake/</url>
    <content><![CDATA[<p>Apriltags的CMakeLists.txt中出现了一行pods_install_executables()，这并不是标准的CMake写法，项目中有cmake/pods.cmake文件，里面写道：Macros to simplify compliance with the pods build policies.</p>
<p>关于pods的资料不多，目前我对它的理解是一组基于CMake语法的宏，本身与CMake并没有任何区别。cmake/pods.cmake的内容可以参考：<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1JvYm90TG9jb21vdGlvbi9jbWFrZS9ibG9iL21hc3Rlci9wb2RzLmNtYWtl">https://github.com/RobotLocomotion/cmake/blob/master/pods.cmake<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9zb3VyY2Vmb3JnZS5uZXQvcHJvamVjdHMvcG9kcy8=">https://sourceforge.net/projects/pods/<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9zb3VyY2Vmb3JnZS5uZXQvcC9wb2RzL3dpa2kvUG9kc1Rvb2wv">https://sourceforge.net/p/pods/wiki/PodsTool/<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>工具链</tag>
      </tags>
  </entry>
  <entry>
    <title>Lipschitz Condition</title>
    <url>/2018/09/13/14.Lipschitz/</url>
    <content><![CDATA[<p>L-lipschitz连续性是最近优化课程经常提到的一个基础内容，老师人很nice，讲课也很有开放性，每次都需要做大量的预习和课后复习才能完全消化。所以最近的Blog基本上都离不开数学主题了。<br>课堂引出L-lipschitz是为了衡量一个凸函数是否容易优化，通过函数光滑的程度和凸的程度来判断，原文是：$\nabla f$ is L-lipschitz and $f$ $\mu$-strongly convex.</p>
<a id="more"></a> 
<hr>
<h1 id="Definition-of-Lipschitz-Continuous"><a href="#Definition-of-Lipschitz-Continuous" class="headerlink" title="Definition of Lipschitz Continuous"></a>Definition of Lipschitz Continuous</h1><p>If $f$ is Lipschitz continuous on $Q$ with constant $L$，if for all $x, y\in Q$ we have：</p>
<script type="math/tex; mode=display">||f(x)-f(y)||\leq L||x-y||</script><p>If $f$ is Lipschitz continuous gradient on $R^n$. Then for any $x, y\in R^n$ we have：</p>
<script type="math/tex; mode=display">|f(y)-f(x)-\langle f'(x)，y-x\rangle |\leq \frac{L}{2}||y-x||^2</script><p>If $f$ is Lipschitz continuous Hessian on $R^n$. Then for any $x, y\in R^n$ we have：</p>
<script type="math/tex; mode=display">|f(y)-f(x)-\langle f'(x)，y-x\rangle-\frac{1}{2}\langle f''(x)(y-x)，y-x\rangle |\leq \frac{L}{6}||y-x||^3</script><p>在知乎文章中主要提到了这三种Lipschitz Continuous：</p>
<ol>
<li>Lipschitz continuous：函数被一次函数上下夹逼</li>
<li>Lipschitz continuous gradient：函数被二次函数上下夹逼</li>
<li>Lipschitz continuous Hessian：函数被三次函数上下夹逼</li>
</ol>
<p>通过对函数求导，可以构造出更高阶的Lipschitz continuous condition。其中，gradient是一阶导数，Hessian是二阶导数。</p>
<p>Lipschitz continuous 用在函数值上是为了不让函数值变化的太快；用在导函数上，是为了不让导函数变化的太快；用在Hessian上，是为了让Hessian不变化的太快。但他们都导致了一个很有意思的结果：这个Lipschitz continuous不管用在什么上，都使的函数被多项式上下夹逼，一方面便于我们处理，另一方面至少我们能控制一下函数的包络信息。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yNzU1NDE5MQ==">https://zhuanlan.zhihu.com/p/27554191<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ2F1Y2h5JUUyJTgwJTkzU2Nod2Fyel9pbmVxdWFsaXR5">https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTg4JUE5JUU2JTk5JUFFJUU1JUI4JThDJUU4JThDJUE4JUU5JTgwJUEzJUU3JUJBJThD">https://zh.wikipedia.org/wiki/%E5%88%A9%E6%99%AE%E5%B8%8C%E8%8C%A8%E9%80%A3%E7%BA%8C<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly91c2Vycy53cGkuZWR1L353YWxrZXIvTUE1MDAvSEFORE9VVFMvTGlwc2NoaXR6Q29udGludWl0eS5wZGY=">https://users.wpi.edu/~walker/MA500/HANDOUTS/LipschitzContinuity.pdf<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3d3dy53aW4udHVlLm5sL35ydmhhc3NlbC9PbmRlcndpanMvT2xkLU9uZGVyd2lqcy8yV0EyMy0yMDExL0hPLTAzLnBkZg==">http://www.win.tue.nl/~rvhassel/Onderwijs/Old-Onderwijs/2WA23-2011/HO-03.pdf<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>google tools</title>
    <url>/2018/09/30/15.google/</url>
    <content><![CDATA[<p>本blog整理google自家的工具链，包括glags、glog、gtest、protocol-buffers</p>
<a id="more"></a>
<h1 id="Googletest-断言"><a href="#Googletest-断言" class="headerlink" title="Googletest-断言"></a>Googletest-断言</h1><p><code>EXPECT_EQ(expected, actual)</code> is the same as <code>EXPECT_TRUE((expected) == (actual))</code></p>
<ol>
<li><code>EXPECT_*</code> 失败时，案例继续往下执行。</li>
<li><code>ASSERT_*</code> 失败时，直接在当前函数中返回，当前函数中<code>ASSERT_*</code>后面的语句将不会执行。 </li>
</ol>
<h2 id="布尔值检查"><a href="#布尔值检查" class="headerlink" title="布尔值检查"></a>布尔值检查</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Fatal assertion</th>
<th>Nonfatal assertion</th>
<th>Verifies</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASSERT_TRUE(condition);</td>
<td>EXPECT_TRUE(condition);</td>
<td>condition is true</td>
</tr>
<tr>
<td>ASSERT_FALSE(condition);</td>
<td>EXPECT_FALSE(condition);</td>
<td>condition is false</td>
</tr>
</tbody>
</table>
</div>
<h2 id="数值型数据检查"><a href="#数值型数据检查" class="headerlink" title="数值型数据检查"></a>数值型数据检查</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Fatal assertion</th>
<th>Nonfatal assertion</th>
<th>Verifies</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASSERT_EQ(expected, actual);</td>
<td>EXPECT_EQ(expected, actual);</td>
<td>expected == actual</td>
</tr>
<tr>
<td>ASSERT_NE(val1, val2);</td>
<td>EXPECT_NE(val1, val2);</td>
<td>val1 != val2</td>
</tr>
<tr>
<td>ASSERT_LT(val1, val2);</td>
<td>EXPECT_LT(val1, val2);</td>
<td>val1 &lt; val2</td>
</tr>
<tr>
<td>ASSERT_LE(val1, val2);</td>
<td>EXPECT_LE(val1, val2);</td>
<td>val1 &lt;= val2</td>
</tr>
<tr>
<td>ASSERT_GT(val1, val2);</td>
<td>EXPECT_GT(val1, val2);</td>
<td>val1 &gt; val2</td>
</tr>
<tr>
<td>ASSERT_GE(val1, val2);</td>
<td>EXPECT_GE(val1, val2);</td>
<td>val1 &gt;= val2</td>
</tr>
</tbody>
</table>
</div>
<h2 id="字符串检查"><a href="#字符串检查" class="headerlink" title="字符串检查"></a>字符串检查</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Fatal assertion</th>
<th>Nonfatal assertion</th>
<th>Verifies</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASSERT_STREQ(expected_str, actual_str);</td>
<td>EXPECT_STREQ(expected_str, actual_str);</td>
<td>the two C strings have the same content</td>
</tr>
<tr>
<td>ASSERT_STRNE(str1, str2);</td>
<td>EXPECT_STRNE(str1, str2);</td>
<td>the two C strings have different content</td>
</tr>
<tr>
<td>ASSERT_STRCASEEQ(expected_str, actual_str);</td>
<td>EXPECT_STRCASEEQ(expected_str, actual_str);</td>
<td>the two C strings have the same content, ignoring case</td>
</tr>
<tr>
<td>ASSERT_STRCASENE(str1, str2);</td>
<td>EXPECT_STRCASENE(str1, str2);</td>
<td>the two C strings have different content, ignoring case</td>
</tr>
</tbody>
</table>
</div>
<h2 id="异常检查"><a href="#异常检查" class="headerlink" title="异常检查"></a>异常检查</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Fatal assertion</th>
<th>Nonfatal assertion</th>
<th>Verifies</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASSERT_THROW(statement, exception_type);</td>
<td>EXPECT_THROW(statement, exception_type);</td>
<td>statement throws an exception of the given type</td>
</tr>
<tr>
<td>ASSERT_ANY_THROW(statement);</td>
<td>EXPECT_ANY_THROW(statement);</td>
<td>statement throws an exception of any type</td>
</tr>
<tr>
<td>ASSERT_NO_THROW(statement);</td>
<td>EXPECT_NO_THROW(statement);</td>
<td>statement doesn’t throw any exception</td>
</tr>
</tbody>
</table>
</div>
<h2 id="浮点型检查"><a href="#浮点型检查" class="headerlink" title="浮点型检查"></a>浮点型检查</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Fatal assertion</th>
<th>Nonfatal assertion</th>
<th>Verifies</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASSERT_FLOAT_EQ(expected, actual);</td>
<td>EXPECT_FLOAT_EQ(expected, actual);</td>
<td>the two float values are almost equal</td>
</tr>
<tr>
<td>ASSERT_DOUBLE_EQ(expected, actual);</td>
<td>EXPECT_DOUBLE_EQ(expected, actual);</td>
<td>the two double values are almost equal</td>
</tr>
</tbody>
</table>
</div>
<p>对相近的两个数比较：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Fatal assertion</th>
<th>Nonfatal assertion</th>
<th>Verifies</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASSERT_NEAR(val1, val2, abs_error);</td>
<td>EXPECT_NEAR(val1, val2, abs_error);</td>
<td>the difference between val1 and val2 doesn’t exceed the given absolute error</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>工具链</tag>
      </tags>
  </entry>
  <entry>
    <title>Bayesian Theorem</title>
    <url>/2018/10/02/16.Bayesian-Theorem/</url>
    <content><![CDATA[<p>根据概率论中的贝叶斯公式，有：</p>
<script type="math/tex; mode=display">
P(\omega_i|x)=\frac{p(x,\omega_i)}{p(x)}=\frac{p(x|\omega_i)p(\omega_i)}{p(x)},i=1,2,...</script><ul>
<li>$p(\omega_i)$是先验概率</li>
<li>$p(x,\omega_i)$是联合概率分布</li>
<li>$p(x)$是总体密度</li>
<li>$p(x|\omega_i)$是第$i$类x的概率密度，即类条件概率</li>
</ul>
<p>这样，后验概率就转换成先验概率与类条件概率密度的乘积，再用总体密度进行归一化。这就是贝叶斯决策。</p>
<a id="more"></a>
<hr>
<h1 id="最小错误率准则"><a href="#最小错误率准则" class="headerlink" title="最小错误率准则"></a>最小错误率准则</h1><p>使错误率最小的分类决策，就是使后验概率最大。默认情况下，贝叶斯决策就是最小错误率决策。其中，后验概率用贝叶斯公式求得：</p>
<script type="math/tex; mode=display">
P(\omega_i|x)=\frac{p(x|\omega_i)p(\omega_i)}{p(x)}=\frac{p(x|\omega_i)p(\omega_i)}{\sum_{j=1}^{2}{p(x|\omega_i)p(\omega_i)}},i=1,2</script><p>最小错误率可以表示多种形式，比如：</p>
<script type="math/tex; mode=display">
minP(e)=\int_{}^{}P(e)p(x)dx \\ 
P(\omega_i|x)=\max P(\omega_j|x),j=1,2</script><p>如果$p(x|\omega_1)p(\omega_1)P(\omega_1)&gt;p(x|\omega_2)p(\omega_2)P(\omega_2)$,则$x\in\omega_1$;反之，则$x\in\omega_2$</p>
<h2 id="似然比"><a href="#似然比" class="headerlink" title="似然比"></a>似然比</h2><p>先验概率$p(\omega_i)$是事先确定的，与当前样本$x$无关，这样可以实现计算似然比$\lambda$，对每一个样本计算$l(x)$作比较</p>
<script type="math/tex; mode=display">
l(x)=p(x|\omega_1)/p(x|\omega_2)，\lambda=P(\omega_2)/P(\omega_1) \\
l(x) > \lambda, x\in \omega_1 \\ 
l(x) < \lambda, x\in \omega_2</script><p>为了计算，有时候使用对数似然比</p>
<script type="math/tex; mode=display">
h(x)=-\ln [l(x)]=-\ln p(x|\omega_1)+\ln p(x|\omega_2)</script><h2 id="错误率"><a href="#错误率" class="headerlink" title="错误率"></a>错误率</h2><p>不同类的分界线称作决策边界，在多维情况下称为决策面，它把特征空间划分成属于各类的区域。<br>对二分类问题的错误率分析：</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
P(e)&= \int_{-\infty}^{t}P(\omega_2|x)p(x)dx+\int_{t}^{\infty}P(\omega_1|x)p(x)dx \\
&=\int_{-\infty}^{t}p(x|\omega_2)P(\omega_2)dx+\int_{t}^{\infty}p(x|\omega_1)P(\omega_1)dx \\
&=P(\omega_2)\int_{-\infty}^{t}p(x|\omega_2)dx+P(\omega_1)\int_{t}^{\infty}p(x|\omega_1)dx \\ 
&=P(\omega_2)P_2(e)+P(\omega_1)P_1(e)
\end{split} 
\end{equation}</script><p>其中，$P_1(e)$是把第一类错判成第二类的错误率，$P_2(e)$是把第二类错判成第一类的错误类。</p>
<h2 id="多决策"><a href="#多决策" class="headerlink" title="多决策"></a>多决策</h2><p>假如不是二分类，就要把特征空间分割成$\mathcal{R_1}，\mathcal{R_2}，…，\mathcal{R_n}$个区域，可能错分的情况就很多，平均错误率$P(e)$将有$c(c-1)$项，一共$c$行，每行$c-1$列。</p>
<script type="math/tex; mode=display">
P(e)=\sum_{i=1}^{c}\sum_{j=1}^{c}{[P(x\in \mathcal{R_j}|\omega_i)]P(\omega_i)}</script><p>可以通过计算平均正确率$P(c)$来降低计算量：</p>
<script type="math/tex; mode=display">
P(c)=\sum_{j=1}^{c}{P(x\in\mathcal{R}|\omega_i)P(\omega_j)}=\sum_{j=1}^{c}{\int_{\mathcal{R_j}}p(x|\omega_j)P(\omega_j)}dx \\
P(e)=1-P(c)</script><hr>
<h1 id="最小风险准则"><a href="#最小风险准则" class="headerlink" title="最小风险准则"></a>最小风险准则</h1><p>所谓最小风险准则，就是考虑各种错误造成损失不同时的一种最优决策。</p>
<ul>
<li>样本$x$看作$d$维随机向量：$x=[x_1,x_2,…,x_d]^T$</li>
<li>状态空间$\Omega$由$c$个可能的状态（$c$类）组成：$\Omega={\omega_1,\omega_2,…,\omega_c}$</li>
<li>对$x$可能采取的决策组成决策空间，由$k$个决策组成：$\mathcal{A}={\alpha_1,\alpha_2,…,\alpha_k}$</li>
<li>对实际为$\omega_j$的x采取决策$\alpha_i$所带来的损失为$\lambda(\alpha_i,\omega_j),i=1,…,k， j=1,…,c$</li>
</ul>
<p>这里没有假定$k=c$，是因为允许拒绝决策，比如不属于任何一类，这是更一般的情况。决策损失可以列出一个$k$行$c$列的决策表。</p>
<p>对于某一个样本属于各个状态的后验概率是$P(\omega_j|x),j=1,…,c$，对它采取决策$\alpha_i,i=1,…,k$ 的期望损失是：</p>
<script type="math/tex; mode=display">
R(\alpha_i|x)=E(\lambda(\alpha_i,\omega_j)|x)=\sum_{j=1}^{c}\lambda(\alpha_i,\omega_j)P(\omega_j|x)，i=1,...,k</script><p>设某一个决策$\alpha(x)$，它对特征空间中所有可能的样本采用决策所造成的期望损失是$R(\alpha)$，称为平均/期望风险，令它最小就是最小风险准则。</p>
<script type="math/tex; mode=display">
\min R(\alpha)=\int R(\alpha(x)|x)p(x)dx</script><p>以二分类为例，</p>
<script type="math/tex; mode=display">
\lambda_{11}P(\omega_1|x)+\lambda_{12}P(\omega_2|x) < \lambda_{21}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)，x\in \omega_1 \\
\lambda_{11}P(\omega_1|x)+\lambda_{12}P(\omega_2|x) > \lambda_{21}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)，x\in \omega_2</script><p>关于决策表，可以写成矩阵吧，对角线一般都是0，最小错误率准则就是0-1决策表，而最小风险准则就是有了不同的参数.</p>
<script type="math/tex; mode=display">
\left[\begin{matrix}\lambda_{11} & \lambda_{12} \\ \lambda_{21} & \lambda_{22}\end{matrix}\right]=\left[\begin{matrix}0 & 1 \\ 1 & 0\end{matrix}\right]</script><hr>
<h1 id="两类错误率、Neyman-Pearson决策与ROC曲线"><a href="#两类错误率、Neyman-Pearson决策与ROC曲线" class="headerlink" title="两类错误率、Neyman-Pearson决策与ROC曲线"></a>两类错误率、Neyman-Pearson决策与ROC曲线</h1><p>待续</p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>Normal distribution</title>
    <url>/2018/10/03/17.Normal-distribution/</url>
    <content><![CDATA[<p>本blog总结单变量正态分布以及多元正态分布和他们的性质。</p>
<a id="more"></a>
<h1 id="单变量正态分布"><a href="#单变量正态分布" class="headerlink" title="单变量正态分布"></a>单变量正态分布</h1><p>概率密度函数：</p>
<script type="math/tex; mode=display">
p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right\}</script><p>期望$\mu$和方差$\sigma^2$：</p>
<script type="math/tex; mode=display">
\mu = E\{x\}=\int_{-\infty}^{\infty}xp(x)dx \\
\sigma^2=\int_{-\infty}^{\infty}(x-\mu)^2p(x)dx</script><h1 id="多元正态分布"><a href="#多元正态分布" class="headerlink" title="多元正态分布"></a>多元正态分布</h1><p>概率密度函数：</p>
<script type="math/tex; mode=display">
p(x)=\frac{1}{(x\pi)^{d/2}|\Sigma|^{1/2}}\exp\left\{ -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right\}</script><ul>
<li>$x=[x_1,x_2,…,x_d]^T$ 是d维向量</li>
<li>$\mu=[\mu_1,\mu_2,…,\mu_d]^T$是d维均值向量</li>
<li>$\Sigma$是dxd维协方差矩阵</li>
</ul>
<p>$<br>\mu=E{x} \<br>\Sigma=E{(x-\mu)(x-\mu)^T}<br>$<br>$\mu$，$\Sigma$分别是向量$x$和矩阵$(x-\mu)(x-\mu)^T$的期望，若$x<em>i$是$x$的第$i$个个分量，$\mu_i$是$\mu$的第$i$个分量，$\sigma</em>{ij}$是$\Sigma$的第$i,j$个元素，则</p>
<script type="math/tex; mode=display">
\mu_i=E\{x_i\}=\int_{E^d}x_ip(x)dx=-\int_{-\infty}^{\infty}x_ip(x)dx_i</script><p>其中，$p(x_i)$为边缘分布</p>
<script type="math/tex; mode=display">
p(x_i)=\int_{-\infty}^{\infty}...\int_{-\infty}^{\infty}p(x)dx_1dx_2...dx_{x-1}dx_{i+1}...dx_d</script><p>而</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
\sigma_{ij}&=E[(x_i-\mu_i)(x_j-\mu_j)] \\
&=\int_{-\infty}^{\infty}...\int_{-\infty}^{\infty}(x_i-\mu_i)(x_j-\mu_j)p(x_i,x_j)dx_idx_j
\end{split} 
\end{equation}</script><p>协方差矩阵总是正定or半正定的矩阵，可以表示为</p>
<script type="math/tex; mode=display">\Sigma = 
\left[\begin{matrix}
\sigma_{11} & \sigma_{12} & \dots & \sigma_{1d} \\ 
\sigma_{21} & \sigma_{22} & \dots & \sigma_{2d} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\sigma_{2d} & \sigma_{2d} & \dots & \sigma_{dd} \\ 
\end{matrix}\right]</script><h2 id="多元分布性质"><a href="#多元分布性质" class="headerlink" title="多元分布性质"></a>多元分布性质</h2><p>多元正态分布被均值向量$\mu$和协方差矩阵$\Sigma$所完全决定。</p>
<script type="math/tex; mode=display">
p(x)\sim N(\mu,\Sigma)</script><p>等密度点的轨迹为以超椭球面，区域中心由均值向量$\mu$决定，区域的大小由协方差矩阵$\Sigma$决定。当指数项为常数时，构成等密度点：</p>
<script type="math/tex; mode=display">
(x-\mu)^T\Sigma^{-1}(x-\mu)=C</script><p>该式子的解是一个超椭球面，且它的主轴方向由$\Sigma$阵的特征向量决定，主轴长度与相应的协方差矩阵$\Sigma$的特征值成正比。</p>
<script type="math/tex; mode=display">
\gamma^2=(x-\mu)^T\Sigma^{-1}(x-\mu)</script><p>称为由$x$到$\mu$的Mahalanobis距离（马氏距离）的平方，对应于Mahalanobis距离为$\gamma$的超椭圆球体积是</p>
<script type="math/tex; mode=display">
V=V_d|\Sigma|^{\frac{1}{2}}\gamma^d</script><p>其中$V_d$是$d$维单位超球体的体积，</p>
<script type="math/tex; mode=display">
V_d= 
\begin{equation} 
\left\{ 
\begin{array}{lc} 
\frac{\pi^{d/2}}{\left( \frac{d}{2} \right)!} & d为偶数\\
\frac{2^d\pi^{(d-1)/2}\left( \frac{d-1}{2} \right)!}{d!} & d为奇数
\end{array} 
\right. 
\end{equation}</script><p>对于给定的维数，样本离散度直接随$|\Sigma|^{1/2}$而变。</p>
<h2 id="不相关性-独立性"><a href="#不相关性-独立性" class="headerlink" title="不相关性=独立性"></a>不相关性=独立性</h2><ul>
<li>不相关：$E{x_ix_j}=E{x_i}E{x-j}$</li>
<li>独立的：$p{x_ix_j}=p{x_i}p{x_j}$</li>
</ul>
<p>独立性比不相关性条件更强，而在多元正态分布中，两者等价。在正态分布中，变量互不相关，则一定独立。</p>
<p>推论：如果多元正态随机向量的协方差矩阵是对角阵，则x的分量是相互独立的正态分布随机变量。</p>
<h2 id="边缘分布和条件分布的正态性"><a href="#边缘分布和条件分布的正态性" class="headerlink" title="边缘分布和条件分布的正态性"></a>边缘分布和条件分布的正态性</h2><p>以二元正态分布为例，</p>
<ul>
<li>向量$x=[x_1,x_2]^T$</li>
<li>均值$\mu=[\mu_1,\mu_2]^T$</li>
<li>协方差矩阵$\Sigma=\left[\begin{matrix}\sigma<em>{11}^2&amp;\sigma</em>{12}^2\\sigma<em>{21}^2&amp;\sigma</em>{22}^2\end{matrix}\right]$</li>
</ul>
<script type="math/tex; mode=display">
p(x_1)\sim N(\mu_1,\sigma_{11}^2) \\
p(x_2)\sim N(\mu_1,\sigma_{22}^2)</script><h2 id="线性变换的正态性"><a href="#线性变换的正态性" class="headerlink" title="线性变换的正态性"></a>线性变换的正态性</h2><p>对于多元正态随机变量$x$，无论是线性变化$y=Ax$还是线性组合$y=\alpha^Tx$，正态分布依然是正态分布。</p>
<script type="math/tex; mode=display">
p(y)\sim N(A\mu,A\Sigma A^T)
p(y)\sim N(\alpha^T\mu,\alpha^T\Sigma\alpha)</script>]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>Fisher</title>
    <url>/2018/10/19/18.Fisher/</url>
    <content><![CDATA[<p>线性判别方法linear discriminant analysis, LDA, R.A.</p>
<a id="more"></a> 
<h1 id="线性判别函数"><a href="#线性判别函数" class="headerlink" title="线性判别函数"></a>线性判别函数</h1><ul>
<li>线性判别函数：$y = Wx$</li>
<li>样本向量：$W$</li>
<li>权值向量：$x$</li>
<li>决策面：$g(x) = g_1(x) - g_2(x) = 0$</li>
<li>$x$到决策面$H$的距离：$g(x) / ||W||$</li>
</ul>
<hr>
<h2 id="涉及多分类"><a href="#涉及多分类" class="headerlink" title="涉及多分类"></a>涉及多分类</h2><ol>
<li><p>每一类别可用单个判别边界与其它类别相分开，各自确定边界：</p>
<script type="math/tex; mode=display">y = A*x，->solve</script></li>
<li><p>每个模式类和其它模式类间可分别用判别平面分开，这样，有$M(M - 1)/2$个判别平面：</p>
<ul>
<li>判别函数： $g<em>{ij}(X) = w</em>{ij}^Tx$</li>
<li>判别边界： $g_{ij}(x) = 0$</li>
<li><script type="math/tex; mode=display">g_{ij}(x)>0, x\in \omega_1 \\  g_{ij}(x)<0, x \in \omega_2</script></li>
<li><strong>结论</strong>：判别区间增大，不确定区间减少，比第一种情况小得多。</li>
</ul>
</li>
<li><p>每类都有一个判别函数, 存在M个判别函数</p>
<ul>
<li>判别函数：$g(x)=W_K^TX ,k=1,2,…,M$</li>
<li>判别规则：$g_i(x)=W_i^TX , Max: x\in\omega_1 , samll other$</li>
<li>判别边界：$g_i(x)=g_j(x) 或者 g_i(x)-g_j(x)=0$  </li>
<li><strong>优点</strong>：考虑了相邻的判别函数，可以保证交于一点，不确定区间没有了，所以这种是最好情况。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="广义线性判别函数"><a href="#广义线性判别函数" class="headerlink" title="广义线性判别函数"></a>广义线性判别函数</h2><script type="math/tex; mode=display">
g(x) = w1f1+w2f2+w3f3+......+wk</script><ol>
<li>这样一个非线性判别函数通过映射，变换成线性判别函数。</li>
<li>原始的特征空间是非线性，</li>
<li>但通过某种映射，在新的空间 能保证是线性函数</li>
</ol>
<h2 id="引入Fisher"><a href="#引入Fisher" class="headerlink" title="引入Fisher"></a>引入Fisher</h2><ol>
<li>为了降维，降低计算复杂度</li>
<li>易于分类</li>
<li>使两类样本在该轴上投影之间的距离尽可能远，</li>
<li>而每一类 样本的投影尽可能紧凑。</li>
<li>评价标准<ul>
<li>类内离散度矩阵$S_w$</li>
<li>类间离散度矩阵$S_b$</li>
</ul>
</li>
</ol>
<hr>
<h1 id="Fisher"><a href="#Fisher" class="headerlink" title="Fisher"></a>Fisher</h1><h2 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h2><ul>
<li>step1：计算均值向量 $mean_1、mean_2、mean_3、…$</li>
<li>step2：计算类内离散度矩阵$Si_1、Si_2、Si_3、…$</li>
<li>step3：计算总样本类内离散度矩阵$S_w$<ul>
<li>$S_w=Si_1+Si_2+Si_3+…$</li>
</ul>
</li>
<li>step4：计算样本类间离散度矩阵$S<em>b$，$Sb</em>{12}、Sb<em>{13}、Sb</em>{23}、…$</li>
<li>step5：最佳投影方向</li>
<li>step6：判别函数求阈值 $W_0$</li>
</ul>
<h2 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h2><p>Fisher并没有考虑到数据的分布，假如数据不是正态分布的，甚至是多峰分布的，Fisher就瞎了，我认为这才是Fisher最大的问题。</p>
<h2 id="改进-amp-多分类"><a href="#改进-amp-多分类" class="headerlink" title="改进 &amp; 多分类"></a>改进 &amp; 多分类</h2><p>通过看书和查阅各种资料，改进/分类的核心思想其实就2点：</p>
<p>类间协方差矩阵的推广，Duda and Hart，1973</p>
<blockquote>
<p>Pattern Classification and Scene Analysis by Richard O. Duda and Peter E. Hart<br>Michael Thompson<br>Leonardo<br>The MIT Press<br>Volume 7, Number 4, Autumn 1974<br>p. 370</p>
</blockquote>
<p>判别准则选择，Fukunage，1990</p>
<blockquote>
<p>Introduction to Statistical Pattern Recognition Second Edition，1990.<br>Keinosuke Fukunaga<br>0327.F85 1990. 006.4 - dc20. 89-18195. CIP.</p>
<script type="math/tex; mode=display">
J(W)=Tr{S_W^{-1}S_B} \\
J(W)=Tr{(W^TS_WW)^{-1}(W^TS_WW)}</script><p>其中，权值由${S_W^{-1}S_B}$的d个特征值决定（降序，$d&lt;D$）。</p>
</blockquote>
<p>取d个最大的标准，D阶方阵的可以算出D个特征值，大多数都是复数，默认取为实数的特征值，再排序。</p>
]]></content>
      <categories>
        <category>模式识别</category>
      </categories>
      <tags>
        <tag>模式识别</tag>
      </tags>
  </entry>
  <entry>
    <title>Cpp-Taskflow</title>
    <url>/2018/10/17/20.cpp-taskflow/</url>
    <content><![CDATA[<p>A fast C++ header-only library to help you quickly write parallel programs with complex task dependencies.<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2NwcC10YXNrZmxvdy9jcHAtdGFza2Zsb3c=">https://github.com/cpp-taskflow/cpp-taskflow<i class="fa fa-external-link-alt"></i></span></p>
<a id="more"></a>
<h2 id="升级Cmake"><a href="#升级Cmake" class="headerlink" title="升级Cmake"></a>升级Cmake</h2><p>指定安装路径进行安装<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /user/bin/cmake-3.8/cmake-x.xx.x</span><br><span class="line">./bootstrap --prefix=/user/bin/cmake-x.x/cmake-x.xx.x</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><br>另一种安装方案：不使用./bootstrap，而通过 ./configure执行<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">sudo ln -sf /....../....../cmake-3.12.4/bin/*  /usr/bin/</span><br></pre></td></tr></table></figure></p>
<h2 id="升级编译器gcc-g"><a href="#升级编译器gcc-g" class="headerlink" title="升级编译器gcc/g++"></a>升级编译器gcc/g++</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:ubuntu-toolchain-r/test</span><br><span class="line">sudo apt-get update </span><br><span class="line">sudo apt-get install gcc-7</span><br><span class="line">sudo apt-get install g++-7</span><br><span class="line"></span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 100</span><br><span class="line">sudo update-alternatives --config gcc</span><br><span class="line"></span><br><span class="line">sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 100</span><br><span class="line">sudo update-alternatives --config g++</span><br><span class="line"></span><br><span class="line">gcc -v</span><br><span class="line">g++ -v</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>特征提取与选择</title>
    <url>/2018/11/20/22.feature-select/</url>
    <content><![CDATA[<p>对分类器设计来说，使用什么样的特征描述事物，也就是说使用什么样的特征空间是个很重要的问题。这个问题称之为描述量的选择问题，意思是指保留哪些描述量，删除哪些描述量的问题。<br>本章节研究对特征空间进行改造,目的在于提高其某方面的性能，因此又称特征的优化问题。</p>
<a id="more"></a>
<hr>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>对特征空间的改造、优化,主要的目的是降维，即把维数高的特征空间改成维数低的特征空间，降维主要有两种途径：</p>
<ul>
<li>一种是<code>筛选</code>掉一些次要的特征，问题在于如何确定特征的重要性，以及如何筛选。</li>
<li>另一种方法是使用<code>变换</code>的手段，限定在线性变换的方法上，通过变换来实现降维。</li>
</ul>
<h2 id="特征的选择与提取"><a href="#特征的选择与提取" class="headerlink" title="特征的选择与提取"></a>特征的选择与提取</h2><ul>
<li>分析各种特征的有效性并选出最有代表性的特征是模式识别系统设计的关键步骤。</li>
<li>降低特征维数在很多情况下是有效设计分类器的重要课题。</li>
</ul>
<h2 id="特征空间的优化"><a href="#特征空间的优化" class="headerlink" title="特征空间的优化"></a>特征空间的优化</h2><p>对初始的特征空间进行优化是为了降维。即初始的特征空间维数较高。能否改成一个维数较低的空间，称为优化。</p>
<h3 id="特征优化两种方法"><a href="#特征优化两种方法" class="headerlink" title="特征优化两种方法"></a>特征优化两种方法</h3><p>假设有D维特征向量空间，$y={y1,y2,…yD}$:</p>
<ol>
<li><code>特征选择</code>是指从原有的D维特征空间，删去一些特征描述量，从而得到精简后的特征空间。在这个特征空间中，样本由降维后的d维的特征向量描述：$x={x1,x2,…xd}，d &lt; D$。由于x只是y的一个子集，因此每个分量$x_i$必然能在 原特征集中找到其对应的描述量$x_i=y_j$。</li>
<li><code>特征提取</code>则是找到一个映射关系：$A:Y→X$，使新样本特征描述维数比原维数降低。其中每个分量$x_i$是原特征向量各分量的函数，即$x_i=W^Ty_i$。</li>
</ol>
<hr>
<h1 id="类别可分离性判据"><a href="#类别可分离性判据" class="headerlink" title="类别可分离性判据"></a>类别可分离性判据</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>特征选择或特征提取任务是从n个特征中求出对分类最有效的m个特征(m＜n)。</p>
<ol>
<li>对于<code>特征选择</code>来讲，从n个特征中选择出m个特征， 有$C_n^m$种组合方式。哪一种特征组的分类效果最好？这需要有一个比较标准，即需要一个定量的准则来衡量选择结果的好坏。</li>
<li>对于<code>特征提取</code>来讲，把n维特征向量变换 成m维特征向量，有各种变换。哪一种变换得到的m维特征向量对分类最有效？需要用一个准则来衡量。</li>
</ol>
<p>用以定量检验分类性能的准则称为类别可分性准则$J_{ij}$, 需要满足以下几点：</p>
<ol>
<li>与错误概率有单调关系，这样使准则取最大值的效果一般来说其错误率也较小。</li>
<li>度量特性：<ul>
<li>$J_{ij} &gt; 0$，当$i\ne j$时</li>
<li>$J_{ij} = 0$，当$i = j$时</li>
<li>$J<em>{ij} = J</em>{ji}$</li>
<li>这里$J<em>{ij}$是第$i$类和第$j$类的可分性准则函数，$J</em>{ij}$越大，两类的分离程度就越大。</li>
</ul>
</li>
<li>单调性：即加入新的特征时候，准则函数不减小。</li>
<li>当特征独立时有可加性。</li>
</ol>
<h2 id="基于距离的可分性判据"><a href="#基于距离的可分性判据" class="headerlink" title="基于距离的可分性判据"></a>基于距离的可分性判据</h2><p>基于距离的可分性判据的实质是<code>Fisher准则</code>的延伸，即综合考虑不同类样本的<code>类内聚集程度</code>与<code>类间离散程度</code>这两个因素。<br>判据的优化体现出降维特征空间较好地体现类内密集。一些不能体现类间分隔开的特征很可能被排除掉了。<br>基于距离度量是常用来进行分类的重要依据，因为一般情况下同类物体在特征空间<code>呈聚类状态</code>，即从总体上说同类物体内各样本由于具有共性，因此类内样本间距离应比跨类样本间距离小。<br><code>Fisher准则</code>是以使类间距离尽可能大同时又保持类内距离较小这一种原理为基础的。同样在特征选择与特征提取中也使用类似的原理，这一类被称为基于距离的可分性判据。<br>为了度量类内、类间的距离，可用其他方法描述方法，即描述样本的离散程度的方法。</p>
<script type="math/tex; mode=display">
J_d(x)=\frac{1}{2}\sum_{i=1}^{c}{P_i}\sum_{i=1}^{c}{P_j}\frac{1}{n_in_j}\sum_{k=1}^{n_i}{}\sum_{l=1}^{n_j}{\delta(x_k^{(i)},x_l^{(j)})}</script><p>各类样本之间的距离越大，则类别可分性越大。因此，可以用各类样本之间的距离的平均值作为可分性准则：</p>
<script type="math/tex; mode=display">
J_d(x)=\sum_{i=1}^{c}{P_i}\left[ \frac{1}{n_i}\sum_{k=1}^{n_i}{\left( x_k^{(i)}-m_i \right)^T\left( x_k^{(i)}-m_i \right)+(m_i-m)^T(m_i-m)} \right]</script><p>即</p>
<script type="math/tex; mode=display">
\tilde{S_b}=\sum_{i=1}^{c}{P_i}(m_i-m)(m_i-m)^T \\
\tilde{S_w}=\sum_{i=1}^{c}{P_i}\frac{1}{n_i}\sum_{k=1}^{n_i}{\left( x_k^{(i)}-m_i \right)\left( x_k^{(i)}-m_i \right)^T} \\
J_d(x) = tr(\tilde{S_w}+\tilde{S_b})</script><p>优点：<br>定义直观、易于实现，因此比较常用。<br>缺点：<br>没有直接考虑样本的分布情况，很难在理论上建立起它们与分类错误率的联系，而且当两类样本的分布有重叠时，这些判据不能反映重叠的情况。</p>
<h2 id="基于概率分布的可分性判据"><a href="#基于概率分布的可分性判据" class="headerlink" title="基于概率分布的可分性判据"></a>基于概率分布的可分性判据</h2><p>显然不同类别在特征空间x中的分布要尽可能不一样，则分类就比较容易，通俗的讲，则不同类别在特征空间的不同区域聚集，则分类就容易，它们重迭的程度越低，越有别于分类。<br>为了考查在不同特征下两类样本概率分布的情况，定义了基于概率分布的可分性判据。<br>分布密度的交叠程度可用$p(X|\omega_1)$及$p(X|\omega_2)$这两个分布密度函数之间的距离$J_p$来度量，距离$J_p$有以下几个共同点：</p>
<ol>
<li>$J_p$是非负，即$J_p \leq 0$</li>
<li>当两类完全不交迭时$J_p$达到其最大值</li>
<li>当两类分布密度相同时，$J_p = 0$</li>
</ol>
<p>常用的概率距离度量：</p>
<ol>
<li>Bhattacharyya距离（巴氏距离）：<ul>
<li>$J<em>B=-\ln \int</em>{}^{}\left[ p(x|\omega_1)p(x|\omega_2) \right]^{1/2}dx$</li>
<li>显然，当$p(X|\omega_1)=p(X|\omega_2)$对所有X值成立时$J_B＝0$，而当两者完全不交迭时$J_B$无穷大。巴氏距离与错误率的上界有直接关系，因此$J_B$不仅用来对特征空间进行降维优化，而且也用来对分类器的错误率作出估计。</li>
</ul>
</li>
<li>Chernoff（切诺夫）界限：<ul>
<li>$J<em>C=-\ln \int</em>{}^{}p^s(x|\omega_1)p^{1-s}(x|\omega_2) dx$</li>
<li>其中S取$[0，1]$区间的一个参数，显然在$S=0.5$时就变为$J_B$式，因此$J_B$是$J_C$的一个特例。</li>
</ul>
</li>
<li>散度：区分$i，j$两类总的平均信息<ul>
<li>$w<em>i，w_j$对数似然比：$l</em>{ij}(x)=p(x|\omega_i)/p(x|\omega_j)$</li>
<li>对$w<em>i$类的可分信息：$I</em>{ij}(x)=E(l<em>{ij}(x)]=\int</em>{x}p(x|\omega_i)In(p(x|\omega_i)/p(x|\omega_j))dx$</li>
<li>对$w<em>j$类的可分信息：$I</em>{ji}(x)=E(l<em>{ji}(x)]=\int</em>{x}p(x|\omega_j)In(p(x|\omega_j)/p(x|\omega_i))dx$</li>
<li>散度$J<em>d$为两类平均可分信息之和：$J_D{x}=I</em>{ij}+I<em>{ji}=\int</em>{x}[p(x|\omega_i)-p(x|\omega_j)In(p(x|\omega_i)/p(x|\omega_j))dx$</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>模式识别</category>
      </categories>
      <tags>
        <tag>模式识别</tag>
      </tags>
  </entry>
  <entry>
    <title>博弈论</title>
    <url>/2018/11/24/24.game/</url>
    <content><![CDATA[<h1 id="纳什均衡"><a href="#纳什均衡" class="headerlink" title="纳什均衡"></a>纳什均衡</h1><p>纳什均衡指的是这样一种策略组合，即在给定别人策略的情况下，没有人愿意单方面改变自己的策略，从而打破这种均衡。<br>数学表示：</p>
<p>// 公式太长了，下次补上</p>
<p>纳什均衡的一致预测性质:<br>一致预测：<br>如果所有博弈方都预测一个特定博弈结果会出现，所有博弈方都不会选择与预测结果不一致的策略，即没有哪个博弈方有偏离这个预测结果的愿望，因此预测结果会成为博弈的最终结果。<br>只有纳什均衡才具有一致预测的性质。一致预测性是纳什均衡的本质属性。<br>一致预测并不意味着一定能准确预测，因为有多重均衡， 预测不一致的可能。</p>
<a id="more"></a>
<h1 id="什么是博弈论"><a href="#什么是博弈论" class="headerlink" title="什么是博弈论"></a>什么是博弈论</h1><p>博弈论又称对策论，它使用严谨的数学模型研究 冲突对抗条件下的最优决策问题、是研究竞争的 逻辑和规律的数学分支。</p>
<p>简单的说，博弈论是研究决策主体在给定信息结 构条件下，如何决策以使自己的效用最大化，以及不同决策主体之间决策的均衡。</p>
<h1 id="博弈的要素"><a href="#博弈的要素" class="headerlink" title="博弈的要素"></a>博弈的要素</h1><h2 id="参与人（玩家-Player-Agent）"><a href="#参与人（玩家-Player-Agent）" class="headerlink" title="参与人（玩家, Player, Agent）:"></a>参与人（玩家, Player, Agent）:</h2><p>是博弈的参与主体，他通过选择自己的行动 （策略）来使自己的 收益最大化。参与人可以是 自然人，企业，国家等。在基于博弈的优化算法中，参与人代表算法中的某个元素。</p>
<h2 id="参与人的策略集："><a href="#参与人的策略集：" class="headerlink" title="参与人的策略集："></a>参与人的策略集：</h2><p>是参与人可供选择的策略的集合。比如囚徒困境中的欺骗和合作。</p>
<h2 id="收益-（效用，支付）："><a href="#收益-（效用，支付）：" class="headerlink" title="收益 （效用，支付）："></a>收益 （效用，支付）：</h2><p>在给定的策略组合下，参与人得到的确定的效用水平，或是参与人得到的期望效用水平。收益由收益函数来计算。</p>
<h2 id="行动："><a href="#行动：" class="headerlink" title="行动："></a>行动：</h2><p>参与人在博弈的某个时间点的策略变量。</p>
<h2 id="行动的顺序："><a href="#行动的顺序：" class="headerlink" title="行动的顺序："></a>行动的顺序：</h2><p>在动态博弈中，行动的顺序对博弈的结果有重要影响。同样的策略集，行动的顺序不同，每个人的最优策略就不同，博弈的结果就不同。尤其在不完全信息博弈中，后行动者根据先行动者的行动来获取信息。</p>
<h2 id="信息"><a href="#信息" class="headerlink" title="信息"></a>信息</h2><p>参与人有关博弈的知识，特别是关于其它参与人的特征和行动的知识，以及自然的选择。<br>完美信息是指参与人对其它参与人的行动选择有准确的掌握。</p>
<h2 id="共同知识："><a href="#共同知识：" class="headerlink" title="共同知识："></a>共同知识：</h2><p>所有参与人知道的每一步的信息集。</p>
<h2 id="策略（战略）"><a href="#策略（战略）" class="headerlink" title="策略（战略）"></a>策略（战略）</h2><ul>
<li>参与人在给定信息集情况下的行动规则，它规定参与人在 什么情况下选择什么行动。</li>
<li>策略与行动：策略是行动的规则而不是行动本身。</li>
<li>在静态博弈中，策略和行动是相同的。</li>
<li>策略必须是完备的：必须给出参与人在每一种可能的情况下的行动选择。</li>
</ul>
<h2 id="均衡："><a href="#均衡：" class="headerlink" title="均衡："></a>均衡：</h2><p>指所有参与人的最优策略组合</p>
<h1 id="基于博弈的控制"><a href="#基于博弈的控制" class="headerlink" title="基于博弈的控制"></a>基于博弈的控制</h1><ol>
<li>合作博弈与非合作博弈 </li>
<li>完全信息博弈与不完全信息博弈 </li>
<li>静态博弈与动态博弈<br>相互组合：完全信息静态博弈、完全信息动态博弈、不完全信息静态博弈、不完全信息动态博弈。</li>
<li>纯策略博弈与混合策略博弈</li>
</ol>
<p>特征函数。。。。。。。。。。。。。</p>
<h2 id="合作博弈"><a href="#合作博弈" class="headerlink" title="合作博弈"></a>合作博弈</h2><p>合作博弈亦称为正和博弈，是指博弈双方的利益都有所增加，或者至少是一方的利益增加，而另一方的利益不受损害，因而整个集体的利益有所<br>增加。<br>合作博弈研究人们达成合作时如何分配合作得到的收益，即收益分配问题。合作博弈采取的是一种合作的方式，或者说是一种妥协。<br>至于收益在博弈各方之间如何分配，取决于博弈各方的力量对比和技巧运用。因此，妥协必须经过博弈各方的讨价还价，达成共识，进行合作。</p>
<p>合作博弈存在的两个基本条件是：</p>
<ol>
<li>对联盟来说，整体收益大于其每个成员单 独经营时的收益之和。</li>
<li>对联盟内部而言，应存在具有帕累托改进 性质的分配规则，即每个成员都能获得比不加入联盟时多一些的收益。</li>
</ol>
<h2 id="非合作博弈"><a href="#非合作博弈" class="headerlink" title="非合作博弈"></a>非合作博弈</h2><p>在参与人利益相互冲突中如何选择策略使自己的收益最大，即策略选择问题。是一种不可能达成具有约束力的 协议的博弈类型。<br>零和博弈、负和博弈是非合作博弈。<br>囚徒困境是非合作博弈。</p>
<h2 id="完全信息博弈："><a href="#完全信息博弈：" class="headerlink" title="完全信息博弈："></a>完全信息博弈：</h2><p>是指每一参与者都拥有所有其他参与者的特征、策略及收益函数等方面的准确信息的博弈。<br>不完全信息博弈，参与人并不完全清楚有关博弈的一些信息。</p>
<h2 id="静态博弈："><a href="#静态博弈：" class="headerlink" title="静态博弈："></a>静态博弈：</h2><p>是指博弈中参与者同时采取行动，或者尽管参与者行动的采取有先后顺序，但后行动的人不知道先采取行动的人采取的是什么行动。</p>
<h2 id="动态博弈："><a href="#动态博弈：" class="headerlink" title="动态博弈："></a>动态博弈：</h2><p>动态博弈(dynamic game)是指参与人的行动有先后顺序，而且行动在后者可以观察到行动在先者的选择，并据此作出相应的选择。<br>不同的参与人在不同时间点行动，先行动者的选择影响后行动者的选择空间，后行动者可以观察到先行动者做了什么选择，因此，为了做最优的行动选择，每个参与人都必须这样思考问题：如果我如此选择，对方将如何应对？如果我是他，我将会如何行动？给定他的应对，什么是我的最优选择？</p>
<p>动态博弈的困难在于，在前一刻最优的决策在下一刻可能不再为最优，因此在求解上发生很大的困难。</p>
<ul>
<li>动态博弈的先动优势与后动优势</li>
<li>在动态博弈中，行动总 有先后顺序。有些博弈具有先动优势(first-mover advantage), 但有些博弈具有后动优势(second-mover advantage)。例如日常我们所说的：“先下手为强，后下手遭殃”；“捷足先登”；“后发制人”、“后发优势”，等等。</li>
</ul>
<h2 id="纯策略博弈与混合策略博弈"><a href="#纯策略博弈与混合策略博弈" class="headerlink" title="纯策略博弈与混合策略博弈"></a>纯策略博弈与混合策略博弈</h2><p>在完全信息博弈中，如果在每个给定信息下，只能选择一种特定策略，这个策略为纯策略（purestrategy）。<br>如果在每个给定信息下只以某种概率选择不同策略，称为混合策略（mixed strategy）。混合策略是纯策略在空间上的概率分布，纯策略是混合策略的特例。纯策略的收益可以用效用表示，混合策略的收益只能以期望效用表示。</p>
<hr>
<h1 id="多重均衡与优化"><a href="#多重均衡与优化" class="headerlink" title="多重均衡与优化"></a>多重均衡与优化</h1><ol>
<li>占优策略与智猪博弈</li>
<li>博弈的多重纳什均衡</li>
<li>帕累托最优均衡</li>
<li>帕累托最优均衡与纳什均衡的关系</li>
<li>如何得到帕累托最优均衡 </li>
</ol>
<h2 id="占优策略与智猪博弈"><a href="#占优策略与智猪博弈" class="headerlink" title="占优策略与智猪博弈"></a>占优策略与智猪博弈</h2><p>在参与人各自的策略集中，如果存在一个与其他竞争对手可能采取的策略无关的最优选择，则称其为占优策略(Dominant Strategy)，与之相对的其他策略则为劣势策略。占优策略是博弈论（game theory）中的专业术语, 所谓的占优策略就是指无论博弈对手如何行动都属于本人最佳选择的策略。</p>
<h3 id="占优策略和纳什均衡的比较"><a href="#占优策略和纳什均衡的比较" class="headerlink" title="占优策略和纳什均衡的比较"></a>占优策略和纳什均衡的比较</h3><ul>
<li>占优策略:“不管你怎么做，我所做的都是我能做得最好的。”</li>
<li>纳什均衡:<ul>
<li>“给定你的做法后，我所做的是我能做得最好的。”</li>
<li>如果你有占优策略, 你可以使用此策略, 以不变应万变;</li>
<li>如果你没有占优策略, 你必须随机应变。在达到了纳什均衡之后, 所有参与者都没有动机想再变了。</li>
</ul>
</li>
</ul>
<p>许多博弈可能有多个纳什均衡。</p>
]]></content>
  </entry>
  <entry>
    <title>矩阵分解</title>
    <url>/2019/01/21/25.matrix/</url>
    <content><![CDATA[<p>开个专题，研究矩阵分解的方法。</p>
<a id="more"></a>
<h1 id="Cholesky分解"><a href="#Cholesky分解" class="headerlink" title="Cholesky分解"></a>Cholesky分解</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JTlGJTgzJUU1JUIwJTk0JUU3JUIxJUIzJUU3JTg5JUI5JUU3JTlGJUE5JUU5JTk4JUI1">Hermitian matrix（埃尔米特矩阵，厄米矩阵，自伴随矩阵）<i class="fa fa-external-link-alt"></i></span>，是共轭对称的方阵。</p>
<script type="math/tex; mode=display">\left[\begin{matrix} 3 & 2+i \\ 2-i & 1 \end{matrix}\right]</script><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU2JUFEJUEzJUU1JUFFJTlBJUU3JTlGJUE5JUU5JTk4JUI1">正定矩阵<i class="fa fa-external-link-alt"></i></span>是Hermitian matrix的一种。<br>设A是一个n阶厄米特正定矩阵(Hermitian positive-definite matrix)。Cholesky分解的目标是：<br>设$A = LL^{T}$，得到：（其中$A<em>{21}$是一个列向量，$A</em>{22}$是一个n-1阶的方阵）</p>
<script type="math/tex; mode=display">\left[ \begin{matrix} a_{11}&A_{21}^{T}\\ A_{21}&A_{22}\\ \end{matrix} \right] =  \left[ \begin{matrix} l_{11}&0\\ L_{21}&L_{22}\\ \end{matrix} \right] \left[ \begin{matrix} l_{11}&L_{21}^{T}\\ 0&L_{22}^{T}\\ \end{matrix} \right]=  \left[ \begin{matrix} l_{11}^{2}&l_{11}L_{21}^{T}\\ l_{11}L_{21}&L_{21}L_{21}^{T}+L_{22}L_{22}^{T}\\ \end{matrix} \right]</script><p>其中，未知量$l<em>{11},L</em>{21},L_{22}$，这3个未知量的求解公式是：</p>
<script type="math/tex; mode=display">l_{11} = \sqrt {a_{11}}，L_{21} = \frac {1}{l_{11}}A_{21}，L_{22}L_{22}^{T} =  A_{22} - L_{21}L_{21}^{T}</script><p>设$A<em>{22}’ = A</em>{22} - L<em>{21}L</em>{21}^{T}$，则化简为$A<em>{22}’ = L</em>{22}L_{22}^{T}$，可以继续Cholesky分解，被分解的矩阵是A的右下角的n-1阶子方阵。所以这个算法具有递归性质。</p>
<p>举个例子：</p>
<script type="math/tex; mode=display">A =  \left[ \begin{matrix} 25&15&-5\\ 15&18&0\\ -5&0&11\\ \end{matrix} \right] =  \left[ \begin{matrix} l_{11}&0&0\\ l_{21}&l_{22}&0\\ l_{31}&l_{32}&l_{33}\\ \end{matrix} \right] \left[ \begin{matrix} l_{11}&l_{21}&l_{31}\\ 0&l_{22}&l_{32}\\ 0&0&l_{33}\\ \end{matrix} \right]</script><p>根据公式，有：</p>
<script type="math/tex; mode=display">l_{11} = \sqrt { a_{11} } = 5</script><script type="math/tex; mode=display">L_{21} = \frac {1}{l_{11}}A_{21} = \frac {1}{5} \left[ \begin{matrix} 15\\ -5\\ \end{matrix} \right] =  \left[ \begin{matrix} 3\\ -1\\ \end{matrix} \right]</script><script type="math/tex; mode=display">A_{22} - L_{21}L_{21}^{T}  =  L_{22}L_{22}^{T}</script><script type="math/tex; mode=display">A_{22} - L_{21}L_{21}^{T}  =  L_{22}L_{22}^{T}</script><script type="math/tex; mode=display">\left[ \begin{matrix} 18&0\\ 0&11\\ \end{matrix} \right] -  \left[ \begin{matrix} 3\\ -1\\ \end{matrix} \right] \left[ \begin{matrix} 3&-1\\ \end{matrix} \right] =   \left[ \begin{matrix} l_{22}&0\\ l_{32}&l_{33}\\ \end{matrix} \right]   \left[ \begin{matrix} l_{22}&l_{32}\\ 0&l_{33}\\ \end{matrix} \right]</script><script type="math/tex; mode=display">\left[ \begin{matrix} 9&3\\ 3&10\\ \end{matrix} \right] =  \left[ \begin{matrix} l_{22}&0\\ l_{32}&l_{33}\\ \end{matrix} \right]   \left[ \begin{matrix} l_{22}&l_{32}\\ 0&l_{33}\\ \end{matrix} \right]</script><p>(注意，这里已经是n-1阶的Cholesky分解)</p>
<script type="math/tex; mode=display">l_{22} = \sqrt { 9 } = 3</script><script type="math/tex; mode=display">l_{32} = \frac {1}{3}3 = 1</script><script type="math/tex; mode=display">10 = l_{32}^{2} + l_{33}^{2} = 1 + l_{33}^{2}</script><script type="math/tex; mode=display">l_{33} = \sqrt {10 - 1} = 3</script><p>综上</p>
<script type="math/tex; mode=display">A =  \left[ \begin{matrix} 25&15&-5\\ 15&18&0\\ -5&0&11\\ \end{matrix} \right] =  \left[ \begin{matrix} 5&0&0\\ 3&3&0\\ -1&1&3\\ \end{matrix} \right] \left[ \begin{matrix} 5&3&-1\\ 0&3&1\\ 0&0&3\\ \end{matrix} \right]</script><p>对矩阵的Cholesky分解，就像对实数的求平方根。根据协方差矩阵的定义，Cholesky分解可以反求期望。</p>
<h1 id="特征分解"><a href="#特征分解" class="headerlink" title="特征分解"></a>特征分解</h1><p>N维非零向量$v$是N×N的矩阵$A$的特征向量，当且仅当下式成立：</p>
<script type="math/tex; mode=display">
Av=\lambda v</script><p>其中$λ$为一标量，称为$v$对应的特征值。也称$v$为特征值$λ$对应的特征向量。也即特征向量被施以线性变换$A$只会使向量伸长或缩短而其方向不被改变。<br>由上式可得</p>
<script type="math/tex; mode=display">
p(\lambda):=det(A-\lambda I)=0.</script><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JThGJUFGJUU1JUFGJUI5JUU4JUE3JTkyJUU1JThDJTk2JUU3JTlGJUE5JUU5JTk4JUI1">可对角化矩阵<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU4JThCJUE1JUU1JUIwJTk0JUU1JUJEJTkzJUU2JUEwJTg3JUU1JTg3JTg2JUU1JTlFJThC">若尔当标准型<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUI4JThDJUU1JUIwJTk0JUU0JUJDJUFGJUU3JTg5JUI5JUU3JUE5JUJBJUU5JTk3JUI0">希尔伯特空间<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU3JTg5JUI5JUU1JUJFJTgxJUU1JTg4JTg2JUU4JUE3JUEz">特征分解<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUE1JTg3JUU1JUJDJTgyJUU1JTgwJUJDJUU1JTg4JTg2JUU4JUE3JUEz">奇异值分解<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>无约束最优化方法</title>
    <url>/2019/02/01/27.optimize1/</url>
    <content><![CDATA[<p>约束规划问题一般形式：</p>
<script type="math/tex; mode=display">
\begin{split}
&\min f(x),&\quad x\in \mathbb{R}^{n} \\ 
&\text{s. t. }&c_{i}(x)=0,\quad i\in E=\{1,2,\dots, l\} \\ 
            & &c_{i}(x)\leq 0, \quad i\in I=\{l+1,l+2,\dots, l+m\} 
\end{split}</script><a id="more"></a>
<h1 id="局部解的必要条件"><a href="#局部解的必要条件" class="headerlink" title="局部解的必要条件"></a>局部解的必要条件</h1><h2 id="一阶必要条件"><a href="#一阶必要条件" class="headerlink" title="一阶必要条件"></a>一阶必要条件</h2><p>考虑上述约束规划问题，这里我们假设$f(x),c_i(x),(i=1,2,…,l+m)$是连续可微函数。我们引进Lagrange函数： </p>
<script type="math/tex; mode=display">L(x,\lambda)=f(x)+\sum_{i=1}^{l+m}\lambda_{i}c_{i}(x)</script><p>设约束问题中 $f(x),c<em>i(x),(i=1,2,…,l+m)$具有连续可微的一阶偏导数，若$x^∗$是该约束问题的局部解，在$x^∗$处存在$$\lambda^*=(\lambda</em>{1}^<em>, \lambda_{2}^</em>,\dots, \lambda_{l+m}^*)^T$$使得： </p>
<script type="math/tex; mode=display">\nabla_{x}L(x^{*}, \lambda^{*})=\nabla f(x^{*})+\sum_{i=1}^{l+m}\lambda_{i}^{*}\nabla c_{i}(x^{*})=0</script><p>其中</p>
<script type="math/tex; mode=display">\begin{gather} c_{i}(x^{*})=0, i\in E=\{1,2,\dots,l\} \\ c_{i}(x^{*})\leq 0, i\in I=\{l+1,l+2,\dots,l+m\} \\ \lambda_{i}^{*}\geq 0, i\in I=\{l+1,l+2,\dots,l+m\} \\ \lambda_{i}^{*}c_{i}(x^{*})=0, i\in I=\{l+1,l+2,\dots,l+m\} \end{gather}</script><p>上述一阶必要条件被称为<strong>Kuhn-Tucker条件</strong>，或简称K-T条件；满足上式的点为<strong>K-T点</strong>；称$λ^∗$为$x^∗$处的Lagrange乘子（Lagrange Multiplier）.</p>
<h2 id="约束限制条件成立的充分条件"><a href="#约束限制条件成立的充分条件" class="headerlink" title="约束限制条件成立的充分条件"></a>约束限制条件成立的充分条件</h2><p>若在前述约束优化问题的局部解$x∗$处下述两条件之一成立：</p>
<script type="math/tex; mode=display">c_{i}(x), i\in E\cup I(x^{*})</script><p>则在 $x∗$ 处约束限制条件成立。此时必存在 $λ^∗$ 使得K-T条件成立。</p>
<h1 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h1><p>区间$[a,b]$上定义的函数$f$，若它对区间中任意两点$x1,x2$均有：</p>
<script type="math/tex; mode=display">f(\frac {x_1+x_2}{2}) \leq \frac{f(x_1)+f(x_2)}{2}</script><p>对实数集上的函数，可通过求解二阶导数来判别：</p>
<ul>
<li>若二阶导数在区间上非负，则称为凸函数</li>
<li>若二阶导数在区间上恒大于0，则称严格凸函数</li>
</ul>
<p>仿射函数也是凸函数，只是不是严格凸函数。</p>
<h1 id="凸优化问题"><a href="#凸优化问题" class="headerlink" title="凸优化问题"></a>凸优化问题</h1><p>凸优化问题是特殊的约束最优化问题。其一般形式形式和约束最优化问题一样。</p>
<p>假设f、g、h在定义域内是连续可微的，且目标函数f和不等式约束函数g是凸函数，等式约束h是仿射函数（线性函数），则这种约束最优化问题称为凸优化问题。<br>因此凸优化问题特征的重要特征：</p>
<ul>
<li>目标函数f，不等式约束函数g是凸函数</li>
<li>等式约束h是仿射函数</li>
<li>满足约束最优化问题的一般形式</li>
</ul>
<h1 id="凸二次规划问题"><a href="#凸二次规划问题" class="headerlink" title="凸二次规划问题"></a>凸二次规划问题</h1><p>凸二次规划问题是凸优化问题的一个特殊形式，当目标函数是二次型函数且约束函数 g 是仿射函数时，就变成一个凸二次规划问题。凸二次规划问题的一般形式为</p>
<script type="math/tex; mode=display">
\begin{matrix}
\min_{x}  &\frac{1}{2}x^TQx+c^Tx\\ 
  s.t.&Wx \leqslant b 
\end{matrix}</script><p>若 Q 为半正定矩阵，则上面的目标函数是凸函数，相应的二次规划为凸二次规划问题；此时若约束条件定义的可行域不为空，且目标函数在此可行域有下界，则该问题有全局最小值。</p>
<p>若Q为正定矩阵，则该问题有唯一的全局最小值。<br>例如，最简单的正定矩阵就是单位矩阵。</p>
<p>凸二次规划问题的特征：</p>
<ul>
<li>目标函数f是二次型函数函数</li>
<li>等式约束h是仿射函数</li>
<li>等式约g是仿射函数</li>
<li>满足约束最优化问题的一般形式</li>
</ul>
<p>常用的二次规划问题求解方法有：</p>
<ul>
<li>椭球法</li>
<li>内点法</li>
<li>增广拉格朗日法</li>
<li>梯度投影法</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><h2 id="概述性文章"><a href="#概述性文章" class="headerlink" title="概述性文章"></a>概述性文章</h2><p>最优化理论与凸优化到底是干嘛的？<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NDIyNjQyL2FydGljbGUvZGV0YWlscy83ODgxNjYzNw==">https://blog.csdn.net/qq_39422642/article/details/78816637<i class="fa fa-external-link-alt"></i></span><br>二次规划 <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbG9uZzExNzE5NC9hcnRpY2xlL2RldGFpbHMvNzgyMDQ5OTQ=">https://blog.csdn.net/lilong117194/article/details/78204994<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="按照顺序"><a href="#按照顺序" class="headerlink" title="按照顺序"></a>按照顺序</h2><p>凸优化基础简述：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BoaWx0aGlua2VyL2FydGljbGUvZGV0YWlscy83ODAyMzA4NQ==">https://blog.csdn.net/philthinker/article/details/78023085<i class="fa fa-external-link-alt"></i></span><br>无约束最优化问题的一般结构与规划方法：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BoaWx0aGlua2VyL2FydGljbGUvZGV0YWlscy83ODE5MTg2NA==">https://blog.csdn.net/philthinker/article/details/78191864<i class="fa fa-external-link-alt"></i></span><br>约束规划问题与凸二次规划：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BoaWx0aGlua2VyL2FydGljbGUvZGV0YWlscy83ODUxMDM2MQ==">https://blog.csdn.net/philthinker/article/details/78510361<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="这些基础铺垫比较全"><a href="#这些基础铺垫比较全" class="headerlink" title="这些基础铺垫比较全"></a>这些基础铺垫比较全</h2><p>无约束最优化方法：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xhbnNhdGlhbmtvbmd4eGMvYXJ0aWNsZS9kZXRhaWxzLzQ1ODczNTk3">https://blog.csdn.net/lansatiankongxxc/article/details/45873597<i class="fa fa-external-link-alt"></i></span><br>凸优化-对偶问题：<span class="exturl" data-url="aHR0cDovL3d3dy5oYW5sb25nZmVpLmNvbS9jb252ZXgvMjAxNS8xMS8wNS9kdWFsaXR5Lw==">http://www.hanlongfei.com/convex/2015/11/05/duality/<i class="fa fa-external-link-alt"></i></span><br>[x]凸优化问题，凸二次规划问题QP，凸函数：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Byb21pc2VqaWEvYXJ0aWNsZS9kZXRhaWxzLzgxMjQxMjAx">https://blog.csdn.net/promisejia/article/details/81241201<i class="fa fa-external-link-alt"></i></span><br>数学优化入门：凸优化: <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpcGVuZ2NuL2FydGljbGUvZGV0YWlscy81MjgxNTY5MA==">https://blog.csdn.net/lipengcn/article/details/52815690<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="涉及到锥"><a href="#涉及到锥" class="headerlink" title="涉及到锥"></a>涉及到锥</h2><p>Farkas引理的几何意义 <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yOTUyNTUxMw==">https://zhuanlan.zhihu.com/p/29525513<i class="fa fa-external-link-alt"></i></span><br>一步一步走向锥规划 - QP：<span class="exturl" data-url="aHR0cHM6Ly93d3cuamlhbnNodS5jb20vcC9mZmUyMzliMTI0YzE=">https://www.jianshu.com/p/ffe239b124c1<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="涉及到模式识别"><a href="#涉及到模式识别" class="headerlink" title="涉及到模式识别"></a>涉及到模式识别</h2><p>凸优化问题实例：LASSO：<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x5ajIwMTQyMTE2MjYvYXJ0aWNsZS9kZXRhaWxzLzc5MTMzMTQ1">https://blog.csdn.net/lyj2014211626/article/details/79133145<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>优化理论</tag>
      </tags>
  </entry>
  <entry>
    <title>MLE和MAP</title>
    <url>/2019/02/03/28.MLE%E5%92%8CMAP/</url>
    <content><![CDATA[<p>频率学派 - Frequentist - Maximum Likelihood Estimation (MLE，最大似然估计)<br>贝叶斯学派 - Bayesian - Maximum A Posteriori (MAP，最大后验估计)</p>
<p>频率学派和贝叶斯学派对世界的认知有本质不同：<br>频率学派认为世界是确定的，有一个本体，这个本体的真值是不变的，我们的目标就是要找到这个真值或真值所在的范围；<br>而贝叶斯学派认为世界是不确定的，人们对世界先有一个预判，而后通过观测数据对这个预判做调整，我们的目标是要找到最优的描述这个世界的概率分布。</p>
<a id="more"></a>
<h1 id="概率与统计的区别"><a href="#概率与统计的区别" class="headerlink" title="概率与统计的区别"></a>概率与统计的区别</h1><p>概率（probabilty）和统计（statistics）研究的问题刚好相反。</p>
<p>概率：已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。<br>统计：已知有一堆数据，要利用这堆数据去预测模型（什么分布）和参数（分布模型的参数）。</p>
<p>最大似然估计MLE，最大后验估计MAP，贝叶斯估计来说Bayes，都属于统计的范畴。</p>
<h1 id="参数估计的方法"><a href="#参数估计的方法" class="headerlink" title="参数估计的方法"></a>参数估计的方法</h1><p>点估计就是用样本统计量的某一具体数值直接推断未知的总体参数。总体参数进行点估计常用的方法有两种：矩估计与最大似然估计，其中最大似然估计就是我们实际中使用非常广泛的一种方法。 </p>
<p>按这两种方法对总体参数进行点估计，能够得到相对准确的结果。如用样本均值X估计总体均值，或者用样本标准差S估计总体标准差σ。 </p>
<p>但是，点估计有一个不足之处，即这种估计方法不能提供估计参数的估计误差大小。对于一个总体来说，它的总体参数是一个常数值，而它的样本统计量却是随机变量。当用随机变量去估计常数值时，误差是不可避免的，只用一个样本数值去估计总体参数是要冒很大风险的。因为这种误差风险的存在，并且风险的大小还未知，所以，点估计主要为许多定性研究提供一定的参考数据，或在对总体参数要求不精确时使用，而在需要用精确总体参数的数据进行决策时则很少使用。</p>
<p>区间估计就是在推断总体参数时，还要根据统计量的抽样分布特征，估计出总体参数的一个区间，而不是一个数值，并同时给出总体参数落在这一区间的可能性大小，概率的保证。</p>
<h1 id="什么是似然"><a href="#什么是似然" class="headerlink" title="什么是似然"></a>什么是似然</h1><p>The likelihood of something happening is how likely it is to happen.</p>
<script type="math/tex; mode=display">P(x|\theta)</script><p>输入有两个：$x$表示某一个具体的数据；$\theta$表示模型的参数。</p>
<ul>
<li>如果$\theta$是已知确定的，$x$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点$x$，其出现概率是多少。</li>
<li>如果$x$是已知确定的，$\theta$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现$x$这个样本点的概率是多少。</li>
</ul>
<h1 id="最大似然估计（MLE）"><a href="#最大似然估计（MLE）" class="headerlink" title="最大似然估计（MLE）"></a>最大似然估计（MLE）</h1><p>设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。<br>于是我们拿这枚硬币抛了10次，得到的数据是：反正正正正反正正正反。我们想求的正面概率θθ是模型参数，而抛硬币模型我们可以假设是二项分布。<br>这是个只关于$θ$的函数。而最大似然估计，顾名思义，就是要最大化这个函数。</p>
<script type="math/tex; mode=display">\displaystyle {\hat {\theta }}_{\mathrm {ML} }(x)=\arg \max _{\theta }f(x|\theta )\!</script><h1 id="最大后验概率估计（MAP）"><a href="#最大后验概率估计（MAP）" class="headerlink" title="最大后验概率估计（MAP）"></a>最大后验概率估计（MAP）</h1><p>最大似然估计是求参数$θ$, 使似然函数$P(x_0|θ)$最大。最大后验概率估计则是想求$θ$使$P(x_0|θ)P(θ)$最大。求得的$θ$不单单让似然函数大，$θ$自己出现的先验概率也得大。参数$θ$有一个先验概率。</p>
<p>这个时候就用到了我们的最大后验概率MAP，基础是贝叶斯公式： </p>
<script type="math/tex; mode=display">P(\theta|x_0) = \frac{P(x_0|\theta)P(\theta)}{P(x_0)}</script><p>其中，$p(x|θ)$就是之前讲的似然函数，$p(θ)$是先验概率，是指在没有任何实验数据的时候对参数$θ$的经验判断，对于一个硬币，大概率认为他是正常的，正面的概率为0.5的可能性最大。</p>
<p>MAP优化的就是一个后验概率，即给定了观测值以后使后验概率最大： </p>
<script type="math/tex; mode=display">
\begin{align*}
{\hat {\theta }}_{\mathrm {MAP} }  & = \arg \max _{\theta } p(\theta|x) \\
 & = \arg \max _{\theta } \frac{p(x|\theta)\times p(\theta)}{P(x)} \\
 & =  \arg \max _{\theta } p(x|\theta)\times p(\theta)
\end{align*}</script><p>从上面公式可以看出，$p(x|θ)$是似然函数，而$p(θ)$就是先验概率。对其取对数： </p>
<script type="math/tex; mode=display">
\begin{align*}
\arg \max _{\theta } p(x|\theta) \cdot p(\theta) &= \arg \max _{\theta } log \prod_{i=0}^n p(x_i | \theta) p(\theta) \\
& = \arg \max _{\theta } \sum_i log (p(x_i | \theta) p(\theta)) \\ 
& = \arg \max _{\theta } \sum_ilog (p(x_i | \theta) + log(p(\theta))
\end{align*}</script><h1 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h1><p>贝叶斯估计是在MAP上做进一步拓展，此时不直接估计参数的值，而是允许参数服从一定概率分布。回忆下贝叶斯公式： </p>
<script type="math/tex; mode=display">p(\theta|x) = \frac{p(x|\theta)\times p(\theta)}{P(x)}</script><p>现在我们不要求后验概率最大，这个时候就需要求$p(X)$，即观察到的$X$的概率。一般来说，用全概率公式可以求$p(X)$</p>
<script type="math/tex; mode=display">p(X) = \int p(X | \theta)p(\theta)d\theta</script><p>那么如何用贝叶斯估计来预测呢？如果我们想求一个值 $x’$ 的概率，可以用下面的方法 </p>
<script type="math/tex; mode=display">
p(\hat{x}|X)=\int_{\theta\in\Theta}p(\hat{x}|\theta)p(\theta|X)d\theta=\int_{\theta\in\Theta}\frac{p(\hat{x}|\theta)p(\theta)}{p(X)}d\theta</script><h1 id="MLE和MAP的区别"><a href="#MLE和MAP的区别" class="headerlink" title="MLE和MAP的区别"></a>MLE和MAP的区别</h1><p>当先验分布均匀之时，MAP与MLE相等。直观讲，它表征了最有可能值的任何先验知识的匮乏。在这一情况中，所有权重分配到似然函数，因此当我们把先验与似然相乘，由此得到的后验极其类似于似然。因此，最大似然方法可被看作一种特殊的MAP。</p>
<p>如果先验认为这个硬币是概率是均匀分布的，被称为无信息先验( non-informative prior )，通俗的说就是“让数据自己说话”，此时贝叶斯方法等同于频率方法。<br>随着数据的增加，先验的作用越来越弱，数据的作用越来越强，参数的分布会向着最大似然估计靠拢。而且可以证明，最大后验估计的结果是先验和最大似然估计的凸组合。</p>
<p>重要结论：随着我们观测到越来越多的数据，我们从数据中获取的信息的置信度是越高的，MAP估计逐步逼近MLE，先验信息占的比重将会越来越少。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><h3 id="这两篇博客把MLE和MAP的原理讲得很清楚："><a href="#这两篇博客把MLE和MAP的原理讲得很清楚：" class="headerlink" title="这两篇博客把MLE和MAP的原理讲得很清楚："></a>这两篇博客把MLE和MAP的原理讲得很清楚：</h3><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE1MDg2NDAvYXJ0aWNsZS9kZXRhaWxzLzcyODE1OTgx">详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdGNhcm1hbmxlZS9hcnRpY2xlL2RldGFpbHMvODE0MTcxNTE=">最大似然估计，最大后验估计，贝叶斯估计联系与区别<i class="fa fa-external-link-alt"></i></span></p>
<h3 id="这篇文章还讲了贝叶斯："><a href="#这篇文章还讲了贝叶斯：" class="headerlink" title="这篇文章还讲了贝叶斯："></a>这篇文章还讲了贝叶斯：</h3><p>机器学习中的MLE、MAP、贝叶斯估计：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNzIxNTI3Ng==">https://zhuanlan.zhihu.com/p/37215276<i class="fa fa-external-link-alt"></i></span></p>
<h3 id="这篇文章把频率学派和贝叶斯学派讲的很清楚："><a href="#这篇文章把频率学派和贝叶斯学派讲的很清楚：" class="headerlink" title="这篇文章把频率学派和贝叶斯学派讲的很清楚："></a>这篇文章把频率学派和贝叶斯学派讲的很清楚：</h3><p>聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMjQ4MDgxMA==">https://zhuanlan.zhihu.com/p/32480810<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title>Particle Filter</title>
    <url>/2019/02/03/29.%E7%B2%92%E5%AD%90%E6%BB%A4%E6%B3%A2/</url>
    <content><![CDATA[<p>这篇文章数学公式推得有点儿长。。。。提前预警= =</p>
<a id="more"></a>
<h1 id="贝叶斯滤波"><a href="#贝叶斯滤波" class="headerlink" title="贝叶斯滤波"></a>贝叶斯滤波</h1><p>假设有一个系统:</p>
<ul>
<li>状态方程：$x<em>k=f_k(x</em>{k-1},v_{k-1})$，x为系统状态，v为过程噪声;</li>
<li>测量方程：$y_k=h_k(x_k,n_k)$，y为测量数据，n为测量噪声。</li>
</ul>
<p>从贝叶斯理论的观点来看，状态估计问题（目标跟踪、信号滤波）就是根据之前一系列的已有数据$y<em>{1:k}$<code>（后验知识）??????????</code>递推的计算出当前状态$x_k$的可信度。这个可信度就是概率公式$p(x_k|y</em>{1:k})$，它需要通过<code>预测</code>和<code>更新</code>两个步骤来递推的计算。</p>
<ul>
<li>预测：利用系统模型预测状态的<code>先验概率密度</code>，即用先验知识对未来的状态进行猜测，$p(x<em>k|x</em>{k-1})$;</li>
<li>更新：利用最新的测量值对<code>先验概率密度</code>进行修正，得到后验概率密度，也就是对之前的猜测进行修正。</li>
</ul>
<p>在处理这些问题时，一般都先假设系统的状态转移服从一阶马尔科夫模型，即当前时刻的状态$x<em>k$只与上一个时刻的状态$x</em>{k-1}$有关。</p>
<p>为了进行递推，不妨假设已知k-1时刻的概率密度函数$p(x<em>{k-1}|y</em>{1:k-1})$</p>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>由上一时刻的概率密度$p(x<em>{k-1}|y</em>{1:k-1})$得到$p(x<em>{k}|y</em>{1:k-1})$，即用前面1：k-1时刻的测量数据，预测状态$x_k$出现的概率。<br>推导：</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
p(x_{k}|y_{1:k-1})
&= \int p(x_k,x_{k-1}|y_{1:k-1})dx_{k-1}  \\
&= \int p(x_k|x_{k-1},y_{1:k-1})p(x_{k-1}|y_{1:k-1})dx_{k-1} &\ \ \ \ //\ 纯粹的贝叶斯推导 \\
&= \int p(x_k|x_{k-1})p(x_{k-1}|y_{1:k-1})dx_{k-1}  &\ \ \ \ //\ 一阶马尔科夫过程假设
\end{split} 
\end{equation}</script><p>如果没有噪声，$x<em>k$完全由$x</em>{k-1}$计算得到，也就没由概率分布这个概念了，由于出现了噪声，所以$x(k)$不好确定，因此才产生了概率。</p>
<h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>上一步还只是预测，这里又多了k时刻的测量，对上面的预测再进行修正，就是滤波了。这里的后验概率也将是代入到下次的预测，形成递推。<br>推导：</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
p(x_k|y_{1:k})
&= \frac{p(y_k|x_k,y_{1:k-1})p(x_k|y_{1:k-1})}{p(y_k|y_{1:k-1})} \\ \\
&= \frac{p(y_k|x_k)p(x_k|y_{1:k-1})}{p(y_k|y_{1:k-1})} & //\ y_k只与x_k有关，p(y_k|x_k)是似然函数
\end{split} 
\end{equation}</script><p>其中归一化常数</p>
<script type="math/tex; mode=display">
p(y_k|y_{1:k-1})=\int p(y_k|x_k)p(x_k|y_{1:k-1})dx_k</script><p>同理，观测数据也是常数，不确定性和概率产生于观测噪声。</p>
<h1 id="蒙特卡洛采样"><a href="#蒙特卡洛采样" class="headerlink" title="蒙特卡洛采样"></a>蒙特卡洛采样</h1><p>上面的推导过程中需要用到积分，这对于一般的非线性，非高斯系统，很难得到后验概率的解析解。为了解决这个问题，就得引进蒙特卡洛采样。</p>
<p>假设我们能从一个目标概率分布p(x)中采样到一系列的样本（粒子）$x_1,\dots,x_N$，（至于怎么生成服从$p(x)$分布的样本，这个问题先放一放），那么就能利用这些样本去估计这个分布的某些函数的期望值。</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
E[f(x)]   &= \int_a^b f(x)p(x)dx \\
Var[f(x)] &= E\{f(x)-E[f(x)]\}^2 = \int_a^b\{f(x)-E[f(x)]\}^2p(x)dx
\end{split} 
\end{equation}</script><p>上面的式子其实都是计算期望的问题，只是被积分的函数不同。蒙特卡洛采样的思想就是用平均值来代替积分，求期望：</p>
<script type="math/tex; mode=display">
E(f(x))\approx \frac{f(x_1)+...+f(x_N)}{N}</script><p>假设可以从后验概率中采样到N个样本，那么后验概率的计算可表示为：</p>
<script type="math/tex; mode=display">
\hat{p}(x_n|y_{1:k})=\frac{1}{n}\sum_{i=1}^{N}\delta(x_n-x_n^{(i)})\approx p(x_n|y_{1:k}) \ \ \ \ //\ 其中\delta(x_n-x_n^{(i)})是狄拉克函数</script><p>用这些采样的粒子的状态值直接平均就得到了期望值，也就是滤波后的值，这里的$f(x)$就是每个粒子的状态函数。这就是粒子滤波了。</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
E[f(x)]   
&\approx \int f(x_n)\hat{p}(x_n|y_{1:k})dx_n \\
&= \frac{1}{n}\sum_{i=1}{N}\int f(x_n)\delta(x_n-x_n^{(i)})dx_n \\
&= \frac{1}{n}\sum_{i=1}{N}f(x_n^{(i)})
\end{split} 
\end{equation}</script><h1 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h1><p>思路看似简单，但是要命的是后验概率不知道，所以这样直接去应用是行不通的，这时候得引入重要性采样这个方法来解决这个问题。</p>
<p>无法从目标分布中采样，就从一个已知的可以采样的分布里去采样如 $q(x|y)$，这样上面的求期望问题就变成了：</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
E[f(x_k)]
&=\int f(x_k)\frac{p(x_k|y_{1:k})}{q(x_k|y_{1:k})}q(x_k|y_{1:k})dx_k \\ \\

&=\int f(x_k)\frac{p(y_{1:k}|x_k)p(x)}{p(y_{1:k})q(x_k|y_{1:k})}q(x_k|y_{1:k})dx_k \\ \\

&=\int f(x_k)\frac{W_k(x_k)}{p(y_{1:k})}q(x_k|y_{1:k})dx_k 
& //\ W_k(x_k)=\frac{p(y_{1:k}|x_k)p(x_k)}{q(x_k|y_{1:k})}\propto\frac{p(x_k|y_{1:k})}{q(x_k|y_{1:k})}\\ \\

&=\frac{1}{p(y_{1:k})}\int f(x_k)W_k(x_k)q(x_k|y_{1:k})dx_k & //\ p(y_{1:k})=\int p(y_{1:k}|x_k)p(x_k)dx_k \\ \\

&=\frac{\int f(x_k)W_k(x_k)q(x_k|y_{1:k})dx_k}{\int p(y_{1:k}|x_k)p(x_k)dx_k} \\ \\

&=\frac{\int f(x_k)W_k(x_k)q(x_k|y_{1:k})dx_k}{\int W_k(x_k)q(x_k|y_{1:k})dx_k} \\ \\

&=\frac{E_{q(x_k|y_{1:k})}[W_k(x_k)f(x_k)]}{E_{q(x_k|y_{1:k})}[W_k(x_k)]} \\ \\

&\approx\frac{\frac{1}{N}\sum_{i=1}^{N}W_k(x_k^{(i)})f(x_k^{(i)})}{\frac{1}{N}\sum_{i=1}^{N}W_k(x_k^{(i)})f(x_k^{(i)})} 
& //\ Sampling\ N\ samples\ \left\{ x_k^{(i)} \right\}\sim q(x_k|y_{1:k}) \\ \\

&=\sum_{i=1}^N\tilde{W_k}(x_k^{(i)})f(x_k^{(i)})
& //\ \tilde{W_k}(x_k^{(i)})=\frac{W_k(x_k{(i)})}{\sum_{i=1}^{N}W_K(x_k^{(i)})}

\end{split} 
\end{equation}</script><p>到这里已经解决了不能从后验概率直接采样的问题，但是上面这种每个粒子的权重都直接计算的方法效率低，因为每增加一个采样，$p(x<em>k|y</em>{1:k})$都得重新计算，并且还不好计算这个式子。所以最佳的形式是能够以递推的方式去计算权重，避开计算$p(x<em>k|y</em>{1:k})$，这就是所谓的序贯重要性采样（SIS），粒子滤波的原型。</p>
<p>下面开始权重w递推形式的推导：假设重要性概率密度函数$q(x<em>{0:k}|y</em>{1:k})$，这里$x$的下标是0:k，也就是说粒子滤波是估计过去所有时刻的状态的后验。假设它可以分解为：</p>
<script type="math/tex; mode=display">
q(x_{0:k}|y_{1:k})=q(x_{0:k-1}|y_{1:k-1})q(x_k|x_{0:k-1},y_{1:k})</script><p>后验概率密度函数的递归形式可以表示为：</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
p(x_{0:k}|Y_k)
&=\frac{p(y_k|x_{0:k},Y_{k-1})p(x_{0:k}|Y_{k-1})}{p(y_k|Y_{k-1})} & //\ Y_k = y_{1:k} \\ \\
&=\frac{p(y_k|x_{0:k},Y_{k-1})p(x_k|x_{0:k-1},Y_{k-1})p(x_{0:k-1}|Y_{k-1})}{p(y_k|Y_{k-1})} \\ \\
&=\frac{p(y_k|x_k)p(x_k|x_{k-1})p(x_{0:k-1}|Y_{k-1})}{p(y_k|Y_{k-1})} \\ \\
&\propto p(y_k|x_k)p(x_k|x_{k-1})p(x_{0:k-1}|Y_{k-1})
\end{split} 
\end{equation}</script><p>上面这个式子和上一节贝叶斯滤波中后验概率的推导是一样的，只是之前的$x<em>k$变成了这里的$x</em>{0:k}$，就是这个不同，导致贝叶斯估计里需要积分，而这里后验概率的分解形式却不用积分。</p>
<p>粒子权值的递归形式可以表示为:</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
w_k^{(i)}
&\propto\frac{p(x_{0:k}^{(i)}|Y_k)}{q(x_{0:k}^{(i)}|Y_k)} \\ \\
&= \frac{p(y_k|x_k^{(i)})p(x_k^{(i)}|x_{k-1}^{(i)})p(x_{0:k-1}^{(i)}|Y_{k-1})}{q(x_k^{(i)}|x_{0:k-1}^{(i)},Y_k)q(x_{0:k-1}^{(i)}|Y_{k-1})} \\ \\
&= w_{k-1}^{(i)}\frac{p(y_k|x_k^{(i)})p(x_k^{(i)}|x_{k-1}^{(i)})}{q(x_k^{(i)}|x_{0:k-1}^{(i)},Y_k)}
\end{split} 
\end{equation}</script><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdWppYWt1aW5vMS9hcnRpY2xlL2RldGFpbHMvNTQzNDM1Mjc=">Particle Filter Tutorial 粒子滤波：从推导到应用（一 二 三 四）<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU3JUIyJTkyJUU1JUFEJTkwJUU2JUJGJUJFJUU2JUIzJUEyJUU1JTk5JUE4">粒子滤波器 - 维基百科<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93ZW5rdS5iYWlkdS5jb20vdmlldy84ODg5NmQyYjQ1MzYxMDY2MWVkOWY0YjQuaHRtbD92aXNpdERzdFRpbWU9MQ==">粒子滤波理论 - 百度文库<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzI1MzcxNDc2">怎样从实际场景上理解粒子滤波（Particle Filter）？- zhihu<i class="fa fa-external-link-alt"></i></span><br>机器人定位问题：<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vMjEyMDctaUhvbWUvcC81MjM3NzAxLmh0bWw=">https://www.cnblogs.com/21207-iHome/p/5237701.html<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>机器人</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>getting started ros</title>
    <url>/2019/02/05/30.ros/</url>
    <content><![CDATA[<p>ROS之前看过一遍，但是过于复杂，我就很快忘记了；现在重新梳理一遍。<br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxz">http://wiki.ros.org/cn/ROS/Tutorials<i class="fa fa-external-link-alt"></i></span></p>
<a id="more"></a>
<h1 id="Tutorials"><a href="#Tutorials" class="headerlink" title="Tutorials"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxz">Tutorials<i class="fa fa-external-link-alt"></i></span></h1><p>官方良心教程</p>
<h1 id="catkin-overview"><a href="#catkin-overview" class="headerlink" title="catkin overview"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4=">catkin<i class="fa fa-external-link-alt"></i></span> <span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vY29uY2VwdHVhbF9vdmVydmlldw==">overview<i class="fa fa-external-link-alt"></i></span></h1><p>Catkin has the following dependencies: <span class="exturl" data-url="aHR0cDovL2NtYWtlLm9yZy8=">CMake<i class="fa fa-external-link-alt"></i></span>、<span class="exturl" data-url="aHR0cDovL3B5dGhvbi5vcmcv">Pythn<i class="fa fa-external-link-alt"></i></span>(2.7)、<span class="exturl" data-url="aHR0cDovL3d3dy5yb3Mub3JnL3dpa2kvY2F0a2luX3BrZw==">catkin_pkg<i class="fa fa-external-link-alt"></i></span>、<span class="exturl" data-url="aHR0cDovL3d3dy5hbGN5b25lLmNvbS9weW9zL2VtcHkv">empy<i class="fa fa-external-link-alt"></i></span>、<span class="exturl" data-url="aHR0cHM6Ly9ub3NlLnJlYWR0aGVkb2NzLm9yZy9lbi9sYXRlc3Qv">nose<i class="fa fa-external-link-alt"></i></span>、<span class="exturl" data-url="aHR0cDovL2NvZGUuZ29vZ2xlLmNvbS9wL2dvb2dsZXRlc3Qv">GTest<i class="fa fa-external-link-alt"></i></span>、<span class="exturl" data-url="aHR0cDovL2djYy5nbnUub3JnLw==">g++<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="catkin-document"><a href="#catkin-document" class="headerlink" title="catkin document"></a><span class="exturl" data-url="aHR0cDovL2RvY3Mucm9zLm9yZy9hcGkvY2F0a2luL2h0bWwv">catkin document<i class="fa fa-external-link-alt"></i></span></h2><p><span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnJvcy5vcmcvbWVsb2RpYy9hcGkvY2F0a2luL2h0bWwvaG93dG8vZm9ybWF0Mi9jYXRraW5fbGlicmFyeV9kZXBlbmRlbmNpZXMuaHRtbA==">C++ catkin library dependencies<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vcGFja2FnZS54bWw=">caktin for package<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="catkin-command"><a href="#catkin-command" class="headerlink" title="catkin command"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vY29tbWFuZHM=">catkin command<i class="fa fa-external-link-alt"></i></span></h2><p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vY29tbWFuZHMvY2F0a2luX2NyZWF0ZV9wa2c=">catkin_create_pkg<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vY29tbWFuZHMvY2F0a2luX21ha2U=">catkin_make<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vY29tbWFuZHMvY2F0a2luX2luaXRfd29ya3NwYWNl">catkin_init_workspace<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="catkin-workspace"><a href="#catkin-workspace" class="headerlink" title="catkin workspace"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vd29ya3NwYWNlcw==">catkin workspace<i class="fa fa-external-link-alt"></i></span></h1><p>catkin workspace是一套规范：Source Space、Build Space、Development (Devel) Space、Install Space、Result space</p>
<h2 id="REP-128"><a href="#REP-128" class="headerlink" title="REP-128"></a><span class="exturl" data-url="aHR0cDovL3d3dy5yb3Mub3JnL3JlcHMvcmVwLTAxMjguaHRtbA==">REP-128<i class="fa fa-external-link-alt"></i></span></h2><p>REP-128是catkin workspace的一套命名约定。</p>
<h1 id="ros-package"><a href="#ros-package" class="headerlink" title="ros package"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vcGFja2FnZS54bWw=">ros package<i class="fa fa-external-link-alt"></i></span></h1><p>创建程序包：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vVHV0b3JpYWxzL0NyZWF0aW5nUGFja2FnZQ==">catkin_create_pkg<i class="fa fa-external-link-alt"></i></span>，程序包存在依赖关系。<br>编译程序包：<code>source devel/setup.bash</code>，ref：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL0luc3RhbGxpbmdhbmRDb25maWd1cmluZ1JPU0Vudmlyb25tZW50">安装并配置ROS环境<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="手动创建"><a href="#手动创建" class="headerlink" title="手动创建"></a>手动创建</h2><h2 id="依赖管理rosdep"><a href="#依赖管理rosdep" class="headerlink" title="依赖管理rosdep"></a>依赖管理<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL3Jvc2RlcA==">rosdep<i class="fa fa-external-link-alt"></i></span></h2><h1 id="catkin-cmakelists"><a href="#catkin-cmakelists" class="headerlink" title="catkin cmakelists"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vQ01ha2VMaXN0cy50eHQ=">catkin cmakelists<i class="fa fa-external-link-alt"></i></span></h1><p>详细的cmake规则说明↑</p>
<h2 id="REP-127"><a href="#REP-127" class="headerlink" title="REP-127"></a><span class="exturl" data-url="aHR0cDovL3d3dy5yb3Mub3JnL3JlcHMvcmVwLTAxMjcuaHRtbA==">REP-127<i class="fa fa-external-link-alt"></i></span></h2><p>package.xml定义在REP127</p>
<h2 id="REP-140"><a href="#REP-140" class="headerlink" title="REP-140"></a><span class="exturl" data-url="aHR0cDovL3d3dy5yb3Mub3JnL3JlcHMvcmVwLTAxNDAuaHRtbA==">REP-140<i class="fa fa-external-link-alt"></i></span></h2><p>REP-140是catkin package的一套命名约定。</p>
<h1 id="graph-resource-name"><a href="#graph-resource-name" class="headerlink" title="graph resource name"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9OYW1lcw==">graph resource name<i class="fa fa-external-link-alt"></i></span></h1><p>Graph Resource Names are an important mechanism in ROS for providing encapsulation。 </p>
<h2 id="http-wiki-ros-org-msg"><a href="#http-wiki-ros-org-msg" class="headerlink" title="http://wiki.ros.org/msg"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9tc2c=">http://wiki.ros.org/msg<i class="fa fa-external-link-alt"></i></span></h2><p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ST1MvTWVzc2FnZV9EZXNjcmlwdGlvbl9MYW5ndWFnZQ==">message description language<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="http-wiki-ros-org-Nodes"><a href="#http-wiki-ros-org-Nodes" class="headerlink" title="http://wiki.ros.org/Nodes"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9Ob2Rlcw==">http://wiki.ros.org/Nodes<i class="fa fa-external-link-alt"></i></span></h2><p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL1dyaXRpbmdQdWJsaXNoZXJTdWJzY3JpYmVy">publisher &amp; subscriber<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="http-wiki-ros-org-srv"><a href="#http-wiki-ros-org-srv" class="headerlink" title="http://wiki.ros.org/srv"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9zcnY=">http://wiki.ros.org/srv<i class="fa fa-external-link-alt"></i></span></h2><p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL1dyaXRpbmdTZXJ2aWNlQ2xpZW50">server &amp; client Tutorials<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="faq"><a href="#faq" class="headerlink" title="faq"></a>faq</h2><p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL0NyZWF0aW5nTXNnQW5kU3J2">Common step for msg and srv.<i class="fa fa-external-link-alt"></i></span>，该step有小问题，主要是因为：<br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jYXRraW4vcGFja2FnZS54bWwjRm9ybWF0XzJfLjI4UmVjb21tZW5kZWQuMjk=">package format<i class="fa fa-external-link-alt"></i></span>，参考<span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnJvcy5vcmcvbWVsb2RpYy9hcGkvY2F0a2luL2h0bWwvaG93dG8vZm9ybWF0Mi9taWdyYXRpbmdfZnJvbV9mb3JtYXRfMS5odG1sI21pZ3JhdGluZy1mcm9tLWZvcm1hdDEtdG8tZm9ybWF0Mg==">format1迁移format2<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxzL1R1dG9yaWFscy9Vc2luZ0NsYXNzTWV0aG9kc0FzQ2FsbGJhY2tz">using class Methods as callback<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="py文件需要对它chmod-x-py-变为可执行才能run"><a href="#py文件需要对它chmod-x-py-变为可执行才能run" class="headerlink" title=".py文件需要对它chmod +x *.py 变为可执行才能run"></a>.py文件需要对它chmod +x *.py 变为可执行才能run</h2><h1 id="understand-ros-topic"><a href="#understand-ros-topic" class="headerlink" title="understand ros topic"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL1VuZGVyc3RhbmRpbmdUb3BpY3M=">understand<i class="fa fa-external-link-alt"></i></span> <span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3N0b3BpYw==">ros topic<i class="fa fa-external-link-alt"></i></span></h1><p>节点之间是通过ROS话题来互相通信的。node在一个topic上发布（publish）msg，而其他node则订阅（subscribe）该topic以接收该msg.<br>进阶：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NweS9PdmVydmlldy9QdWJsaXNoZXJzJTIwYW5kJTIwU3Vic2NyaWJlcnMjcm9zcHkuUHVibGlzaGVyX2luaXRpYWxpemF0aW9u">http://wiki.ros.org/rospy/Overview/Publishers%20and%20Subscribers#rospy.Publisher_initialization<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="rqt-graph"><a href="#rqt-graph" class="headerlink" title="rqt_graph"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ycXRfZ3JhcGg=">rqt_graph<i class="fa fa-external-link-alt"></i></span></h2><p>一个几乎没啥功能的图形化界面，显示框图、线条的粗细颜色表示通信数据流量和延迟。可以保存各种图片格式，便于梳理机器人代码结构。</p>
<h2 id="rxplot"><a href="#rxplot" class="headerlink" title="rxplot"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yeHBsb3Q=">rxplot<i class="fa fa-external-link-alt"></i></span></h2><p>一个plot UI，可以显示波形，和rqt_graph差不多。</p>
<h1 id="ros-service"><a href="#ros-service" class="headerlink" title="ros service"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL1VuZGVyc3RhbmRpbmdTZXJ2aWNlc1BhcmFtcw==">ros service<i class="fa fa-external-link-alt"></i></span></h1><p>服务（services）是节点之间通讯的另一种方式。服务允许节点发送请求（request） 并获得一个响应（response）.</p>
<h2 id="using-rosparam"><a href="#using-rosparam" class="headerlink" title="using rosparam"></a>using rosparam</h2><p>rosparam使得我们能够存储并操作ROS 参数服务器（Parameter Server）上的数据。参数服务器能够存储整型、浮点、布尔、字符串、字典和列表等数据类型。rosparam使用YAML标记语言的语法。</p>
<h1 id="actionlib"><a href="#actionlib" class="headerlink" title="actionlib"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9hY3Rpb25saWI=">actionlib<i class="fa fa-external-link-alt"></i></span></h1><p>actionlib就是带feedback的srv，but基于msg，原理描述：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9hY3Rpb25saWIvRGV0YWlsZWREZXNjcmlwdGlvbg==">http://wiki.ros.org/actionlib/DetailedDescription<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="ros-console"><a href="#ros-console" class="headerlink" title="ros console"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3Njb25zb2xl">ros console<i class="fa fa-external-link-alt"></i></span></h1><p>编写node、server、client，API，such as ROS_INFO：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxz">http://wiki.ros.org/roscpp_tutorials<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxzL1R1dG9yaWFscy9Xcml0aW5nUHVibGlzaGVyU3Vic2NyaWJlcg==">Writing a Simple Publisher and Subscriber<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxzL1R1dG9yaWFscy9Xcml0aW5nU2VydmljZUNsaWVudA==">Writing a Simple Service and Client<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxzL1R1dG9yaWFscy9QYXJhbWV0ZXJz">Using Parameters in roscpp<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxzL1R1dG9yaWFscy9BY2Nlc3NpbmdQcml2YXRlTmFtZXNXaXRoTm9kZUhhbmRsZQ==">Accessing Private Names from a NodeHandle<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxzL1R1dG9yaWFscy9Vc2luZ0NsYXNzTWV0aG9kc0FzQ2FsbGJhY2tz">Using Class Methods as Callbacks<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxzL1R1dG9yaWFscy9UaW1lcnM=">Understanding Timers<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NjcHBfdHV0b3JpYWxzL1R1dG9yaWFscy9QdWJsaXNoZXIlMjBhbmQlMjBTdWJzY3JpYmVyJTIwd2l0aCUyMFBhcmFtZXRlcnMlMjBhbmQlMjBEeW5hbWljJTIwUmVjb25maWd1cmU=">Dynamic Reconfigure<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9DbG9jaw==">clock<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="ros-launch"><a href="#ros-launch" class="headerlink" title="ros launch"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NsYXVuY2g=">ros launch<i class="fa fa-external-link-alt"></i></span></h1><p>ros launch可以启动多个node，有固定文件写法。<br>进阶：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL1Jvc2xhdW5jaCUyMHRpcHMlMjBmb3IlMjBsYXJnZXIlMjBwcm9qZWN0cw==">http://wiki.ros.org/cn/ROS/Tutorials/Roslaunch%20tips%20for%20larger%20projects<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="ros-bash"><a href="#ros-bash" class="headerlink" title="ros bash"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NiYXNo">ros bash<i class="fa fa-external-link-alt"></i></span></h1><p>这里是各种ROS的bash指令，参考<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL05hdmlnYXRpbmdUaGVGaWxlc3lzdGVt">ROS文件系统介绍<i class="fa fa-external-link-alt"></i></span><br>rosed 是 rosbash 的一部分。利用它可以直接通过package名来获取到待编辑的文件而无需指定该文件的存储路径了。</p>
<h1 id="debugging"><a href="#debugging" class="headerlink" title="debugging"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL1VzaW5nUnF0Y29uc29sZVJvc2xhdW5jaA==">debugging<i class="fa fa-external-link-alt"></i></span></h1><h2 id="ros-record"><a href="#ros-record" class="headerlink" title="ros record"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL1JlY29yZGluZw==">ros record<i class="fa fa-external-link-alt"></i></span></h2><h2 id="ros-wtf"><a href="#ros-wtf" class="headerlink" title="ros wtf"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3N3dGY=">ros wtf<i class="fa fa-external-link-alt"></i></span></h2><h2 id="rqt-console"><a href="#rqt-console" class="headerlink" title="rqt_console"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ycXRfY29uc29sZQ==">rqt_console<i class="fa fa-external-link-alt"></i></span></h2><p>rqt_console属于ROS日志框架(logging framework)的一部分，用来显示节点的输出信息。</p>
<h2 id="rqt-logger-level"><a href="#rqt-logger-level" class="headerlink" title="rqt_logger_level"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ycXRfbG9nZ2VyX2xldmVs">rqt_logger_level<i class="fa fa-external-link-alt"></i></span></h2><p>rqt_logger_level允许我们修改节点运行时输出信息的日志等级（logger levels）（包括 DEBUG、WARN、INFO和ERROR）。</p>
<h1 id="master-and-newwork"><a href="#master-and-newwork" class="headerlink" title="master and newwork"></a>master and newwork</h1><h2 id="ros-core"><a href="#ros-core" class="headerlink" title="ros core"></a>ros core</h2><p>如果 roscore 运行后无法正常初始化，很有可能是存在网络配置问题。<br>如果 roscore 不能初始化并提示缺少权限，这可能是因为~/.ros文件夹归属于root</p>
<p>运行类似于rosnode的指令时出现一些问题,也许是环境变量的影响。</p>
<h2 id="ros-out"><a href="#ros-out" class="headerlink" title="ros out"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9yb3NvdXQ=">ros out<i class="fa fa-external-link-alt"></i></span></h2><h2 id="network-setup"><a href="#network-setup" class="headerlink" title="network setup"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ST1MvTmV0d29ya1NldHVwI1NpbmdsZV9tYWNoaW5lX2NvbmZpZ3VyYXRpb24=">network setup<i class="fa fa-external-link-alt"></i></span></h2><h2 id="multiple-machines"><a href="#multiple-machines" class="headerlink" title="multiple machines"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9jbi9ST1MvVHV0b3JpYWxzL011bHRpcGxlTWFjaGluZXM=">multiple machines<i class="fa fa-external-link-alt"></i></span></h2><h1 id="ros-tf-Introduction"><a href="#ros-tf-Introduction" class="headerlink" title="ros tf Introduction"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy90Zg==">ros tf<i class="fa fa-external-link-alt"></i></span> <span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy90Zi9UdXRvcmlhbHMvSW50cm9kdWN0aW9uJTIwdG8lMjB0Zg==">Introduction<i class="fa fa-external-link-alt"></i></span></h1><p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy90Zi9UdXRvcmlhbHM=">TF Tutorials<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="tf-API"><a href="#tf-API" class="headerlink" title="tf API"></a>tf API</h2><p>数据类型：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy90Zi9PdmVydmlldy9EYXRhJTIwVHlwZXM=">http://wiki.ros.org/tf/Overview/Data%20Types<i class="fa fa-external-link-alt"></i></span><br>变换与参考系：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy90Zi9PdmVydmlldy9UcmFuc2Zvcm1hdGlvbnM=">http://wiki.ros.org/tf/Overview/Transformations<i class="fa fa-external-link-alt"></i></span><br>发布tf变换广播：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy90Zi9PdmVydmlldy9Ccm9hZGNhc3RpbmclMjBUcmFuc2Zvcm1z">http://wiki.ros.org/tf/Overview/Broadcasting%20Transforms<i class="fa fa-external-link-alt"></i></span><br>接收并使用tf广播：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy90Zi9PdmVydmlldy9Vc2luZyUyMFB1Ymxpc2hlZCUyMFRyYW5zZm9ybXM=">http://wiki.ros.org/tf/Overview/Using%20Published%20Transforms<i class="fa fa-external-link-alt"></i></span><br>异常：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy90Zi9PdmVydmlldy9FeGNlcHRpb25z">http://wiki.ros.org/tf/Overview/Exceptions<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="tf-monitor"><a href="#tf-monitor" class="headerlink" title="tf_monitor"></a>tf_monitor</h2><h2 id="tf-echo"><a href="#tf-echo" class="headerlink" title="tf_echo"></a>tf_echo</h2><h2 id="static-transform-publisher"><a href="#static-transform-publisher" class="headerlink" title="static_transform_publisher"></a>static_transform_publisher</h2><h2 id="view-frames"><a href="#view-frames" class="headerlink" title="view_frames"></a>view_frames</h2><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5naG0xOTk1L2FydGljbGUvZGV0YWlscy84NDY0NDk4NA==">详解tf<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="ros-navigation"><a href="#ros-navigation" class="headerlink" title="ros navigation"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9uYXZpZ2F0aW9u">ros navigation<i class="fa fa-external-link-alt"></i></span></h1><p>frist：sudo apt-get install ros-kinetic-navigation<br>tutorial：<span class="exturl" data-url="aHR0cHM6Ly93d3cubmNueW5sLmNvbS9jYXRlZ29yeS9yb3MtbmF2aWdhdGlvbi8=">https://www.ncnynl.com/category/ros-navigation/<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly93d3cucm9zLm9yZy9yZXBzL3JlcC0wMTA1Lmh0bWw=">https://www.ros.org/reps/rep-0105.html<i class="fa fa-external-link-alt"></i></span></p>
<p>map：<span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9tYXBfc2VydmVyP2Rpc3Rybz1raW5ldGlj">http://wiki.ros.org/map_server?distro=kinetic<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9nbG9iYWxfcGxhbm5lcj9kaXN0cm89a2luZXRpYw==">http://wiki.ros.org/global_planner?distro=kinetic<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9tb3ZlX2Jhc2U/ZGlzdHJvPWtpbmV0aWM=">http://wiki.ros.org/move_base?distro=kinetic<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9uYXZfY29yZT9kaXN0cm89a2luZXRpYw==">http://wiki.ros.org/nav_core?distro=kinetic<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="rviz-user-guide"><a href="#rviz-user-guide" class="headerlink" title="rviz user guide"></a><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ydml6">rviz<i class="fa fa-external-link-alt"></i></span> <span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ydml6L1VzZXJHdWlkZQ==">user guide<i class="fa fa-external-link-alt"></i></span></h1><p><span class="exturl" data-url="aHR0cDovL3dpa2kucm9zLm9yZy9ydml6L0Rpc3BsYXlUeXBlcy9NYXJrZXI=">http://wiki.ros.org/rviz/DisplayTypes/Marker<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2RvY3Mucm9zLm9yZy9hcGkvdmlzdWFsaXphdGlvbl9tc2dzL2h0bWwvbXNnL01hcmtlci5odG1s">http://docs.ros.org/api/visualization_msgs/html/msg/Marker.html<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL1BpY2tOaWtSb2JvdGljcy9ydml6X3Zpc3VhbF90b29scw==">https://github.com/PickNikRobotics/rviz_visual_tools<i class="fa fa-external-link-alt"></i></span></p>
<h1 id="gazebo"><a href="#gazebo" class="headerlink" title="gazebo"></a><span class="exturl" data-url="aHR0cDovL2dhemVib3NpbS5vcmcv">gazebo<i class="fa fa-external-link-alt"></i></span></h1><h1 id="v-rep"><a href="#v-rep" class="headerlink" title="v-rep"></a><span class="exturl" data-url="aHR0cDovL3d3dy5jb3BwZWxpYXJvYm90aWNzLmNvbS9oZWxwRmlsZXMvaW5kZXguaHRtbA==">v-rep<i class="fa fa-external-link-alt"></i></span></h1><h1 id="simulator"><a href="#simulator" class="headerlink" title="simulator"></a>simulator</h1><p>rviz是三维可视化工具，强调把已有的数据可视化显示；<br>gazebo是三维物理仿真平台，强调的是创建一个虚拟的仿真环境。<span class="exturl" data-url="aHR0cDovL2dhemVib3NpbS5vcmcv">http://gazebosim.org/<i class="fa fa-external-link-alt"></i></span></p>
<p>used ML for simulator <span class="exturl" data-url="aHR0cDovL3d3dy5tdWpvY28ub3JnLw==">http://www.mujoco.org/<i class="fa fa-external-link-alt"></i></span> </p>
]]></content>
      <categories>
        <category>机器人</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title>ROS通信解析</title>
    <url>/2019/02/09/32.ros%20tcp/</url>
    <content><![CDATA[<p>了解ROS的几种通信方式以后，对于ROS通信延迟有一些困惑，更多的是想追究背后原理，这篇博客《<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1pYUUhCRC9hcnRpY2xlL2RldGFpbHMvNzI4NDY4NjU=">ROS之topic和service通信比较<i class="fa fa-external-link-alt"></i></span>》起到了抛砖引玉的作用。</p>
<p>更多可以参考2013年的这一篇论文《<span class="exturl" data-url="aHR0cHM6Ly93d3cucG9saXRlc2kucG9saW1pLml0L2JpdHN0cmVhbS8xMDU4OS83ODUwMi8xLzIwMTNfMDRfWm9wcGkuUERG">A lightweight Opensource communication framework for native integration of resource constrained robotics devices with ROS<i class="fa fa-external-link-alt"></i></span>》，目前网上关于ROS通信模型的中文资料大部分都是对这篇文章的翻译。</p>
<a id="more"></a>
<h1 id="先上结论"><a href="#先上结论" class="headerlink" title="先上结论"></a>先上结论</h1><h2 id="Topic（异步）"><a href="#Topic（异步）" class="headerlink" title="Topic（异步）"></a>Topic（异步）</h2><p>topic基于ros tcp/udp，pub/sub通信延迟大约0.7ms，但是ros::spin()是忙等待，对于异步架构，回调函数的执行频率设计对系统整体性能影响很大，node不宜过多。</p>
<h2 id="Service（同步）"><a href="#Service（同步）" class="headerlink" title="Service（同步）"></a>Service（同步）</h2><p>server基于RPC，它在接收到调用请求前都处于休眠状态；将休眠状态的server唤醒，所需要的时间大约是2.7ms，约占总通信延迟的82%，实际传输延迟大于0.3ms，只有topic的一半。server唤醒耗时太大，不建议用于通讯频率高的情况。</p>
<p>Client维护着一个与server的持久连接(persistent connection)，在单纯的数据传输上可以做到更快，尤其是在分布式作业。</p>
<h1 id="阻塞非阻塞与同步异步"><a href="#阻塞非阻塞与同步异步" class="headerlink" title="阻塞非阻塞与同步异步"></a>阻塞非阻塞与同步异步</h1><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzE5NzMyNDczL2Fuc3dlci8yMDg1MTI1Ng==">怎样理解阻塞非阻塞与同步异步的区别？ - 严肃的回答 - 知乎<i class="fa fa-external-link-alt"></i></span></p>
<ul>
<li>同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication).</li>
<li>阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.</li>
</ul>
<p>在处理 IO 的时候，阻塞和非阻塞都是同步 IO。只有使用了特殊的 API 才是异步 IO。</p>
<h2 id="异步消息的传递－回调机制"><a href="#异步消息的传递－回调机制" class="headerlink" title="异步消息的传递－回调机制"></a><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWJtLmNvbS9kZXZlbG9wZXJ3b3Jrcy9jbi9saW51eC9sLWNhbGxiYWNrL2luZGV4Lmh0bWw=">异步消息的传递－回调机制<i class="fa fa-external-link-alt"></i></span></h2><p>目前的粗浅理解，异步通信总会带点儿callback。</p>
]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>工具链</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenMP</title>
    <url>/2018/08/01/06.openmp/</url>
    <content><![CDATA[<p>OpenMP（Open Multi-Processing）是一套支持跨平台共享内存方式的多线程并发的编程API，使用C,C++和Fortran语言，可以在大多数的处理器体系和操作系统中运行。</p>
<a id="more"></a> 
<hr>
<h1 id="摘录"><a href="#摘录" class="headerlink" title="摘录"></a>摘录</h1><p><span class="exturl" data-url="aHR0cDovL3dkeHR1Yi5jb20vMjAxNi8wMy8yMC9vcGVubXAtZ3VpZGUv">http://wdxtub.com/2016/03/20/openmp-guide/<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cuaWJtLmNvbS9kZXZlbG9wZXJ3b3Jrcy9jbi9haXgvbGlicmFyeS9hdS1haXgtb3Blbm1wLWZyYW1ld29yay9pbmRleC5odG1s">https://www.ibm.com/developerworks/cn/aix/library/au-aix-openmp-framework/index.html<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZHJ6aG91d2VpbWluZy9hcnRpY2xlL2RldGFpbHMvNDA5MzYyNA==">http://blog.csdn.net/drzhouweiming/article/details/4093624<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZHJ6aG91d2VpbWluZy9hcnRpY2xlL2RldGFpbHMvMTEzMTUzNw==">http://blog.csdn.net/drzhouweiming/article/details/1131537<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZHJ6aG91d2VpbWluZy9hcnRpY2xlL2RldGFpbHMvMjAzMzI3Ng==">http://blog.csdn.net/drzhouweiming/article/details/2033276<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZHJ6aG91d2VpbWluZy9hcnRpY2xlL2RldGFpbHMvMjAzMzI3Ng==">http://blog.csdn.net/drzhouweiming/article/details/2033276<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZHJ6aG91d2VpbWluZy9hcnRpY2xlL2RldGFpbHMvMTY4OTg1Mw==">http://blog.csdn.net/drzhouweiming/article/details/1689853<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZHJ6aG91d2VpbWluZy9hcnRpY2xlL2RldGFpbHMvMjQ3MjQ1NA==">http://blog.csdn.net/drzhouweiming/article/details/2472454<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><p>在gcc编译器下，条件编译选项<code>-fopenmp</code>就可以开启openMP支持。<br>在Visual Studio下，<code>项目属性</code> —&gt; <code>C/C++</code> —&gt; <code>语言</code> —&gt; <code>OpenMP支持</code> —&gt; <code>Yes</code> 就可以开启OpenMP支持。</p>
<hr>
<h1 id="OpenMP指令和库函数介绍"><a href="#OpenMP指令和库函数介绍" class="headerlink" title="OpenMP指令和库函数介绍"></a>OpenMP指令和库函数介绍</h1><p>在C/C++中，OpenMP指令使用的格式为：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">＃pragma omp 指令 [子句[子句]…]</span><br></pre></td></tr></table></figure><br>OpenMP的指令有以下一些：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">parallel          <span class="comment">// 用在一个代码段之前，表示这段代码将被多个线程并行执行</span></span><br><span class="line"><span class="keyword">for</span>               <span class="comment">// 用于for循环之前将循环并行，必须保证每次循环之间无相关性。</span></span><br><span class="line">parallel <span class="keyword">for</span>      <span class="comment">// 用在一个for循环之前，表示for循环的代码将被多个线程并行执行。</span></span><br><span class="line">sections          <span class="comment">// 用在可能会被并行执行的代码段之前</span></span><br><span class="line">parallel sections <span class="comment">// parallel和sections两个语句的结合</span></span><br><span class="line">critical          <span class="comment">// 用在一段代码临界区之前</span></span><br><span class="line">single            <span class="comment">// 用在一段只被单个线程执行的代码段之前，表示后面的代码段将被单线程执行。</span></span><br><span class="line">barrier           <span class="comment">// 用于并行区内代码的线程同步，所有线程执行到barrier时要停止，</span></span><br><span class="line">                  <span class="comment">// 直到所有线程都执行到barrier时才继续往下执行。</span></span><br><span class="line">atomic            <span class="comment">// 用于指定一块内存区域被制动更新</span></span><br><span class="line">master            <span class="comment">// 用于指定一段代码块由主线程执行</span></span><br><span class="line">ordered           <span class="comment">// 用于指定并行区域的循环按顺序执行</span></span><br><span class="line">threadprivate     <span class="comment">// 用于指定一个变量是线程私有的。</span></span><br></pre></td></tr></table></figure><br>OpenMP除上述指令外，还有一些库函数，下面列出几个常用的库函数：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">omp_get_num_procs()     <span class="comment">// 返回运行本线程的多处理机的处理器个数</span></span><br><span class="line">omp_get_num_threads()   <span class="comment">// 返回当前并行区域中的活动线程个数</span></span><br><span class="line">omp_get_thread_num()    <span class="comment">// 返回线程号</span></span><br><span class="line">omp_set_num_threads()   <span class="comment">// 设置并行执行代码时的线程个数</span></span><br><span class="line">omp_init_lock()         <span class="comment">// 初始化一个简单锁</span></span><br><span class="line">omp_set_lock()          <span class="comment">// 上锁操作</span></span><br><span class="line">omp_unset_lock()        <span class="comment">// 解锁操作，要和omp_set_lock函数配对使用。</span></span><br><span class="line">omp_destroy_lock()      <span class="comment">// omp_init_lock函数的配对操作函数，关闭一个锁</span></span><br></pre></td></tr></table></figure><br>OpenMP的子句有以下一些：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>       <span class="comment">// 指定每个线程都有它自己的变量私有副本</span></span><br><span class="line">firstprivate  <span class="comment">// 指定每个线程都有它自己的变量私有副本，并且变量要被继承主线程中的初值</span></span><br><span class="line">lastprivate   <span class="comment">// 主要是用来指定将线程中的私有变量的值在并行处理结束后复制回主线程中的对应变量</span></span><br><span class="line">reduce        <span class="comment">// 用来指定一个或多个变量是私有的，并且在并行处理结束后这些变量要执行指定的运算</span></span><br><span class="line">nowait        <span class="comment">// 忽略指定中暗含的等待</span></span><br><span class="line">num_threads   <span class="comment">// 指定线程的个数</span></span><br><span class="line">schedule      <span class="comment">// 指定如何调度for循环迭代</span></span><br><span class="line">shared        <span class="comment">// 指定一个或多个变量为多个线程间的共享变量</span></span><br><span class="line">ordered       <span class="comment">// 用来指定for循环的执行要按顺序执行</span></span><br><span class="line">copyprivate   <span class="comment">// 用于single指令中的指定变量为多个线程的共享变量</span></span><br><span class="line">copyin        <span class="comment">// 用来指定一个threadprivate的变量的值要用主线程的值进行初始化。</span></span><br><span class="line"><span class="keyword">default</span>       <span class="comment">// 用来指定并行处理区域内的变量的使用方式，缺省是shared</span></span><br></pre></td></tr></table></figure><br>第一个程序：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Hello World!\n&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>发生了什么？<code>#pragma omp parallel</code> 仅在指定了<code>-fopenmp编译器选项</code>后才会发挥作用。在编译期间， 会根据硬件和操作系统配置在运行时生成代码，创建尽可能多的线程。每个线程的起始例程为代码块中位于指令之后的代码。这种行为是<code>隐式的并行化</code>，而OpenMP本质上由一组功能强大的编译指示组成，省去了编写大量样本文件的工作。<br>在电脑上输出了4行”Hello World”，说明创建了4个线程。<br>使用编译命令的<code>num_threads参数</code>控制线程的数量非常简单。如下，可用线程的数量被指定为 5<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel num_threads(5)</span></span><br></pre></td></tr></table></figure><br>这里没有使用<code>num_threads</code>方法，而是使用另一种方法来修改运行代码的线程的数量。这还使用的第一个<code>OpenMP API：omp_set_num_threads</code>。在<code>omp.h</code>头文件中定义该函数。不需要链接到额外的库就可以运行。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;omp.h&gt;</span></span></span><br><span class="line">omp_set_num_threads(<span class="number">5</span>); </span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Hello World!\n&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>OpenMP使用<code>隐式并行化</code>技术，可以使用编译指示、显式函数和环境变量来指导编译器的行为。例如，可以将 for 循环分为几个部分，在不同的核心中运行它们。<code>parallel for</code>编译指示可以将 for 循环工作负载划分到多个线程中，每个线程都可以在不同的核心上运行，这显著减少了总的计算时间。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">1000000</span>], b[<span class="number">1000000</span>]; </span><br><span class="line"><span class="keyword">int</span> c[<span class="number">1000000</span>];</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for </span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000000</span>; ++i)</span><br><span class="line">        c[i] = a[i] * b[i] + a[i<span class="number">-1</span>] * b[i+<span class="number">1</span>];</span><br></pre></td></tr></table></figure></h2><h1 id="fork-join并行执行模式"><a href="#fork-join并行执行模式" class="headerlink" title="fork / join并行执行模式"></a>fork / join并行执行模式</h1><p>OpenMP是一个编译器指令和库函数的集合，主要是为共享式存储计算机上的并行程序设计使用的。OpenMP并行执行的程序要全部结束后才能执行后面的非并行部分的代码。这就是标准的并行模式：fork/join式并行模式，共享存储式并行程序就是使用fork / join式并行的。<br>标准并行模式执行代码的基本思想是：程序开始时只有一个主线程，程序中的串行部分都由主线程执行，并行的部分是通过派生其他线程来执行，但是如果并行部分没有结束时是不会执行串行部分的。</p>
<hr>
<h1 id="parallel-指令的用法"><a href="#parallel-指令的用法" class="headerlink" title="parallel 指令的用法"></a>parallel 指令的用法</h1><p>parallel 是用来构造一个并行块的，可以使用其他指令如<code>for</code>、<code>sections</code>等和它配合使用。<br>在C/C++中，<code>parallel</code>的使用方法如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel [for | sections] [子句[子句]…]</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 多个线程并行的代码</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><code>parallel块</code>中的每行代码都被多个线程重复执行，等于给一个线程入口函数重复调用创建线程函数来创建线程并等待线程执行完。<br>for指令则是用来将一个for循环分配到多个线程中执行。for指令一般和parallel指令合起来形成<code>parallel for</code>使用，也可以单独用在parallel语句的并行块中。<br>for指令要和parallel指令结合起来使用才有效果，单纯的<code>#pragma omp for</code>是没有用的，来一个例子：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; j++) &#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">&quot;j = %d, ThreadId = %d \n&quot;</span>, j, omp_get_thread_num());</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><br>上面这段代码也可以改写成以下形式：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">	<span class="keyword">int</span> j = <span class="number">0</span>; <span class="comment">// 变量 j 只定义一次，不要放在parallel块里面</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel</span></span><br><span class="line">	&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp for</span></span><br><span class="line">	<span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; <span class="number">4</span>; j++) &#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">&quot;j = %d, ThreadId = %d \n&quot;</span>, j, omp_get_thread_num());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>值得一提的是，在一个<code>parallel块</code>中也可以有多个for语句</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">int</span> j;  <span class="comment">// 并行外定义j</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp for</span></span><br><span class="line">     <span class="keyword">for</span> ( j = <span class="number">0</span>; j &lt; <span class="number">100</span>; j++ )&#123; <span class="comment">// j赋值0</span></span><br><span class="line">         <span class="comment">// 循环体1 …</span></span><br><span class="line">     &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp for</span></span><br><span class="line">     <span class="keyword">for</span> (  j = <span class="number">0</span>; j &lt; <span class="number">100</span>; j++ )&#123; <span class="comment">// j重新赋值0</span></span><br><span class="line">         <span class="comment">// 循环体2 …</span></span><br><span class="line">     &#125;</span><br><span class="line">…</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></h2><h1 id="for-循环并行化的约束条件"><a href="#for-循环并行化的约束条件" class="headerlink" title="for 循环并行化的约束条件"></a>for 循环并行化的约束条件</h1><p>尽管OpenMP可以方便地对for循环进行并行化，但并不是所有的for循环都可以进行并行化。以下几种情况不能进行并行化：</p>
<ol>
<li>for循环中的循环变量必须是有符号整形。例如，for (unsigned int i = 0; i &lt; 10; ++i){}会编译不通过</li>
<li>for循环中比较操作符必须是&lt;, &lt;=, &gt;, &gt;=。例如for (int i = 0; i != 10; ++i){}会编译不通过</li>
<li>for循环中的第三个表达式，必须是整数的加减，并且加减的值必须是一个循环不变量。例如for (int i = 0; i != 10; i = i + 1){}会编译不通过；感觉只能++i; i++; –i; 或i–</li>
<li>如果for循环中的比较操作为&lt;或&lt;=，那么循环变量只能增加；反之亦然。例如for (int i = 0; i != 10; –i)会编译不通过<br>循环必须是单入口、单出口，也就是说循环内部不允许能够达到循环以外的跳转语句，exit除外。异常的处理也必须在循环体内处理。例如：若循环体内的break或goto会跳转到循环体外，那么会编译不通过</li>
</ol>
<hr>
<h1 id="sections和section指令"><a href="#sections和section指令" class="headerlink" title="sections和section指令"></a>sections和section指令</h1><p>section语句是用在sections语句里用来将sections语句里的代码划分成几个不同的段，每段都并行执行。用法如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp [parallel] sections [子句]</span></span><br><span class="line">&#123;</span><br><span class="line">   <span class="meta">#<span class="meta-keyword">pragma</span> omp section</span></span><br><span class="line">   &#123;</span><br><span class="line">       <span class="comment">// 并行代码块1</span></span><br><span class="line">   &#125; </span><br><span class="line">   <span class="meta">#<span class="meta-keyword">pragma</span> omp section</span></span><br><span class="line">   &#123;</span><br><span class="line">       <span class="comment">// 并行代码块2</span></span><br><span class="line">   &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>使用section语句时，需要注意的是这种方式需要保证各个section里的代码执行时间相差不大，否则某个section执行时间比其他section过长就达不到并行执行的效果了。<br>用for语句来分摊是由系统自动进行，只要每次循环间没有时间上的差距，那么分摊是很均匀的，使用section来划分线程是一种手工划分线程的方式，最终并行性的好坏得依赖于程序员。<br>再看一种写法：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> omp sections</span></span><br><span class="line">    &#123;</span><br><span class="line">	<span class="meta">#<span class="meta-keyword">pragma</span> omp section</span></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;section 1 ThreadId = %d \n&quot;</span>, omp_get_thread_num());</span><br><span class="line">	<span class="meta">#<span class="meta-keyword">pragma</span> omp section</span></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;section 2 ThreadId = %d \n&quot;</span>, omp_get_thread_num());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> omp sections</span></span><br><span class="line">    &#123;</span><br><span class="line"> 	<span class="meta">#<span class="meta-keyword">pragma</span> omp section</span></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;section 3 ThreadId = %d \n&quot;</span>, omp_get_thread_num());</span><br><span class="line">	<span class="meta">#<span class="meta-keyword">pragma</span> omp section</span></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;section 4 ThreadId = %d \n&quot;</span>, omp_get_thread_num());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>这种方式和前面那种方式的区别是，两个sections语句是串行执行的，即第二个sections语句里的代码要等第一个sections语句里的代码执行完后才能执行。</p>
<hr>
<h1 id="数据的共享与私有化"><a href="#数据的共享与私有化" class="headerlink" title="数据的共享与私有化"></a>数据的共享与私有化</h1><p>在并行区域中，若多个线程共同访问同一存储单元，并且至少会有一个线程更新数据单元中的内容时，会发送数据竞争。<br>除了以下三种情况外，并行区域中的所有变量都是共享的：</p>
<ol>
<li>并行区域中定义的变量</li>
<li>多个线程用来完成循环的循环变量</li>
<li>private、firstprivate、lastprivate或reduction字句修饰的变量</li>
</ol>
<p>并行区域中变量val是私有的，即每个线程拥有该变量的一个拷贝<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>(val1, val2, ...)</span><br></pre></td></tr></table></figure><br>与private不同的是，每个线程在开始的时候都会对该变量进行一次初始化。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">firstprivate(val1, val2, ...)</span><br></pre></td></tr></table></figure><br>与private不同的是，并发执行的最后一次循环的私有变量将会拷贝到val<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">lastprivate(val1, val2, ...)</span><br></pre></td></tr></table></figure><br>声明val是共享的<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">shared(val1, val2, ...)</span><br></pre></td></tr></table></figure><br>如果使用private，无论该变量在并行区域外是否初始化，在进入并行区域后，该变量均不会初始化。</p>
<h2 id="private子句"><a href="#private子句" class="headerlink" title="private子句"></a>private子句</h2><p>private子句用于将一个或多个变量声明成线程私有的变量，变量声明成私有变量后，指定每个线程都有它自己的变量私有副本，其他线程无法访问私有副本。即使在并行区域外有同名的共享变量，共享变量在并行区域内不起任何作用，并且并行区域内不会操作到外面的共享变量。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">int</span> k = <span class="number">100</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for private(k)</span></span><br><span class="line">    <span class="keyword">for</span> (k = <span class="number">0</span>; k &lt; <span class="number">10</span>; k++)</span><br><span class="line">    &#123;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;k=%d \n&quot;</span>, k);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;last k=%d \n&quot;</span>, k);</span><br></pre></td></tr></table></figure><br>从打印结果可以看出，for循环前的变量k和循环区域内的变量k其实是两个不同的变量。<br>用private子句声明的私有变量的初始值在并行区域的入口处是未定义的，它并不会继承同名共享变量的值。<br>出现在reduction子句中的参数不能出现在private子句中。</p>
<h2 id="firstprivate子句"><a href="#firstprivate子句" class="headerlink" title="firstprivate子句"></a>firstprivate子句</h2><p>private声明的私有变量不能继承同名变量的值，但实际情况中有时需要继承原有共享变量的值，OpenMP提供了firstprivate子句来实现这个功能。<br>以下的代码例子：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">int</span> k = <span class="number">100</span>, i=<span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for firstprivate(k)</span></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">	k += i;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;k=%d \n&quot;</span>, k);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;last k=%d \n&quot;</span>, k);</span><br></pre></td></tr></table></figure><br>从打印结果可以看出，并行区域内的私有变量k继承了外面共享变量k的值100作为初始值，并且在退出并行区域后，共享变量k的值保持为100未变。</p>
<h2 id="lastprivate子句"><a href="#lastprivate子句" class="headerlink" title="lastprivate子句"></a>lastprivate子句</h2><p>有时在并行区域内的私有变量的值经过计算后，在退出并行区域时，需要将它的值赋给同名的共享变量，前面的private和firstprivate子句在退出并行区域时都没有将私有变量的最后取值赋给对应的共享变量，lastprivate子句就是用来实现在退出并行区域时将私有变量的值赋给共享变量。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">int</span> k = <span class="number">100</span>, i=<span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for firstprivate(k),lastprivate(k)</span></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">	k += i;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;k=%d \n&quot;</span>, k);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;last k=%d \n&quot;</span>, k);</span><br></pre></td></tr></table></figure><br>从打印结果可以看出，退出for循环的并行区域后，共享变量k的值变成了103，而不是保持原来的100不变。<br>由于在并行区域内是多个线程并行执行的，最后到底是将那个线程的最终计算结果赋给了对应的共享变量呢？OpenMP规范中指出，如果是循环迭代，那么是将最后一次循环迭代中的值赋给对应的共享变量；如果是section构造，那么是最后一个section语句中的值赋给对应的共享变量。注意这里说的最后一个section是指程序语法上的最后一个，而不是实际运行时的最后一个运行完的。<br>如果是类（class）类型的变量使用在lastprivate参数中，那么使用时有些限制，需要一个可访问的，明确的缺省构造函数，除非变量也被使用作为firstprivate子句的参数；还需要一个拷贝赋值操作符，并且这个拷贝赋值操作符对于不同对象的操作顺序是未指定的，依赖于编译器的定义。</p>
<h2 id="threadprivate子句"><a href="#threadprivate子句" class="headerlink" title="threadprivate子句"></a>threadprivate子句</h2><p>threadprivate子句用来指定全局的对象被各个线程各自复制了一个私有的拷贝，即各个线程具有各自私有的全局对象。<br>用法如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp threadprivate(list) new-<span class="meta-keyword">line</span></span></span><br></pre></td></tr></table></figure><br>下面用threadprivate命令来实现一个各个线程私有的计数器，各个线程使用同一个函数来实现自己的计数。计数器代码如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> counter = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp threadprivate(counter)</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">increment_counter</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    counter++;</span><br><span class="line">    <span class="keyword">return</span> counter;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>如果对于静态变量也同样可以使用threadprivate声明成线程私有的，上面的counter变量如改成用static类型来实现时，代码如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">increment_counter2</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> counter = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp threadprivate(counter)</span></span><br><span class="line">    counter++;</span><br><span class="line">    <span class="keyword">return</span> counter;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>threadprivate和private的区别在于threadprivate声明的变量通常是全局范围内有效的，而private声明的变量只在它所属的并行构造中有效。<br>threadprivate的对应只能用于copyin，copyprivate，schedule，num_threads和if子句中，不能用于任何其他子句中。<br>用作threadprivate的变量的地址不能是常数。<br>对于C++的类（class）类型变量，用作threadprivate的参数时有些限制，当定义时带有外部初始化时，必须具有明确的拷贝构造函数。<br>对于windows系统，threadprivate不能用于动态装载（使用LoadLibrary装载）的DLL中，可以用于静态装载的DLL中，关于windows系统中的更多限制，请参阅MSDN中有关threadprivate子句的帮助材料。<br>有关threadprivate命令的更多限制方面的信息，详情请参阅OpenMP2.5规范。</p>
<hr>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="shared子句"><a href="#shared子句" class="headerlink" title="shared子句"></a>shared子句</h2><p>shared子句用来声明一个或多个变量是共享变量。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">shared(<span class="built_in">list</span>)</span><br></pre></td></tr></table></figure><br>需要注意的是，在并行区域内使用共享变量时，如果存在写操作，必须对共享变量加以保护，否则不要轻易使用共享变量，尽量将共享变量的访问转化为私有变量的访问。<br>循环迭代变量在循环构造区域里是私有的。声明在循环构造区域内的自动变量都是私有的。</p>
<h2 id="default子句"><a href="#default子句" class="headerlink" title="default子句"></a>default子句</h2><p>default子句用来允许用户控制并行区域中变量的共享属性。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">default</span>(shared | none)</span><br></pre></td></tr></table></figure><br>使用shared时，缺省情况下，传入并行区域内的同名变量被当作共享变量来处理，不会产生线程私有副本，除非使用private等子句来指定某些变量为私有的才会产生副本。<br>如果使用none作为参数，那么线程中用到的变量必须显示指定是共享的还是私有的，除了那些由明确定义的除外。</p>
<h2 id="reduction子句"><a href="#reduction子句" class="headerlink" title="reduction子句"></a>reduction子句</h2><p>reduction子句主要用来对一个或多个参数条目指定一个操作符，每个线程将创建参数条目的一个私有拷贝，在区域的结束处，将用私有拷贝的值通过指定的运行符运算，原始的参数条目被运算结果的值更新。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">reduction(<span class="keyword">operator</span>:<span class="built_in">list</span>)</span><br></pre></td></tr></table></figure><br>下表列出了可以用于reduction子句的一些操作符以及对应私有拷贝变量缺省的初始值，私有拷贝变量的实际初始值依赖于redtucion变量的数据类型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>运算符</th>
<th>数据类型</th>
<th>默认初始值</th>
</tr>
</thead>
<tbody>
<tr>
<td>+</td>
<td>整数、浮点</td>
<td>0</td>
</tr>
<tr>
<td>-</td>
<td>整数、浮点</td>
<td>0</td>
</tr>
<tr>
<td>*</td>
<td>整数、浮点</td>
<td>1</td>
</tr>
<tr>
<td>&amp;</td>
<td>整数</td>
<td>所有位均为1</td>
</tr>
<tr>
<td>&#124;</td>
<td>整数</td>
<td>0</td>
</tr>
<tr>
<td>^</td>
<td>整数</td>
<td>0</td>
</tr>
<tr>
<td>&amp;&amp;</td>
<td>整数</td>
<td>1</td>
</tr>
<tr>
<td>&#124;&#x7C;</td>
<td>整数</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>例如一个整数求和的程序如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">int</span> i, sum = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for reduction(+: sum)</span></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) sum += i;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;sum = %ld \n&quot;</span>, sum);</span><br></pre></td></tr></table></figure><br>其中sum是共享的，采用reduction之后，每个线程根据reduction（+: sum）的声明算出自己的sum，然后再将每个线程的sum加起来。<br>reduction声明可以看作：</p>
<ol>
<li>保证了对sum的原则操作</li>
<li>多个线程的执行结果通过reduction中声明的操作符进行计算</li>
</ol>
<p>以加法操作符为例：<br>假设sum的初始值为10，<code>reduction（+: sum）</code>声明的并行区域中每个线程的sum初始值为0（规定），并行处理结束之后，会将sum的初始化值10以及每个线程所计算的sum值相加。<br>注意：<br>如果在并行区域内不加锁保护就直接对共享变量进行写操作，存在数据竞争问题，会导致不可预测的异常结果。共享数据作为private、firstprivate、lastprivate、threadprivate、reduction子句的参数进入并行区域后，就变成线程私有了，不需要加锁保护了。</p>
<h2 id="copyin子句"><a href="#copyin子句" class="headerlink" title="copyin子句"></a>copyin子句</h2><p>copyin子句用来将主线程中threadprivate变量的值拷贝到执行并行区域的各个线程的threadprivate变量中，便于线程可以访问主线程中的变量值<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">copyin(<span class="built_in">list</span>)</span><br></pre></td></tr></table></figure><br>copyin中的参数必须被声明成threadprivate的，对于类类型的变量，必须带有明确的拷贝赋值操作符。<br>对于前面threadprivate中讲过的计数器函数，如果多个线程使用时，各个线程都需要对全局变量counter的副本进行初始化，可以使用copyin子句来实现，示例代码如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">int</span> iterator;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel sections copyin(counter)</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> omp section</span></span><br><span class="line">	&#123;</span><br><span class="line">	    <span class="keyword">int</span> count1;</span><br><span class="line">            <span class="keyword">for</span> (iterator = <span class="number">0</span>; iterator &lt; <span class="number">100</span>; iterator++)</span><br><span class="line">	    &#123;</span><br><span class="line">                count1 = increment_counter();</span><br><span class="line">	    &#125;</span><br><span class="line">	    <span class="built_in">printf</span>(<span class="string">&quot;count1 = %ld \n&quot;</span>, count1);</span><br><span class="line">	&#125;</span><br><span class="line">        <span class="meta">#<span class="meta-keyword">pragma</span> omp section</span></span><br><span class="line">	&#123;</span><br><span class="line">	    <span class="keyword">int</span> count2;</span><br><span class="line">	    <span class="keyword">for</span> (iterator = <span class="number">0</span>; iterator &lt; <span class="number">200</span>; iterator++)</span><br><span class="line">	    &#123;</span><br><span class="line">		count2 = increment_counter();</span><br><span class="line">	    &#125;</span><br><span class="line">	    <span class="built_in">printf</span>(<span class="string">&quot;count2 = %ld \n&quot;</span>, count2);</span><br><span class="line">	&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;counter = %ld \n&quot;</span>, counter);</span><br></pre></td></tr></table></figure><br>从打印结果可以看出，两个线程都正确实现了各自的计数。</p>
<h2 id="copyprivate子句"><a href="#copyprivate子句" class="headerlink" title="copyprivate子句"></a>copyprivate子句</h2><p>copyprivate子句提供了一种机制用一个私有变量将一个值从一个线程广播到执行同一并行区域的其他线程。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">copyprivate(<span class="built_in">list</span>)</span><br></pre></td></tr></table></figure><br>copyprivate子句可以关联single构造，在single构造的barrier到达之前就完成了广播工作。copyprivate可以对private和threadprivate子句中的变量进行操作，但是当使用single构造时，copyprivate的变量不能用于private和firstprivate子句中。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> counter = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp threadprivate(counter)</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">increment_counter</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    counter++;</span><br><span class="line">    <span class="keyword">return</span> counter;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> count;</span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> omp single copyprivate(counter)</span></span><br><span class="line">    &#123;</span><br><span class="line">        counter = <span class="number">50</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    count = increment_counter();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;ThreadId: %ld, count = %ld \n&quot;</span>, omp_get_thread_num(), count);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>从打印结果可以看出，使用copyprivate子句后，single构造内给counter赋的值被广播到了其他线程里，但没有使用copyprivate子句时，只有一个线程获得了single构造内的赋值，其他线程没有获取single构造内的赋值。</p>
<hr>
<h1 id="OpenMP中的任务调度"><a href="#OpenMP中的任务调度" class="headerlink" title="OpenMP中的任务调度"></a>OpenMP中的任务调度</h1><p>OpenMP中，任务调度主要用于并行的for循环中，当循环中每次迭代的计算量不相等时，如果简单地给各个线程分配相同次数的迭代的话，会造成各个线程计算负载不均衡，这会使得有些线程先执行完，有些后执行完，造成某些CPU核空闲，影响程序性能。<br>为了解决这些问题，OpenMP中提供了几种对for循环并行化的任务调度方案。</p>
<h2 id="Schedule子句用法"><a href="#Schedule子句用法" class="headerlink" title="Schedule子句用法"></a>Schedule子句用法</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">schedule(type[,size])  <span class="comment">// size参数是可选的</span></span><br></pre></td></tr></table></figure>
<h3 id="type参数"><a href="#type参数" class="headerlink" title="type参数"></a>type参数</h3><p>表示调度类型，有四种调度类型如下：</p>
<ul>
<li>dynamic</li>
<li>guided</li>
<li>runtime</li>
<li>static<br>这四种调度类型实际上只有static、dynamic、guided三种调度方式，runtime实际上是根据环境变量来选择前三种中的某中类型。<h3 id="size参数-可选"><a href="#size参数-可选" class="headerlink" title="size参数 (可选)"></a>size参数 (可选)</h3>size参数表示循环迭代次数，size参数必须是整数。static、dynamic、guided三种调度方式都可以使用size参数，也可以不使用size参数。当type参数类型为runtime时，size参数是非法的（不需要使用，如果使用的话编译器会报错）。</li>
</ul>
<h2 id="静态调度-static"><a href="#静态调度-static" class="headerlink" title="静态调度(static)"></a>静态调度(static)</h2><p>当<code>parallel for</code>编译指导语句没有带schedule子句时，大部分系统中默认采用static调度方式，这种调度方式非常简单。假设有n次循环迭代，t个线程，那么给每个线程静态分配大约n/t次迭代计算。这里为什么说大约分配n/t次呢？因为n/t不一定是整数，因此实际分配的迭代次数可能存在差1的情况，如果指定了size参数的话，那么可能相差一个size。<br>静态调度时可以不使用size参数，也可以使用size参数。<br>不使用size参数时，分配给每个线程的是n/t次连续的迭代。使用size参数时，分配给每个线程的size次连续的迭代计算。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for schedule(static,5)</span></span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;i=%d, thread_id=%d \n&quot;</span>, i, omp_get_thread_num());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="动态调度-dynamic"><a href="#动态调度-dynamic" class="headerlink" title="动态调度(dynamic)"></a>动态调度(dynamic)</h2><p>动态调度是动态地将迭代分配到各个线程，动态调度可以使用size参数也可以不使用size参数，不使用size参数时是将迭代逐个地分配到各个线程，使用size参数时，每次分配给线程的迭代次数为指定的size次。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for schedule(dynamic,size)</span></span><br></pre></td></tr></table></figure></p>
<h2 id="guided调度（guided）"><a href="#guided调度（guided）" class="headerlink" title="guided调度（guided）"></a>guided调度（guided）</h2><p>guided调度是一种采用指导性的启发式自调度方法。开始时每个线程会分配到较大的迭代块，之后分配到的迭代块会逐渐递减。迭代块的大小会按指数级下降到指定的size大小，如果没有指定size参数，那么迭代块大小最小会降到1。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for schedule(guided,size)</span></span><br></pre></td></tr></table></figure></p>
<h2 id="runtime调度（rumtime）"><a href="#runtime调度（rumtime）" class="headerlink" title="runtime调度（rumtime）"></a>runtime调度（rumtime）</h2><p>runtime调度并不是和前面三种调度方式似的真实调度方式，它是在运行时根据环境变量OMP_SCHEDULE来确定调度类型，最终使用的调度类型仍然是上述三种调度方式中的某种。<br>例如在unix系统中，可以使用setenv命令来设置<code>OMP_SCHEDULE</code>环境变量：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">setenv OMP_SCHEDULE “dynamic, 2”</span><br></pre></td></tr></table></figure><br>上述命令设置调度类型为动态调度，动态调度的迭代次数为2。<br>在windows环境中，可以在 “系统属性 | 高级 | 环境变量” 对话框中进行设置环境变量。</p>
]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>优化加速</tag>
      </tags>
  </entry>
  <entry>
    <title>图像算子概述</title>
    <url>/2018/08/01/07.Constant-scale/</url>
    <content><![CDATA[<p>用机器视觉系统分析未知场景时，计算机并不预先知道图像中物体的尺度。我们需要同时考虑图像在多尺度下的描述，获知感兴趣物体的最佳尺度。如果不同的尺度下都有同样的关键点，那么在不同的尺度的输入图像下就都可以检测出来关键点匹配，也就是尺度不变性。<br><a id="more"></a><br>尺度空间中各尺度图像的模糊程度逐渐变大，能够模拟人在距离目标由近到远时目标在视网膜上的形成过程。<br>尺度越大图像越模糊。</p>
<h1 id="尺度空间表达与金字塔多分辨率表达"><a href="#尺度空间表达与金字塔多分辨率表达" class="headerlink" title="尺度空间表达与金字塔多分辨率表达"></a>尺度空间表达与金字塔多分辨率表达</h1><p>高斯核是唯一可以产生多尺度空间的核（<span class="exturl" data-url="aHR0cHM6Ly93d3cudGFuZGZvbmxpbmUuY29tL2RvaS9hYnMvMTAuMTA4MC83NTc1ODI5NzY=">Scale-space theory: A basic tool for analysing structures at different scales<i class="fa fa-external-link-alt"></i></span>）。一个图像的尺度空间$L(x,y,σ)$ ,定义为原始图像$I(x,y)$与一个可变尺度的2维高斯函数$G(x,y,σ)$卷积运算。</p>
<p>二维空间高斯函数：</p>
<script type="math/tex; mode=display">G(x_{i},y_{i},\sigma)=\frac{1}{2\pi\sigma}\exp\left( -\frac{(x-x_{i})^2+(y-y_{i})^2}{2\sigma^2} \right)</script><p>尺度空间：</p>
<script type="math/tex; mode=display">L(x,y,\sigma)=G(x,y,\sigma)\ast I(x,y,\sigma)</script><p>尺度是自然客观存在的，不是主观创造的。<code>高斯卷积只是表现尺度空间的一种形式</code>。<br>二维空间高斯函数是等高线从中心成正太分布的同心圆：</p>
<p><img src="\img\viusal-scale\Dog-1.jpg" alt="连续的高斯函数图">S分布不为零的点组成卷积阵与原始图像做变换，即每个像素值是周围相邻像素值的高斯平均。一个5*5的高斯模版如下所示：</p>
<p><img src="\img\viusal-scale\Dog-7-300x252.png" alt="离散的高斯"><code>高斯模版是圆对称的</code>，且卷积的结果使原始像素值有最大的权重，距离中心越远的相邻像素值权重也越小。<br>在实际应用中，在计算高斯函数的离散近似时，在大概3σ距离之外的像素都可以看作不起作用，这些像素的计算也就可以忽略。所以，通常程序只计算(6σ+1)*(6σ+1)就可以保证相关像素影响。</p>
<p>高斯模糊另一个很厉害的性质就是<code>线性可分</code>：使用二维矩阵变换的高斯模糊可以通过在水平和竖直方向各进行一维高斯矩阵变换相加得到。</p>
<p><img src="\img\viusal-scale\Dog-1-750x375.png" alt="高斯的线性可分">$O(N^2\ast m\ast n)$次乘法就缩减成了$O(N\ast m\ast n)+O(N\ast m\ast n)$次乘法。（N为高斯核大小，m,n为二维图像高和宽）。</p>
<h1 id="金字塔多分辨率"><a href="#金字塔多分辨率" class="headerlink" title="金字塔多分辨率"></a>金字塔多分辨率</h1><p>金字塔是早期图像多尺度的表示形式。图像金字塔化一般包括两个步骤：使用低通滤波器平滑图像；对平滑图像进行降采样（通常是水平，竖直方向1/2），从而得到一系列尺寸缩小的图像。</p>
<p><img src="\img\viusal-scale\Dog-2.png" alt="金字塔">上图中（a）是对原始信号进行低通滤波，（b）是降采样得到的信号。<br>而对于二维图像，一个传统的金字塔中，每一层图像由上一层分辨率的长、宽各一半，也就是四分之一的像素组成：</p>
<p><img src="\img\viusal-scale\Dog-3.png" alt="金字塔的采样"> </p>
<h1 id="多尺度和多分辨率"><a href="#多尺度和多分辨率" class="headerlink" title="多尺度和多分辨率"></a>多尺度和多分辨率</h1><p>尺度空间表达和金字塔多分辨率表达之间最大的不同是：</p>
<ul>
<li>尺度空间表达是由不同高斯核平滑卷积得到，在所有尺度上有相同的分辨率；</li>
<li>而金字塔多分辨率表达每层分辨率减少固定比率。<br>所以，金字塔多分辨率生成较快，且占用存储空间少；而多尺度表达随着尺度参数的增加冗余信息也变多。<br>多尺度表达的优点在于图像的局部特征可以用简单的形式在不同尺度上描述；而金字塔表达没有理论基础，难以分析图像局部特征。</li>
</ul>
<hr>
<h1 id="Moravec算子"><a href="#Moravec算子" class="headerlink" title="Moravec算子"></a>Moravec算子</h1><p>1977年，Moravec提出了兴趣点（Points of Interests)的概念，并应用于解决Stanford Cart的导航问题。1981年， Moravec在International Joint Conference on Artificial Intelligence发表了篇题为：Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover的文章，并将其应用与立体匹配。</p>
<p>Moravec的原理如果有一句话来说就是：通过滑动二值矩形窗口寻找灰度变化的局部最大值。具体来说主要包括四个过程：</p>
<h2 id="滑动窗口计算灰度变化（Calculate-intensity-variantion-from-shifting-windows）"><a href="#滑动窗口计算灰度变化（Calculate-intensity-variantion-from-shifting-windows）" class="headerlink" title="滑动窗口计算灰度变化（Calculate intensity variantion from shifting windows）"></a>滑动窗口计算灰度变化（Calculate intensity variantion from shifting windows）</h2><p>滑动窗口在现有的技术中已经有了很多应用，如模板匹配、目标检测（hog特征的行人检测）等。在Moravec算子中，一般窗口的大小取3×3、5×5、7×7等等，但是随着窗口的增大，计算量也就越大。Moravec算子通过对窗口的水平、垂直和对角八个方向进行移动（Horizontally、Vertically and four diagonals），计算原窗口与滑动窗口差的平方和来得到灰度的变化。我们进一步通过下图一个3×3的滑窗来进行说明：</p>
<p><img src="\img\viusal-scale\Moravec1.png" alt="Moravec的滑动窗口"> 图中，红色框表示的是原始框，而蓝色框表示向右上的滑动框，白色框表示前景255，黑色框表示背景0。那么原始框和滑动框的灰度变化通过对应位置差的平方和来表示，也即通过下式来计算：</p>
<script type="math/tex; mode=display">\sum_{i=1}^{9}{(A_{i}-B_{2})^2}</script><p>同样，根据上式计算另外七个方向滑动框的灰度变化（水平向左、水平向右、垂直向上、垂直向下以及四个对角）。至此，我们就计算完成了8个方向的灰度变化，我们称此操作位Moravec operator（Moravec算子）。</p>
<h2 id="构造角点性映射图（Construct-the-cornerness-map）"><a href="#构造角点性映射图（Construct-the-cornerness-map）" class="headerlink" title="构造角点性映射图（Construct the cornerness map）"></a>构造角点性映射图（Construct the cornerness map）</h2><p>在构造角点映射图之前，我们先来分析下，通过上式的我们可以得到角点吗？或者凭什么通过计算两个框对应位置的差的平方和就可以检测到角点？问题问得好，我们来看下面的图：</p>
<p><img src="\img\viusal-scale\Moravec3.png" alt="Moravec的角点">上面四张图上的四个红色的框表示我们正在处理的窗，第一幅图中的窗在表示在目标内部或者是背景上，该区域灰度分布均与，通过对其在8个方向上灰度，灰度变化很小；第二幅图中的窗跨在图像的边缘处，当垂直于边缘方向滑动窗口时将会导致灰度的很大变化，而沿着边缘滑动窗时，灰度变化较小；第三幅图中的窗在角点处，不管往哪个方向滑动窗口，都会导致灰度的很大变化；而第四幅图中的框内是一个离散点，滑动窗向任意方向滑动也会导致灰度的很大变化。</p>
<p>因此，通过上面的描述和分析，我们可以知道，Moravec算子可以作为一种角点性的度量，这种度量是通过求8个方向的滑窗来的最小值来表示。用公式表示如下：</p>
<script type="math/tex; mode=display">C(x,y)=min(V_{u,v}(x,y))</script><p>我们通过下图来描述角点映射图的构造：</p>
<p><img src="\img\viusal-scale\Moravec5.png" alt="Moravec的角点">上图中的是通过Moravec算子计算得到的，其中1表示$1\times 255^2$，2表示$2\times 255^2$。通过上图可以知道：</p>
<ol>
<li>角点位于局部最大值处，我们可以应用非极大值抑制找到局部最大值（non-maximal suppression）。</li>
<li>离散点（噪声点）与角点有相同的角点性（cornerness），因此Moravec算子对噪声敏感，但是通过增大滑窗的大小可以对噪声起到一定的抑制作用，可同时增加了计算量。另一方面，可以通过设定一个阈值T来对cornerness map进行二值化，小于阈值T的cornerness map设置为0，从而对离散点的局部最大值进行抑制。<br>阈值的选择引用原文的话：<blockquote>
<p>Choosing this threshold is difficult as it must be set high enough to avoid these false corners(isolated pixel), but low enough to retain as many true corners as possible.</p>
</blockquote>
</li>
<li>Moravec算子不能应用与图像边界的一定区域（标记为X的区域），对于这部分区域，一般直接忽略，在cornerness map中这些区域对应的值置0。</li>
</ol>
<p>Moravec算子对角点的检测效果还不错，但是对于对角线上的角点容易出现误检。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Moravec算子作为第一个广泛应用的角点检测算法，开创了角点检测的新纪念，后续的很多角点检测算子都是在其基础上通过扩展得到的。但是Moravec算子也存在诸如方向各异性、噪声敏感、对旋转不具备不变形（角点不具备repeatability）、滑动窗内的各个像素权重同质性（中心像素权重大，离中心越远，权重越小）的缺点，有待改进。<br>三篇参考：<br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tlenVuaGFpL2FydGljbGUvZGV0YWlscy8xMTE3NjA2NQ==">https://blog.csdn.net/kezunhai/article/details/11176065<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9jb2RlLmdvb2dsZS5jb20vYXJjaGl2ZS9wL2pmZWF0dXJlbGliLw==">https://code.google.com/archive/p/jfeaturelib/<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly93d3cucmkuY211LmVkdS9wdWJfZmlsZXMvcHViNC9tb3JhdmVjX2hhbnNfMTk4MF8xL21vcmF2ZWNfaGFuc18xOTgwXzEucGRm">https://www.ri.cmu.edu/pub_files/pub4/moravec_hans_1980_1/moravec_hans_1980_1.pdf<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h1 id="Harris算子"><a href="#Harris算子" class="headerlink" title="Harris算子"></a>Harris算子</h1><p>在前面的博文中，介绍了Moravec算子，并对moravec算子的不足也进行了简单的描述，Harris算子针对Moravec算子的不足，提出了以下的改进……</p>
<h2 id="Moravec算子各向异性响应（Anisotropic-Response-of-Operator）"><a href="#Moravec算子各向异性响应（Anisotropic-Response-of-Operator）" class="headerlink" title="Moravec算子各向异性响应（Anisotropic Response of Operator）"></a>Moravec算子各向异性响应（Anisotropic Response of Operator）</h2><p>Moravec算子仅仅在8个方向（水平、垂直和四个对角方向）计算灰度变化，为了对其扩展，有必要设计一个可以在任何方向对灰度变化进行测度的函数。1988年，Harris和Stephen通过对Moravec算子进行展开，推导得到了Plessey算子，也即Harris算子。</p>
<p>我们先来看看与Harris相关的背景知识。通常，Prewitt算子被用来对图像的梯度进行近似。然而，在实际应用中，一阶梯度通过下图中的公式来进行近似：</p>
<p><img src="\img\viusal-scale\harris.png" alt="Moravec的角点">对Morevec算子进行分析可以得到：Two Morevec windows中对应像素差的和可以作为图像梯度的合理近似。我们再来看下图：</p>
<p><img src="\img\viusal-scale\harris-2.png" alt="Moravec的角点">通过对上图的分析，我们有可以进一步得到：morevec算子中的灰度变化可以采用图像梯度进行近似。灰度的变化可以表示为图像梯度的函数，公式表示如下：</p>
<script type="math/tex; mode=display">V_{u,v}(x,y)=\sum_{\forall i \in mask(x,y)}^{}{\left( u\frac{\partial I_{i}}{\partial x} +v\frac{\partial I_{i}}{\partial y}\right)^2}</script><p>其中，（u,v)表示滑动，x方向为（1,0），y方向为（0,1），微分的计算如上图所示。</p>
<p>到这里，大家非常明了：上式可以对moravec算子中的灰度变化计算进行精确的逼近。但是又与Moravec算子中灰度变化不同的是通过合理的选择（u，v）可以对任何方向的灰度变化进行测度。</p>
<h2 id="噪声响应（Noise-Response）"><a href="#噪声响应（Noise-Response）" class="headerlink" title="噪声响应（Noise Response）"></a>噪声响应（Noise Response）</h2><p>在Moravec算子中，滑动窗采用的是方形的（square window），方形窗使得不同方向上的中心像素与边界像素的欧式距离是变化的。为了克服这个问题，Harris&amp;Stephen提出只需将方向窗改成圆窗（circle window）。同时，窗中的每个像素是同等地位的，理论上应该是离中心越近的权重越大，而离中心越远，权重越小，因此我们加入高斯权重。因此，灰度变化的新测度方式可以通过下图来表示：</p>
<p><img src="\img\viusal-scale\harris4.png" alt="Moravec的角点">通过公式表示如下：</p>
<script type="math/tex; mode=display">V_{u,v}(x,y)=\sum_{\forall i \in mask(x,y)}^{}{W_{i}\left( u\frac{\partial I_{i}}{\partial x} +v\frac{\partial I_{i}}{\partial y}\right)^2}</script><p>其中，wi表示位置i处的高斯权重。</p>
<h2 id="边缘的强响应（Large-Response-of-Edge）"><a href="#边缘的强响应（Large-Response-of-Edge）" class="headerlink" title="边缘的强响应（Large Response of Edge）"></a>边缘的强响应（Large Response of Edge）</h2><p>因为Moravec算子在边缘处很容易出现误检，Harris&amp;Stephen通过考虑不同方向的灰度度量形成新的角度性测度（cornerness measure）。接着，我们对上面的式子进行变换，如下式：</p>
<script type="math/tex; mode=display">\begin{equation} \begin{split} V_{u,v}(x,y)&=\sum_{\forall i\in mask(x,y)}^{}{\omega_{i}\left( u\frac{\partial I}{\partial x} +v\frac{\partial I}{\partial y}\right)^2}\\ &=\sum_{\forall i\in mask(x,y)}^{}{\omega_{i}\left( u^2\frac{\partial I^2}{\partial x} +2uv\frac{\partial I}{\partial x}\frac{\partial I}{\partial y}+v^2\frac{\partial I^2}{\partial y}\right)}\\&=Au^2+2Cuv+Bv^2 \end{split} \end{equation}\\
where A=\left( \frac{\partial I}{\partial x} \right)^2\otimes\omega_{i}，B=\left( \frac{\partial I}{\partial x} \right)^2\otimes\omega_{i}，C=\left(\frac{\partial I}{\partial x}\frac{\partial I}{\partial y}\right)\otimes\omega_{i},</script><p>Harris&amp;Stephen同时也注意到，上式可以写成：</p>
<script type="math/tex; mode=display">\begin{equation} \begin{split} V_{u,v}(x,y)&=Au^2+2Cuv+Bv^2 \\&=\left[\begin{matrix}u&v\end{matrix}\right]M\left[\begin{matrix}u\\v\end{matrix}\right]\end{split} \end{equation}\\
where：M=\left[\begin{matrix}A&C\\C&B\end{matrix}\right]</script><p>对上面的矩阵M，其特征值与图像表面的主曲率是成正比的，并且形成了对M的旋转不变的描述（Proportional to the principle curvature of the image surface and form a rotationally invariant description of M）。然后，由于M是通过水平和垂直方向的梯度来近似的，他们不是真正的旋转不变。</p>
<p>同样，与Moravec算子一样，我们再来看下面的四张张图：</p>
<p><img src="\img\viusal-scale\harris8-750x270.png" alt="Moravec的角点">图中A表示在一个物体的内部或背景上，窗口内的灰度值相对不变，因此该窗口表面上几乎没有曲率，因此M的特征值相对很小；B窗口在一个边缘处，垂直于边缘的地方将有明显很大的曲率，而平行于边缘的地方几乎没什么曲率，因此该形式下M的特征值一个会比较大，另一个较小；C和D对应于角度和离散点，在两个方向都会有很大的曲率，因此，M的特征值都将会很大。假设r1和r2是M的两个特征值，通过上面的分析，可以将一个平面表示为以下三个可区分的区域：</p>
<p><img src="\img\viusal-scale\harris9-300x292.png" alt="Moravec的角点">Harris&amp;Stephen提出下面的角点性测度（cornerness measure）</p>
<script type="math/tex; mode=display">C(x,y)=det(M)-k(trace(M))^2\\
det(M)=\lambda_{1}\lambda_{2}=AB-C^2\\
trace(M)=\lambda_{1}+\lambda_{2}=A+B</script><p>k一般取值04~0.6。</p>
<h2 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h2><ol>
<li>对每一个像素计算自相关矩阵M<script type="math/tex; mode=display">M=\left[\begin{matrix}A&C\\C&B\end{matrix}\right]\\
A=\left( \frac{\partial I}{\partial x} \right)^2\otimes\omega_{i}，B=\left( \frac{\partial I}{\partial x} \right)^2\otimes\omega_{i}，C=\left(\frac{\partial I}{\partial x}\frac{\partial I}{\partial y}\right)\otimes\omega_{i}</script></li>
<li>构造角点性映射图（Construct cornerness map）<script type="math/tex; mode=display">C(x,y)=det(M)-k(trace(M))^2\\
det(M)=\lambda_{1}\lambda_{2}=AB-C^2\\
trace(M)=\lambda_{1}+\lambda_{2}=A+B</script></li>
<li>阈值化，对得到的C(x,y)进行阈值</li>
<li>非极大值抑制</li>
</ol>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>Harris算子针对Moravec算子的不足进行了改进，提高了特征点的检测率以及Repeatability。但是，Harris算子计算量大，对尺度很敏感，不具有尺度不变形；另外Harris对特征点的定位也不是很精确，而且Harris也是各向异性的，对噪声敏感。<br>两篇参考：<br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tlenVuaGFpL2FydGljbGUvZGV0YWlscy8xMTI2NTE2Nw==">https://blog.csdn.net/kezunhai/article/details/11265167<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpYXhpYXpscy9hcnRpY2xlL2RldGFpbHMvODE4NDEyNA==">https://blog.csdn.net/xiaxiazls/article/details/8184124<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h1 id="SUSAN算子"><a href="#SUSAN算子" class="headerlink" title="SUSAN算子"></a>SUSAN算子</h1><p>SUSAN算子（Smallest Univalue Segment Assimilating Nucleus）是一种高效的边缘和角点检测算子，并且具有结构保留的降噪功能（structure preserving noise reduction )。</p>
<p>SUSAN算子通过用一个圆形模板在图像上移动，一般这个圆形模板的半径是（3.4pixels）的包含37个像素。模板内的每一个像素与中心像素进行比较。</p>
<p>为了介绍和分析的需要，我们首先来看下面这个图：</p>
<p><img src="\img\viusal-scale\susan1.png" alt="SUSAN算子">该图是在一个白色的背景上，有一个深度颜色的区域（dark area），用一个圆形模板在图像上移动，若模板内的像素灰度与模板中心的像素（被称为核Nucleus）灰度值小于一定的阈值，则认为该点与核Nucleus具有相同的灰度，满足该条件的像素组成的区域就称为USAN（Univalue Segment Assimilating Nucleus）。</p>
<p>接下来，我们来分析下上图中的五个圆形模的USAN值。对于上图中的e圆形模板，它完全处于白色的背景中，根据前面对USAN的定义，该模板处的USAN值是最大的；随着模板a到d的移动，USAN值逐渐减少；当圆形模板移动到b处时，其中心位于边缘直线上，此时其USAN值逐渐减少为最大值的一半；而圆形模板运行到角点处a时，此时的USAN值最小。因此通过上面的描述：我们可以推导出：边缘处的点的USAN值小于或等于最大值一半。由此，我们可以得出SUSAN提取边缘和角点算法的基本原理：在边缘或角点处的USAN值最小，可以根据USAN区域的大小来检测边缘、角点等特征的位置和方向信息。</p>
<p>上面都是口头阐述，文字的力量是单薄的，下面我们进入公式阶段。SUSAN算子通过用一个圆形模板在图像上移动，一般这个圆形模板的半径是（3.4pixels）的包含37个像素。模板内的每一个像素与中心像素进行比较，比较方式如下所示：</p>
<script type="math/tex; mode=display">C(\overrightarrow{r},\overrightarrow{r_{0}})=
\begin{equation}
\left\{
\begin{array}{lr}
1&if\left| I(\overrightarrow{r})-I(\overrightarrow{r_{0}}) \right|\leq t \\
0&if\left| I(\overrightarrow{r})-I(\overrightarrow{r_{0}}) \right|< t \\
\end{array}
\right.
\end{equation}</script><p>其中r0是中心像素，r是掩膜内的其他像素，t是一个像素差异阈值（通常对于对比度比较低的区域，选取较小的t；反之，则t的阈值可以选择大些）。 接着，对上式进行统计，统计方式如下式：</p>
<script type="math/tex; mode=display">n(\overrightarrow{r_{0}})=\sum_{\overrightarrow{r_{0}}}^{}{C(\overrightarrow{r},\overrightarrow{r_{0}})}</script><p>得到的n值就是USAN的大小。<br>得到USAN值后，通过阈值化就可以得到初步的边缘响应，公式表示如下：</p>
<script type="math/tex; mode=display">C(\overrightarrow{r},\overrightarrow{r_{0}})=
\begin{equation}
\left\{
\begin{array}{lr}
g-n(\overrightarrow{r_{0}})&if &n(\overrightarrow{r_{0}}) < g \\
0 && otherwise
\end{array}
\right.
\end{equation}</script><p>其中，g=3Nmax/4，也即g的取值为USAN最大值的3/4。USAN值越小，边缘的响应就越强。<br>对得出的边缘响应进行非极大值抑制，就可以得到图像的边缘信息了。</p>
<p>以上完成了SUSAN检测边缘的功能，或许你已经想到了怎么用SUSAN算子来检测角点了。通过上面对a、b、c、d、e等几个圆形模板的USAN值的分析，当模板的中心位于角点处时，USAN的值最小。下面简单叙述下利用SUSAN算子检测角点的步骤：</p>
<ol>
<li>利用圆形模板遍历图像，计算每点处的USAN值</li>
<li>设置一阈值g，一般取值为1/2(Max(n)， 也即取值为USAN最大值的一半，进行阈值化，得到角点响应</li>
<li>使用非极大值抑制来寻找角点。</li>
</ol>
<p>通过上面的方式得到的角点，存在很大伪角点。为了去除伪角点，SUSAN算子可以由以下方法实现：①计算USAN区域的重心，然后计算重心和模板中心的距离，如果距离较小则不是正确的角点；②判断USAN区域的重心和模板中心的连线所经过的像素都是否属于USAN区域的像素，如果属于那么这个模板中心的点就是角点。</p>
<h2 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h2><p>SUSAN算子是一个原理简单、易于了解的算子。由于其指数基于对周边象素的 灰度比较，完全不涉及梯度的运算，因此其抗噪声能力很强，运算量也比较小；同时，SUSAN算子还是一个各向同性的算子；最后，通过控制参数t和g，可以根据具体情况很容易地对不同对比度、不同形状的图像通过设置恰当的t和g进行控制。比如图像的对比度较大，则可选取较大的t值，而图像的对比度较小，则可选取较小的t值。总之，SUSAN算子是一个非常难得的算子，不仅具有很好的边缘检测性能；而且对角点检测也具有很好的效果。<br>参考：<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V6dW5oYWkvYXJ0aWNsZS9kZXRhaWxzLzExMjY5Nzkz">http://blog.csdn.net/kezunhai/article/details/11269793<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3VzZXJzLmZtcmliLm94LmFjLnVrL35zdGV2ZS9zdXNhbi8=">http://users.fmrib.ox.ac.uk/~steve/susan/<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXVndXNkaS9hcnRpY2xlL2RldGFpbHMvOTAxMjU1NQ==">http://blog.csdn.net/augusdi/article/details/9012555<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h1 id="FAST算子"><a href="#FAST算子" class="headerlink" title="FAST算子"></a>FAST算子</h1><p>FAST算子是Rosten等人在SUSAN角点特征检测方法的基础上利用机器学习方法提出的。全名是：Features from Accelerated Segment Test。它计算速度快，可以应用与实时场景中。在FAST特征提出之后，实时计算机视觉应用中特征提取性能才有显著改善。</p>
<p>目前以其高计算效率(computational performance)、高可重复性(high repeatability)成为计算机视觉领域最流行的角点检测方法。<br>FAST算法包含3个主要步骤：</p>
<ol>
<li>对固定半径圆上的像素进行分割测试，通过逻辑测试可以去处大量的非特征候选点；</li>
<li>基于分类的角点特征检测，利用ID3 分类器根据16个特征判决候选点是否为角点特征，每个特征的状态为-1，0，1。</li>
<li>利用非极大值抑制进行角点特征的验证。</li>
</ol>
<p>下面我们对这3个步骤进行分析。</p>
<h2 id="固定半径圆上的像素进行分割测试（Segment-Test）"><a href="#固定半径圆上的像素进行分割测试（Segment-Test）" class="headerlink" title="固定半径圆上的像素进行分割测试（Segment Test）"></a>固定半径圆上的像素进行分割测试（Segment Test）</h2><p>在分割测试中，可以去除大量的非候选角点，这样就可以把可能的角点筛选出来。分割测试是通过对一固定半径的圆形模板的比较和计算进行的，在FAST角点检测算子中，一般是通过半径为3.4 pixel、外围16个像素的圆的作为模板，通过下图我们来具体分析下分割测试。</p>
<p><img src="\img\viusal-scale\fast1-1024x519.png" alt="SUSAN算子">在上图中，是12点的分割测试角点检测算法的示意图（还有9点的分割测试角点检测，具体参考后面的参考资料）。12点分割测试角点检测算法是在一个图像块上进行，如上图中的左边方块，其中p是中心像素点，12点取的是图上用弧线连接的12个点的像素值都大于或都小于中心像素值，则认为该点处是候选角点（为什么选择12点，因为通过测试，12点的角点检测性能最稳定、速度更快、效果也很好，当然有些文献指出9点的方式也很好）。分割测试是怎么进行的呢？用下面的公式来说，便可一目了然。</p>
<script type="math/tex; mode=display">S_{p\rightarrow x}=
\begin{equation}
\left\{
\begin{array}{lr}
d, I_{p\rightarrow x}<I_p-t & darker\\
s, I_p-t<I_{p\rightarrow x}<I_p+t& similar\\
b,I_p+t<I_{p\rightarrow x} & brighter
\end{array}
\right.
\end{equation}</script><p>上式中，t是一个阈值（默认取值为10，不同场景取值有差异），Ip表示的是中心像素的像素值，Ip-&gt;x表示的是圆形模板中的像素值上式的意思是：当中心像素的像素值Ip小于x处的像素值Ip-&gt;x+t时，则该像素属于darker，Sp-&gt;x=d，其他两种情况分别表示亮些和相似。这样一个块（圆形）区域就可以分成d、s和b三种类型。这时候只要统计圆形区域中d或b的次数，只要d或b出现的次数大于n（（当是12点分割测试角点检测时，n=12；当是9点时，则n=9），那么该点就被认为是候选角点。</p>
<p>在分割测试步骤中，为了加快速度，其实不需要对这些像素进行逐一的比较。简单来说：首先比较1、5、9、13处点的像素值（也即水平方向和垂直方向上的4个点）与中心像素值的大小，如果这四个点中的像素值有3个或3个以上大于Ip+t或小于Ip-t，那么则认为该处是一个候选角点，否则就不可能是角点。</p>
<h2 id="ID3决策树算法来训练角点检测"><a href="#ID3决策树算法来训练角点检测" class="headerlink" title="ID3决策树算法来训练角点检测"></a>ID3决策树算法来训练角点检测</h2><p>通过上式中比较，可以将模板内的像素分成三部分d、s、b，分别记为：Pd，Ps，Pb。因此对于每个Sp-&gt;x都属于Pd，Ps，Pb中的一个。另外，令Kp为true，如果p为角点，否则为false。通过ID3算法来选择具有最大信息增益的像素来判断一个像素是否为角点。Kp的熵用下式来计算：</p>
<script type="math/tex; mode=display">c=|\left\{ p|K_{p}=true \right\}|，角点\\
\bar{c}=|\left\{ p|K_{p}=false \right\}|，非角点</script><p>某一像素的信息增益通过下式来表示：</p>
<script type="math/tex; mode=display">H(P)-H(P_{d})-H(Ps)-H(P_{b})</script><p>对上述像素依次进行如上处理，选择像素增益最大的像素作为判断角点的依据，生成决策树，从而实现角点的正确分类。</p>
<h2 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h2><p>在上面的分割测试中，没有计算角点响应函数（Corner Response Function），非极大值抑制无法直接应用于提取的特征。因此，定义一个角点响应函数V，考虑到分割测试的特征以及计算速度的需要，角点响应函数的定义如下：</p>
<script type="math/tex; mode=display">V=\max(\sum_{x\in S_{bright}}^{}{|I_{p\rightarrow x}-I_{p}|-t}，\sum_{x\in S_{dark}}^{}{|I_{p}-I_{p\rightarrow x}|-t})</script><p>定义了角点响应函数后，就可以采用常规的非极大值抑制方法对非角点进行排除了。</p>
<h2 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h2><p>FAST角点检测算法是一种具有高计算效率(computational performance)、高可重复性(high repeatability)特征提取算子，在立体图像匹配、图像配准、目标识别、目标跟踪、场景重构等领域得到了广泛的应用，成为计算机视觉领域最流行的角点检测方法。但是，噪声对该算子的影响比较大，而且阈值t对算子的影响比较大。<br>参考：<br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2V6dW5oYWkvYXJ0aWNsZS9kZXRhaWxzLzExMjkwNzQ5">http://blog.csdn.net/kezunhai/article/details/11290749<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3d3dy5lZHdhcmRyb3N0ZW4uY29tL3dvcmsvZmFzdC5odG1s">FAST Corner Detection — Edward Rosten<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYW5zaGFuMTk4NC9hcnRpY2xlL2RldGFpbHMvODg2NzY1Mw==">A Brief History of FAST corner detector<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3d3dzYuaW4udHVtLmRlL01haW4vUmVzZWFyY2hBZ2FzdA==">AGAST Corner Detector: faster than FAST and even FAST-ER<i class="fa fa-external-link-alt"></i></span></p>
<hr>
<h1 id="DoG算子"><a href="#DoG算子" class="headerlink" title="DoG算子"></a>DoG算子</h1><p>DoG算子是由Lowe D.G.提出的，对噪声、尺度、仿射变化和旋转等具有很强的鲁棒性，能够提供更丰富的局部特征信息。</p>
<p>作为一个增强算法，DOG可以被用来增加边缘和其他细节的可见性，大部分的边缘算子使用增强高频信号的方法，但是因为随机噪声也是高频信号，很多锐化算子也增强了噪声。DOG算法去除的高频信号中通常包含了随机噪声，所以这种方法是最适合处理那些有高频噪声的图像。这个算法的一个主要缺点就是在调整图像对比度的过程中信息量会减少。</p>
<p>当它被用于图像增强时，DOG算法中两个高斯核的半径之比通常为4:1或5:1。当k设为1.6时，即为高斯拉普拉斯算子的近似。高斯拉普拉斯算子在多尺度多分辨率像片中，用于近似高斯拉普拉斯算子两个高斯核的确切大小决定了两个高斯模糊后的影像间的尺度。</p>
<p>DOG也被用于尺度不变特征变换中的斑点检测。事实上，DOG算法作为两个多元正态分布的差通常总额为零，把它和一个恒定信号进行卷积没有意义。当K约等于1.6时它很好的近似了高斯拉普拉斯变换，当K约等于5时又很好的近似了视网膜上神经节细胞的视野。它可以很好的作为一个实时斑点检测算子和尺度选择算子应用于递归程序。</p>
<h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>我们这次从数学推导开始，以LoG作为铺垫，再介绍DoG。</p>
<p>Difference of Gaussian(DOG)是高斯函数的差分。它是可以通过将图像与高斯函数进行卷积得到一幅图像的低通滤波结果，即去噪过程，这里的Gaussian和高斯低通滤波器的高斯一样，是一个函数，即为正态分布函数。同时，它是对Laplacian of Gaussian(LoG)的近似，在某一尺度上的特征检测可以通过对两个相邻高斯尺度空间的图像相减，得到DoG的响应值图像。</p>
<p>Laplace算子对通过图像进行操作实现边缘检测时，对离散点和噪声比较敏感。于是，首先对图像进行高斯暖卷积滤波进行降噪处理，再采用Laplace算子进行边缘检测，就可以提高算子对噪声和离散点的Robust, 这一个过程中Laplacian of Gaussian(LOG)算子就诞生了。</p>
<p>高斯卷积函数定义为：</p>
<script type="math/tex; mode=display">G_{\sigma_{1}}(x,y)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left( -\frac{x^2+y^2}{2\sigma^2} \right)</script><p>而原始图像与高斯卷积定义为：</p>
<script type="math/tex; mode=display">\Delta[G_{\sigma}(x,y)\ast f(x,y)]=[\Delta G_{\sigma}(x,y)]\ast f(x,y)=LoG\ast f(x,y)</script><p>因为：</p>
<script type="math/tex; mode=display">\frac{d}{dt}[h(t)\ast f(t)] = \frac{d}{dt}\int_{}^{}f(\tau)h(t-\tau)d\tau=\int_{}^{}f(\tau)\frac{d}{dt}h(t-\tau)d\tau=f(t)\ast \frac{d}{dt}h(t)</script><p>所以Laplacian of Gaussian(LOG)</p>
<script type="math/tex; mode=display">\Delta G_{\sigma}(x,y)</script><p>可以通过先对高斯函数进行偏导操作，然后进行卷积求解。公式表示为：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial x}G_{\sigma}(x,y)=\frac{\partial}{\partial x}e^{-(x^2+y^2)/2\sigma^2}=-\frac{1}{\sigma^2}e^{-(x^2+y^2)/2\sigma^2}</script><script type="math/tex; mode=display">\frac{\sigma^2}{\sigma^2x}G_{\sigma}(x,y)=\frac{x^2}{\sigma^4}e^{-(x^2+y^2)/2\sigma^2}-\frac{1}{\sigma^2}e^{-(x^2+y^2)/2\sigma^2}=\frac{x^2-\sigma^2}{\sigma^4}e^{-(x^2+y^2)/2\sigma^2}</script><p>因此，我们可以把LOG核函数定义为：</p>
<script type="math/tex; mode=display">LoG\stackrel{\Delta}{=}\Delta G_{\sigma}(x,y)=\frac{\sigma^2}{\sigma x^2}G_{\sigma}(x,y)+\frac{\sigma^2}{\sigma y^2}G_{\sigma}(x,y)=\frac{x^2+y^2-2\sigma^2}{\sigma^4}e^{-(x^2+y^2)/2\sigma^2}</script><script type="math/tex; mode=display">\begin{equation}
\begin{split}
\nabla^2g(x,y)&=\frac{\partial^2g(x,y,\sigma)}{\partial x^2}U_{x}+\frac{\partial^2g(x,y,\sigma)}{\partial y^2}U_{y} \\&=
\frac{\partial\nabla g(x,y,\sigma)}{\partial x}U_{x}+\frac{\partial\nabla g(x,y,\sigma)}{\partial y}U_{y} \\&=
\left( \frac{x^2}{\sigma^2}-1 \right)\frac{e^{-(x^2+y^2)/2\sigma^2}}{\sigma^2}+\left( \frac{y^2}{\sigma^2}-1 \right)\frac{e^{-(x^2+y^2)/2\sigma^2}}{\sigma^2} \\&=
\frac{1}{\sigma^2}\left( \frac{x^2+y^2}{\sigma^2}-2 \right)e^{-(x^2+y^2)/2\sigma^2}
\end{split}
\end{equation}</script><p>高斯函数和一级、二阶导数如下图所示：</p>
<p><img src="\img\viusal-scale\log-1024x341.png" alt="SUSAN算子">Laplacian of Gaussian计算可以利用高斯差分来近似，其中差分是由两个高斯滤波与不同变量的卷积结果求得的</p>
<script type="math/tex; mode=display">\sigma\nabla^2g(x,y,\sigma)=\frac{\partial g}{\partial\sigma}\approx\frac{g(x,y,\sigma)-g(x,y,\sigma)}{k\sigma-\sigma}</script><p>Difference of Gaussian(DOG)是高斯函数的差分。它是可以通过将图像与高斯函数进行卷积得到一幅图像的低通滤波结果，即去噪过程，这里的Gaussian和高斯低通滤波器的高斯一样，是一个函数，即为正态分布函数。同时，它对高斯拉普拉斯LoG(博文LOG算子介绍了实现原理)的近似，在某一尺度上的特征检测可以通过对两个相邻高斯尺度空间的图像相减，得到DoG的响应值图像。</p>
<p>两幅图像的高斯滤波表示为：</p>
<script type="math/tex; mode=display">g_{1}(x,y)=G_{\sigma_{1}}(x,y)*f(x,y) \\
g_{2}(x,y)=G_{\sigma_{2}}(x,y)*f(x,y)</script><p>最后，将上面滤波得到的两幅图像g1和g2相减</p>
<script type="math/tex; mode=display">g_{1}(x,y)-g_{2}(x,y)=G_{\sigma_{1}}*f(x,y) -
G_{\sigma_{2}}*f(x,y)=(G_{\sigma_{1}}-G_{\sigma_{2}})*f(x,y)=DoG*f(x,y)</script><p>即：可以DOG表示为：</p>
<script type="math/tex; mode=display">DoG\stackrel{\Delta}{=}G_{\sigma_{1}}-
G_{\sigma_{2}}=\frac{1}{\sqrt{2\pi}}\left( \frac{1}{\sigma_{1}}e^{-(x^2+y^2)/2\sigma_{1}^2} - \frac{1}{\sigma_{2}}e^{-(x^2+y^2)/2\sigma_{2}^2} \right)</script><p>在具体图像处理中，就是将两幅图像在不同参数下的高斯滤波结果相减，得到DoG图。</p>
<p><img src="\img\viusal-scale\DoG_tile.jpg" alt="SUSAN算子">标记红色当前像素点，黄色的圈标记邻接像素点，用这个方式，最多检测相邻尺度的26个像素点。如果它是所有邻接像素点的最大值或最小值点，则标记红色被标记为特征点，如此依次进行，则可以完成图像的特征点提取。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><span class="exturl" data-url="aHR0cDovL2ZvdXJpZXIuZW5nLmhtYy5lZHUvZTE2MS9sZWN0dXJlcy9ncmFkaWVudC9ub2RlOS5odG1s">Laplacian of Gaussian<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2ZvdXJpZXIuZW5nLmhtYy5lZHUvZTE2MS9sZWN0dXJlcy9ncmFkaWVudC9ub2RlMTAuaHRtbA==">Difference of Gaussian(DOG)<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3d3dy50aGlua2ZhY2UuY24vdGhyZWFkLTYyNi0xLTEuaHRtbA==">http://www.thinkface.cn/thread-626-1-1.html<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGlhb3dlaV9jcXUvYXJ0aWNsZS9kZXRhaWxzLzgwNjc4ODE=">http://blog.csdn.net/xiaowei_cqu/article/details/8067881<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGlhb3dlaV9jcXU=">http://blog.csdn.net/xiaowei_cqu<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuc2luYS5jb20uY24vcy9ibG9nXzRiZGIxNzBiMDEwMW92aWYuaHRtbA==">http://blog.sina.com.cn/s/blog_4bdb170b0101ovif.html<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYWJjamVubmlmZXIvYXJ0aWNsZS9kZXRhaWxzLzc2Mzk0ODg=">http://blog.csdn.net/abcjennifer/article/details/7639488<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3d3dy50YW5kZm9ubGluZS5jb20vZG9pL2Ficy8xMC4xMDgwLzc1NzU4Mjk3Ng==">http://www.tandfonline.com/doi/abs/10.1080/757582976<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL3d3dy52b2lkY24uY29tL2Jsb2cvaGg1NTU4MDAvYXJ0aWNsZS9wLTM1OTc5MjcuaHRtbA==">http://www.voidcn.com/blog/hh555800/article/p-3597927.html<i class="fa fa-external-link-alt"></i></span><br><span class="exturl" data-url="aHR0cDovL2ZvdXJpZXIuZW5nLmhtYy5lZHUvZTE2MS9sZWN0dXJlcy9ncmFkaWVudC9ub2RlOS5odG1s">http://fourier.eng.hmc.edu/e161/lectures/gradient/node9.html<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>图像处理</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>Apriltags</title>
    <url>/2018/08/07/09.apriltags/</url>
    <content><![CDATA[<p>AprilTag是一套免费开源的视觉定位系统，他们的主页：<span class="exturl" data-url="aHR0cHM6Ly9hcHJpbC5lZWNzLnVtaWNoLmVkdS9zb2Z0d2FyZS9hcHJpbHRhZy8=">https://april.eecs.umich.edu/software/apriltag/<i class="fa fa-external-link-alt"></i></span><br>根据他们的论文<span class="exturl" data-url="aHR0cHM6Ly9hcHJpbC5lZWNzLnVtaWNoLmVkdS9tZWRpYS9wZGZzL29sc29uMjAxMXRhZ3MucGRm">AprilTag - A robust and flexible visual fiducial system, ICRA 2011<i class="fa fa-external-link-alt"></i></span>，AprilTag主要的贡献是：</p>
<ul>
<li>提出了一种可靠的检测视觉基准的方法：基于图形的图像分割、基于局部梯度，可以精确估计线条。还描述了一种可以处理重要遮挡的四边形提取方法。</li>
<li>描述了2D条形码系统独有的特性：旋转的稳健性，以及自然图像产生的误报的稳健性。</li>
<li>号称比<span class="exturl" data-url="aHR0cHM6Ly9pbnNpZGUubWluZXMuZWR1L353aG9mZi9jb3Vyc2VzL0VFTkc1MTIvbGVjdHVyZXMvb3RoZXIvQVJUYWcucGRm">ARTag<i class="fa fa-external-link-alt"></i></span>和<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FzdGFuaW4vbWlycm9yLXN0dWRpZXJzdHViZQ==">Studierstube Tracker<i class="fa fa-external-link-alt"></i></span>更快更稳定。</li>
</ul>
<a id="more"></a> 
<hr>
<h1 id="ARToolkit"><a href="#ARToolkit" class="headerlink" title="ARToolkit"></a>ARToolkit</h1><p>ARToolkit的标签包含一个由黑色边框包围，没有用二进制编码，而是使用了诸如拉丁字符“A”之类的符号。主要缺点是计算成本，第二个缺点是难以生成彼此近似正交的模板。 ARToolkit使用的标签检测方案基于用户指定的阈值对输入图像进行简单的二值化。该方案非常快，但对于照明变化不稳健，而且无法处理标签边框的适度遮挡。 </p>
<hr>
<h1 id="ARTag"><a href="#ARTag" class="headerlink" title="ARTag"></a>ARTag</h1><p>ARTag是改进的方案，检测机制基于图像梯度，使其对光照变化具有鲁棒性，同时能够检测边界部分被遮挡的标签。ARTag还提供了基于前向纠错的第一个编码系统，使标签更容易生成，更快地相关，并提供标签之间更大的正交性。ARTag是挺有意义的一项工作，不过是不开源的。</p>
<hr>
<h1 id="Apriltags"><a href="#Apriltags" class="headerlink" title="Apriltags"></a>Apriltags</h1><p>参考了之前的工作，Apriltags由两个主要部分组成：标签检测器(detector) 和 编码系统(encode/decode)。 </p>
<h1 id="检测线段-Detecting-line-segments"><a href="#检测线段-Detecting-line-segments" class="headerlink" title="检测线段 Detecting line segments"></a>检测线段 Detecting line segments</h1><p>首先检测图像中的线条。与ARTag探测器的基本方法类似，计算每个像素的梯度方向和幅度，并将像素聚集成具有相似梯度方向和幅度的分量。</p>
<p><img src="\img\apriltags\pic3.png" alt="早期处理步骤">首先计算每个像素的梯度，计算它们的幅度（图1）和方向（图2）。使用基于图的方法，具有相似梯度方向和幅度的像素被聚类成组件（图3）。使用加权最小值然后，将线段拟合到每个组件中的像素（图四）。</p>
<p>线段的方向由梯度方向确定，因此，左边是暗段，右边是浅。 线的方向通过其中点处的短垂直“凹口”可视化; 注意这些“缺口”总是指向较轻的区域。<br>聚类算法类似于<span class="exturl" data-url="aHR0cDovL3Blb3BsZS5jcy51Y2hpY2Fnby5lZHUvfnBmZi9wYXBlcnMvc2VnLWlqY3YucGRm">Felzenszwalb的基于图的方法<i class="fa fa-external-link-alt"></i></span>：</p>
<ol>
<li>创建了一个图，其中每个节点代表一个像素。在相邻像素之间添加边缘，边缘权重等于像素在梯度方向上的差异。</li>
<li>然后根据增加的边缘权重对这些边缘进行分类和处理：<br>对于每个边缘，我们测试像素所属的连接分量是否应该连接在一起。给定分量n，我们将梯度方向的范围表示为$D(n)$，将幅度范围表示为$M(n)$。换句话说，$D(n)$和$M(n)$是标量值，分别表示梯度方向和幅度的最大值和最小值之间的差值。在$D()$的情况下，必须小心处理2π环绕。但是，由于有用边缘的跨度远小于π度，因此这很简单。给定$n$和$m$两个分量，如果满足以下两个条件，我们将它们连接在一起：<script type="math/tex; mode=display">D(n\cup m) \leq \min(D(n),D(m))+K_{D}/\left| n\cup m \right| \\
M(n\cup m) \leq \min(M(n),M(m))+K_{M}/\left| n\cup m \right|</script>$D()$和$M()$的小值表示具有很小的组件内变化的组件。如果它们的结合大致与单独采集的簇一样均匀，则两个簇连接在一起。通过$K_D$和$K_M$参数允许适度增加组件内变化，但随着组件变大，这会迅速缩小。在早期迭代期间，K参数基本上允许每个组件“学习”其簇内变化。在我们的实验中，我们使用$K_D = 100$和$K_M = 1200$，尽管该算法在很宽的范围内都能很好地工作。</li>
</ol>
<p>出于性能原因，边缘权重被量化并存储为定点数。这允许使用线性时间计数排序对边进行排序。 <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RtX3ZpbmNlbnQvYXJ0aWNsZS9kZXRhaWxzLzc2NTU3NjQ=">unionfind算法<i class="fa fa-external-link-alt"></i></span>可以有效地执行实际的合并操作，梯度方向和幅度的上限和下限存储在由每个组件的代表成员索引的简单数组中。</p>
<p>这种基于梯度的聚类方法对图像中的噪声敏感：<br>即使适量的噪声也会导致局部梯度方向变化，从而抑制组件的生长。解决这个问题的方法是对图像进行低通滤波。与此过滤可能会模糊图像中的有用信息的其他问题域不同，标记的边缘本质上是大规模的特征（特别是与数据字段相比），因此这种过滤不会导致信息丢失。我们建议值$σ= 0.8$。</p>
<p>聚类操作完成后，使用传统的最小二乘法将线段拟合到每个连接的组件，按梯度幅度对每个点进行加权。我们调整每个线段，使线的暗侧位于其左侧，而光侧位于其右侧。在下一个处理阶段，这允许我们围绕每个四边形强制执行绕线规则。</p>
<ul>
<li>分割算法是我们检测方案中最慢的阶段。</li>
</ul>
<p>作为一种选择，这种分割可以在图像分辨率的一半下执行，速度提高4倍。子采样操作可以与推荐的低通滤波器有效地组合。这种优化的结果是适度减小的检测范围，因为可能不再检测到非常小的四边形。</p>
<h1 id="四元检测-Quad-detection"><a href="#四元检测-Quad-detection" class="headerlink" title="四元检测 Quad detection"></a>四元检测 Quad detection</h1><p><img src="\img\apriltags\pic4.png" alt="四元检测和采样">在图像中检测到四个四边形（包含两个标签）。第三个检测到的四边形对应于前景标签的三个边缘加上纸张的边缘。 在较大标签的一个payload field位周围检测到第四个四边形。 这两个无关的检测最终被丢弃，因为它们的payload field无效。 白点对应于标签边界周围的样本，用于拟合“白色”像素强度的线性模型; 模型同样适合黑色像素。 这两个模型用于阈值数据有效负载位，如黄点所示。</p>
<p>此时，已经为图像计算了一组有向线段。下一个任务是找到形成4边形状的线段序列，即四边形。面临的挑战是尽可能地保持线段中的遮挡和噪声。我们的方法基于深度为4的递归深度优先搜索：<br>搜索树的每个级别为四边形添加边缘。在深度1处，我们考虑所有线段。在深度为2到4的情况下，我们认为所有的线段开始“足够接近”前一个线段结束的位置并且遵循逆时针缠绕顺序。通过调整“足够接近”阈值来处理遮挡和分割误差的稳健性：通过使阈值变大，可以处理边缘周围的显着间隙。<br>我们的“足够接近”的阈值是线的长度加上五个额外像素的两倍。这是一个较大的阈值，导致较低的假阴性率，但也导致较高的假阳性率。我们填充二维查找表以加速对在空间中的点附近开始的线段的查询。通过这种优化，以及不遵守绕组规则的候选四边形的早期拒绝，或者多次使用一个分段，四元检测算法代表总计算要求的一小部分。一旦找到四条线，就会创建一个候选四元检测。这个四边形的角是构成它的线的交叉点。因为使用来自许多像素的数据来拟合线，所以这些角估计精确到像素的一小部分。</p>
<h1 id="单应性以及外估计-Homography-and-extrinsics-estimation"><a href="#单应性以及外估计-Homography-and-extrinsics-estimation" class="headerlink" title="单应性以及外估计 Homography and extrinsics estimation"></a>单应性以及外估计 Homography and extrinsics estimation</h1><p>我们计算3×3单应矩阵，它从标签的坐标系（其中$[\begin{matrix}0&amp;0&amp;1\end{matrix}]^T$位于标签的中心并且标签在x和y方向上延伸一个单位）在同构坐标中投影2D点到2D图像坐标系。使用直接<span class="exturl" data-url="aHR0cHM6Ly93ZW5rdS5iYWlkdS5jb20vdmlldy8yODYzNWU1ZTgwNGQyYjE2MGI0ZWMwNDguaHRtbA==">线性变换（DLT）算法计算单应性<i class="fa fa-external-link-alt"></i></span>。<br>由于单应性投影指向齐次坐标，因此仅按比例定义。计算标签的位置和方向需要额外的信息：相机的焦距和标签的物理尺寸。<br>3×3单应矩阵（由DLT计算）可以写成3×4相机投影矩阵P（我们假设已知）和4×3截断的外在矩阵E的乘积。外部矩阵通常是4× 4，但标签上的每个位置在标签的坐标系中都是z = 0。因此，我们可以将每个标记坐标重写为2D齐次点，其中z隐式为零，并删除外部矩阵的第三列，形成截断的外部矩阵。我们将P的旋转分量表示为$R_{ij}$，将平移分量表示为$T_k$。我们还将未知比例因子表示为s：</p>
<script type="math/tex; mode=display">\left[ \begin{matrix}
h_{00} & h_{01} & h_{02} \\
h_{10} & h_{11} & h_{12} \\
h_{20} & h_{21} & h_{22} \\
 \end{matrix}\right] = sPE=s
\left[ \begin{matrix}
f_x & 0 & 0 & 0\\
0 & f_y & 0 & 0\\
0 & 0 & 1 & 0 \\
 \end{matrix}\right] = 
\left[ \begin{matrix}
R_{00} & R_{01} & T_{x} \\
R_{10} & R_{11} & T_{y} \\
R_{20} & R_{21} & T_{z} \\
0 & 0 & 1
 \end{matrix}\right]</script><p>这里不能直接求解E，因为P是秩不足的。 我们可以扩展Eqn的右侧，并将每个$h_{ij}$的表达式写为一组联立方程：</p>
<script type="math/tex; mode=display">h_{00}=sR_{00}f_x \\
h_{01}=sR_{01}f_x  \\
h_{02}=sR_xT_x \\
...</script><p>除了未知的比例因子s之外，这些都可以很容易地解决$R_{ij}$和{T_k}的元素。但是，由于旋转矩阵的列必须都是单位幅度，我们可以约束s的幅度。我们有两列旋转矩阵，因此我们将s计算为几何平均值的几何平均值。s的符号可以通过要求标签出现在摄像机前面来恢复，即$T_z&lt;0$。旋转矩阵的第三列可以通过计算两个已知列的叉积来恢复，因为旋转矩阵必须是正交的。上面的DLT过程和归一化过程不保证旋转矩阵严格地是正交的。为了纠正这个问题，我们计算了R的<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU2JTlFJTgxJUU1JTg4JTg2JUU4JUE3JUEz">极分解<i class="fa fa-external-link-alt"></i></span>，它产生了一个合适的旋转矩阵，同时最小化了误差的Frobenius矩阵范数(F范数：矩阵各项元素的绝对值平方的总和)。</p>
<h1 id="解码-playload-decoding"><a href="#解码-playload-decoding" class="headerlink" title="解码 playload decoding"></a>解码 playload decoding</h1><p>最后的任务是从payload field中读取位。我们通过计算每个位域的标签相对坐标，使用单应性将它们转换为图像坐标，然后对得到的像素进行阈值处理来完成此操作。为了对照明（不仅可以从标签到标签，也可以在标签内）变化，我们使用空间变化的阈值。<br>具体来说，我们建立了“黑色”像素强度的空间变化模型，以及“白色”模型强度的第二个模型。 我们使用标签的边框来包含白色和黑色像素的已知示例，以学习此模型。我们使用以下强度模型：</p>
<script type="math/tex; mode=display">I(x,y)=Ax+Bxy+Cy+D</script><p>该模型具有四个参数，可以使用<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3poLWhhbnMvJUU2JTlDJTgwJUU1JUIwJThGJUU0JUJBJThDJUU0JUI5JTk4JUU2JUIzJTk1">最小二乘回归轻松计算<i class="fa fa-external-link-alt"></i></span>。 我们建造了两个这样的模型，一个用于黑色，另一个用于白色。 解码数据位时使用的阈值就是黑白模型的预测强度值的平均值。</p>
<h1 id="关于编码-playload-encoding"><a href="#关于编码-playload-encoding" class="headerlink" title="关于编码 playload encoding"></a>关于编码 playload encoding</h1><p>Apriltags描述了一个新基于 lexicodes 的编码系统, 提供了显著优于以前的方法。程序可以生成具有多种属性的 lexicodes, 允许用户使用符合其需要的代码。</p>
<p>我们建议使用修改的词典。经典词典由两个量参数化：每个码字中的比特数n和任何两个码字之间的最小汉明距离d。 Lexicodes可以纠正$⌊（d - 1）/2⌋$位错误并检测$d/2$位错误。为方便起见，我们将表示36位编码，最小汉明距离为10（例如）为36h10代码。</p>
<p>Lexicodes从用于生成有效码字的启发式中得出它们的名称：候选码字以字典顺序（从最小到最大）被考虑，当它们距离先前添加到码本的每个码字至少距离d时，将新码字添加到码本。<br>在视觉基准的情况下，编码方案必须对旋转是鲁棒的。换句话说，当标签旋转90度，180度或270度时，至关重要的是它仍然与其他所有代码的汉明距离为d。标准词典生成算法不保证此属性。但是，标准生成算法可以通过简单的扩展来支持这一点：在测试新的候选码字时，我们可以简单地确保所有四个旋转都具有所需的最小汉明距离。事实上，lexicode算法可以很容易地扩展到包含额外的约束，这是我们的方法的一个优点。</p>
<p>尽管满足汉明距离约束，一些码字是糟糕的选择。例如，由全零组成的代码字将导致看起来像单个黑色方块的标签。这种简单的几何图案通常出现在自然场景中，导致误报。例如，ARTag编码系统明确禁止两个代码，因为它们很可能偶然发生。我们不是手动识别有问题的标签，而是通过拒绝导致简单几何图案的候选码字来进一步修改词典生成算法。我们的度量标准基于生成标签2D图案所需的矩形数量。例如，实心图案只需要一个矩形，而黑白黑色条纹则需要两个矩形（一个大的黑色矩形，第二个是较小的白色矩形）。我们的假设，在本文后面的实验结果的支持下，具有高复杂性的标签模式（需要重建许多矩形）在自然界中发生的频率较低，因此导致较低的假阳性率。使用这个想法，我们再次修改词典生成算法以拒绝过于简单的候选码字。我们使用简单的贪婪方法来近似生成标签模式所需的矩形数量，该方法重复考虑所有可能的矩形并添加最能减少误差的矩形。由于标签通常非常小，因此该计算不是瓶颈。最小复杂度小于阈值的标签（在我们的实验中通常为10）被拒绝。结果部分演示了此启发式的适当性和有效性。</p>
<p>最后，我们通过对词典生成算法进行一次修改，凭经验观察到较低的假阳性分数。我们考虑（b，b + 1p，b + 2p，b + 3p，…）而不是按顺序（0,1,2,3 ……）测试码字，其中b是任意数，p是一个大的素数，每一步都保留最低的n位。直观地说，这种方法生成的标签在每个比特位置都有更大的熵;另一方面，词典顺序有利于小值代码。这种方法的缺点是可以创建更少的可区分代码：词典排序倾向于非常密集地打包码字，而更随机的顺序导致码字的打包效率较低。总而言之，我们使用一个词典系统，可以生成任意标签大小（例如，3x3,4x4,5x5,6x6）和最小汉明距离的代码。我们的方法明确保证每个标签的所有四个旋转的最小汉明距离，并消除几何复杂度低的标签。计算标签可能是一项昂贵的操作，但是可以离线完成。小标签（5x5）可以在几秒或几分钟内轻松计算，但较大的标签（6x6）可能需要几天的CPU时间。许多有用的代码系列已经使用我们的软件进行计算和分发;大多数用户不需要生成自己的代码系列。</p>
<h1 id="纠错分析"><a href="#纠错分析" class="headerlink" title="纠错分析"></a>纠错分析</h1><p>可以容易地估计理论假阳性率。假设识别出错误的四边形并且位模式是随机的。误报的概率是被接受为有效标签的码字的分数与可能的码字的总数，2 n。更积极的纠错会增加此速率，因为它会增加接受的码字数量。下面的36h10和36h15代码说明了这种不可避免的错误率增加：</p>
<p><img src="\img\apriltags\pic5.png" alt="错误率">当然，36h15编码的更好性能是有代价的：只有27个可区分的代码字，而不是36h10的2221个可区分的代码字。<br>我们的编码方案明显强于以前的方案，包括ARTag使用的方案和ARToolkitPlus使用的两种系统：我们的编码系统在编码大量可区分的ID时实现了所有码字对之间更大的最小汉明距离。最小汉明距离的改进如上图和下表所示：</p>
<p><img src="\img\apriltags\pic6.png" alt="错误率">为了解码可能损坏的代码字，计算代码字和代码簿中的每个有效代码字之间的汉明距离。如果最佳匹配的汉明距离小于用户指定的阈值，则报告检测。通过指定该阈值，用户能够控制误报和误报之间的权衡。</p>
<p>这个方法的缺点是该解码过程花费了码本大小的线性时间，因为必须考虑每个有效的码字。<br>然而，该系数非常小，以至于与其他图像处理步骤相比，计算复杂度可以忽略不计。对于给定的编码方案，较大的标签（即，具有36位而不是25位的标签）具有比较小标签明显更好的编码性能，尽管这是有代价的。在所有其他条件相同的情况下，给定摄像机读取36位标签的范围将短于同一摄像机读取16或25位标签的范围。然而，由于边界的4像素开销，较小标签的范围的益处非常适中；通过使用16位标签而不是36位标签，可以预期检测范围仅提高25％。因此，仅在范围最敏感的应用中，较小的标签是有利的。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>机器人</tag>
        <tag>图像处理</tag>
        <tag>Opencv</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑回归</title>
    <url>/2020/04/12/34.logistic/</url>
    <content><![CDATA[<p>本次作业的目的是建立一个逻辑回归模型，用于预测一个学生是否应该被大学录取。</p>
<p>简单起见，大学通过<strong>两次考试的成绩来确定一个学生是否应该录取</strong>。你有以前数届考生的成绩，可以做为训练集学习逻辑回归模型。每个训练样本包括了考生两次考试的成绩和对应的录取决定。</p>
<p>你的任务是建立一个分类模型，根据两次考试的成绩来估计考生被录取的概率。<br>本次实验需要实现的函数</p>
<ul>
<li><code>plot_data</code> 绘制二维的分类数据。</li>
<li><code>sigmoid</code>函数</li>
<li><code>cost_function</code> 逻辑回归的代价函数</li>
<li><code>cost_gradient</code> 逻辑回归的代价函数的梯度，无正则化</li>
<li><code>predict</code> 逻辑回归的预测函数</li>
<li><code>cost_function_reg</code> 逻辑回归带正则化项的代价函数</li>
<li><code>cost_gradient_reg</code> 逻辑回归的代价函数的梯度，带正则化</li>
</ul>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入需要用到的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> op</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>在实现机器学习算法前，可视化的显示数据以观察其规律通常是有益的。本次作业中，你需要实现 <code>plot_data</code> 函数，用于绘制所给数据的散点图。你绘制的图像应如下图所示，两坐标轴分别为两次考试的成绩，正负样本分别使用不同的标记显示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_data</span>(<span class="params">X, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;This function plots the data points X and y into a new figure.</span></span><br><span class="line"><span class="string">    It plots the data points with red + for the positive examples,</span></span><br><span class="line"><span class="string">    and blue o the negative examples. X is assumed to be a Mx2 matrix.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    X: shape:nx2 </span></span><br><span class="line"><span class="string">    y: shape:nx1</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    m, _ = X.shape </span><br><span class="line">    flag_Admitted = <span class="number">1</span>        <span class="comment"># 图例的标志位</span></span><br><span class="line">    flag_Not_Admitted = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">if</span> y[i]==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> flag_Admitted:</span><br><span class="line">                plt.plot(X[i,<span class="number">0</span>], X[i,<span class="number">1</span>], <span class="string">&#x27;r+&#x27;</span>, label=<span class="string">&#x27;Admitted&#x27;</span>)</span><br><span class="line">                flag_Admitted = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                plt.plot(X[i,<span class="number">0</span>], X[i,<span class="number">1</span>], <span class="string">&#x27;r+&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> flag_Not_Admitted:</span><br><span class="line">                plt.plot(X[i,<span class="number">0</span>], X[i,<span class="number">1</span>], <span class="string">&#x27;b.&#x27;</span>, label=<span class="string">&#x27;Not Admitted&#x27;</span>)</span><br><span class="line">                flag_Not_Admitted = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                plt.plot(X[i,<span class="number">0</span>], X[i,<span class="number">1</span>], <span class="string">&#x27;b.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line"></span><br><span class="line">    plt.xlabel(<span class="string">&quot;Exam 1 Score&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Exam 2 Score&quot;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>调用 <code>plot_data</code>，可视化第一个文件<code>LR_data1</code>数据。绘制的图像如下：‘</p>
<p><img src="https://github.com/fredqi/fredqi.github.io/raw/master/teaching/PRML/LR_data1_visual.png" alt="data1"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载数据 注意使用 !ls 或 !find 命令确定数据文件所在的目录 dataXXXX 。</span></span><br><span class="line">data = np.loadtxt(<span class="string">&quot;LR_data1.txt&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line">X, y = data[:, :<span class="number">2</span>], data[:, <span class="number">2</span>]  <span class="comment"># X是二维的数据，y是标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line"><span class="comment"># ====================== YOUR CODE HERE ================</span></span><br><span class="line">plot_data(X, y)</span><br><span class="line"><span class="comment"># ======================================================</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img/PRML/output_6_0.png" alt="png"></p>
<p>绘制分类面</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制分类面。&quot;&quot;&quot;</span></span><br><span class="line">    plot_data(X[:, <span class="number">1</span>:], y)</span><br><span class="line"></span><br><span class="line">    _, d = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> d &lt;= <span class="number">3</span>:</span><br><span class="line">        plot_x = np.array([np.<span class="built_in">min</span>(X[:, <span class="number">1</span>])-<span class="number">2</span>, np.<span class="built_in">max</span>(X[:, <span class="number">1</span>])+<span class="number">2</span>])</span><br><span class="line">        plot_y = -<span class="number">1.0</span> / theta[<span class="number">2</span>]*(theta[<span class="number">1</span>]*plot_x + theta[<span class="number">0</span>])</span><br><span class="line">        plt.plot(plot_x, plot_y, <span class="string">&#x27;m-&#x27;</span>, label=<span class="string">&quot;Decision Boundary&quot;</span>)</span><br><span class="line"></span><br><span class="line">        plt.xlim([<span class="number">30</span>, <span class="number">100</span>])</span><br><span class="line">        plt.ylim([<span class="number">30</span>, <span class="number">100</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        n_grid = <span class="number">50</span></span><br><span class="line">        u = np.linspace(-<span class="number">1</span>, <span class="number">1.5</span>, n_grid)</span><br><span class="line">        v = np.linspace(-<span class="number">1</span>, <span class="number">1.5</span>, n_grid)</span><br><span class="line"></span><br><span class="line">        z = np.zeros((n_grid, n_grid))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_grid):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n_grid):</span><br><span class="line">                uu, vv = np.array([u[i]]), np.array([v[j]])</span><br><span class="line">                z[i, j] = np.dot(map_feature(uu, vv), theta)</span><br><span class="line"></span><br><span class="line">        z = z.T</span><br><span class="line"></span><br><span class="line">        CS = plt.contour(u, v, z, linewidths=<span class="number">2</span>, levels=[<span class="number">0.0</span>], colors=[<span class="string">&#x27;m&#x27;</span>])</span><br><span class="line">        CS.collections[<span class="number">0</span>].set_label(<span class="string">&#x27;Decision boundary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<h2 id="热身练习：Sigmoid函数"><a href="#热身练习：Sigmoid函数" class="headerlink" title="热身练习：Sigmoid函数"></a>热身练习：Sigmoid函数</h2><p>逻辑回归的假设模型为：</p>
<script type="math/tex; mode=display">h_{\theta}(x) = g(\theta^{\mathrm{T}} x)</script><p>其中函数 $g(\cdot)$ 是Sigmoid函数，定义为：</p>
<script type="math/tex; mode=display">g(z) = \frac{1}{1+\exp(-z)}</script><p>本练习中第一步需要你实现 Sigmoid 函数。在实现该函数后，你需要确认其功能正确。对于输入为矩阵和向量的情况，你实现的函数应当对每一个元素执行Sigmoid 函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute sigmoid function&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    z = np.asarray(z)</span><br><span class="line">    g = np.zeros_like(z)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    g = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> g</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试 sigmoid 函数</span></span><br><span class="line">z = np.array([-<span class="number">10.0</span>, -<span class="number">5.0</span>, <span class="number">0.0</span>, <span class="number">5.0</span>, <span class="number">10.0</span>])</span><br><span class="line">g = sigmoid(z)</span><br><span class="line">print(<span class="string">&quot;Value of sigmoid at [-10, -5, 0, 5, 10] are:\n&quot;</span>, g)</span><br></pre></td></tr></table></figure>
<pre><code>Value of sigmoid at [-10, -5, 0, 5, 10] are:
 [4.53978687e-05 6.69285092e-03 5.00000000e-01 9.93307149e-01
 9.99954602e-01]
</code></pre><h2 id="代价函数与梯度"><a href="#代价函数与梯度" class="headerlink" title="代价函数与梯度"></a>代价函数与梯度</h2><p>现在你需要实现逻辑回归的代价函数及其梯度。补充完整<code>cost_function</code>函数，使其返回正确的代价。补充完整<code>cost_gradient</code>函数，使其返回正确的梯度。</p>
<p>逻辑回归的代价函数为:</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \Big[ -y^{(i)} \log \big( h_{\theta}(x^{(i)}) \big) - (1-y^{(i)}) \log \big( 1-h_{\theta}(x^{(i)}) \big) \Big]</script><p>对应的梯度向量各分量为</p>
<script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta_{j}} = \frac{1}{m} \sum_{i=1}^{m} \big( h_{\theta}(x^{(i)}) - y^{(i)} \big) x_{j}^{(i)}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逻辑回归的代价函数，无正则项。&quot;&quot;&quot;</span></span><br><span class="line">    J = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    m, _ = X.shape</span><br><span class="line">    hypothesis = sigmoid(np.dot(theta, X.T))</span><br><span class="line">    J = <span class="number">1</span>/m * (-y * np.log(hypothesis) - (<span class="number">1</span>-y) * np.log(<span class="number">1</span>-hypothesis)).<span class="built_in">sum</span>() <span class="comment"># 由于log(0)的偶尔存在，这里会爆warning，但似乎并不影响结果</span></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_gradient</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逻辑回归的代价函数的梯度，无正则项。&quot;&quot;&quot;</span></span><br><span class="line">    m = <span class="number">1.0</span>*<span class="built_in">len</span>(y)</span><br><span class="line">    grad = np.zeros_like(theta)</span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    <span class="comment"># hypothesis = np.zeros_like(y)</span></span><br><span class="line">    hypothesis = sigmoid(np.dot(theta, X.T))</span><br><span class="line">    loss = hypothesis-y</span><br><span class="line">    grad = <span class="number">1</span>/m * np.dot(loss, X) <span class="comment"># 对公式进行向量形式的推导，得出这样的写法</span></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<h2 id="预测函数"><a href="#预测函数" class="headerlink" title="预测函数"></a>预测函数</h2><p>在获得模型参数后，你就可以使用模型预测一个学生能够被大学录取。如果某学生考试一的 成绩为45，考试二的成绩为85，你应该能够得到其录取概率约为0.776。</p>
<p>你需要完成 <code>predict</code> 函数，该函数输出“1”或“0”。通过计算分类正确的样本百分数， 我们可以得到训练集上的正确率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">theta, X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Predict whether the label is 0 or 1</span></span><br><span class="line"><span class="string">    using learned logistic regression parameters theta.</span></span><br><span class="line"><span class="string">    input： theta：model&#x27;s parameters</span></span><br><span class="line"><span class="string">            X: input samples</span></span><br><span class="line"><span class="string">    output：0 or 1</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m, _ = X.shape</span><br><span class="line">    pred = np.zeros((m, <span class="number">1</span>), dtype=np.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    g = sigmoid(np.dot(theta, X.T))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">if</span> g[i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            pred[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pred[i] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<h2 id="使用scipy-optimize-fmin-cg学习模型参数"><a href="#使用scipy-optimize-fmin-cg学习模型参数" class="headerlink" title="使用scipy.optimize.fmin_cg学习模型参数"></a>使用<code>scipy.optimize.fmin_cg</code>学习模型参数</h2><p>在本次作业中，希望你使用 <code>scipy.optimize.fmin_cg</code> 函数实现代价函数 $J(\theta)$ 的优化，得到最佳参数 $\theta^{*}$ 。</p>
<p>使用该优化函数的代码已经在程序中实现，调用方式示例如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ret &#x3D; op.fmin_cg(cost_function,</span><br><span class="line">                 theta,</span><br><span class="line">                 fprime&#x3D;cost_gradient,</span><br><span class="line">                 args&#x3D;(X, y),</span><br><span class="line">                 maxiter&#x3D;400,</span><br><span class="line">                 full_output&#x3D;True)</span><br><span class="line">theta_opt, cost_min, _, _, _ &#x3D; ret</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>其中<code>cost_function</code>为代价函数， <code>theta</code> 为需要优化的参数初始值， <code>fprime=cost_gradient</code> 给出了代价函数的梯度， <code>args=(X, y)</code> 给出了需要优化的函数与对应的梯度计算所需要的其他参数， <code>maxiter=400</code> 给出了最大迭代次数， <code>full_output=True</code> 则指明该函数除了输出优化得到的参数 <code>theta_opt</code> 外，还会返回最小的代价函数值 <code>cost_min</code> 等内容。</p>
<p>对第一组参数，得到的代价约为 0.203 (cost_min)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;针对第一组数据建立逻辑回归模型。&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    data = np.loadtxt(<span class="string">&quot;LR_data1.txt&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    X, y = data[:, :<span class="number">2</span>], data[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算代价与梯度</span></span><br><span class="line">    m, _ = X.shape</span><br><span class="line">    X = np.hstack((np.ones((m, <span class="number">1</span>)), X))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    theta_initial = np.ones_like(X[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 计算并打印初始参数对应的代价与梯度</span></span><br><span class="line">    cost = cost_function(theta_initial, X, y)</span><br><span class="line">    grad = cost_gradient(theta_initial, X, y)</span><br><span class="line">    print(<span class="string">&quot;Cost at initial theta (zeros): &quot;</span>, cost)</span><br><span class="line">    print(<span class="string">&quot;Gradient at initial theta (zeros): &quot;</span>, grad)</span><br><span class="line">    <span class="comment"># 使用 scipy.optimize.fmin_cg 优化模型参数</span></span><br><span class="line">    args = (X, y)</span><br><span class="line">    maxiter = <span class="number">200</span></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    ret = op.fmin_cg(cost_function,</span><br><span class="line">                     theta_initial,</span><br><span class="line">                     cost_gradient,</span><br><span class="line">                     args)</span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line">    theta_opt = ret   <span class="comment"># 看过fmin_cg的代码之后，我决定这样修改老师的代码，可能是库版本的问题</span></span><br><span class="line">    print(<span class="string">&quot;theta_op: \n&quot;</span>, theta_opt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制分类面ret </span></span><br><span class="line">    plot_decision_boundary(theta_opt, X, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测考试一得45分，考试二得85分的学生的录取概率</span></span><br><span class="line">    x_test = np.array([<span class="number">1</span>, <span class="number">45</span>, <span class="number">85.0</span>])</span><br><span class="line">    prob = sigmoid(np.dot(theta_opt, x_test))</span><br><span class="line">    print(<span class="string">&#x27;For a student with scores 45 and 85, we predict an admission probability of: &#x27;</span>, prob)</span><br><span class="line"></span><br><span class="line">logistic_regression()</span><br></pre></td></tr></table></figure>
<pre><code>Cost at initial theta (zeros):  nan
Gradient at initial theta (zeros):  [ 0.4        20.81292044 21.84815684]
Optimization terminated successfully.
         Current function value: 0.203498
         Iterations: 37
         Function evaluations: 111
         Gradient evaluations: 111
theta_op: 
 [-25.15777637   0.20620326   0.20144282]
For a student with scores 45 and 85, we predict an admission probability of:  0.7762612158589474


/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log
  import sys
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in multiply
  import sys
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp

/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp

/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log
  import sys
/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in multiply
  import sys
</code></pre><p><img src="/img/PRML/output_18_2.png" alt="png"></p>
<h2 id="正则化的逻辑回归"><a href="#正则化的逻辑回归" class="headerlink" title="正则化的逻辑回归"></a>正则化的逻辑回归</h2><h2 id="数据可视化-1"><a href="#数据可视化-1" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>调用函数<code>plot_data</code>可视化第二组数据 <code>LR_data2.txt</code> 。<br>正确的输出如下：</p>
<p><img src="https://github.com/fredqi/fredqi.github.io/raw/master/teaching/PRML/LR_data2_visual.png" alt="LR_data2"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = np.loadtxt(<span class="string">&quot;LR_data2.txt&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line">X, y = data[:, :<span class="number">2</span>], data[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化数据</span></span><br><span class="line"><span class="comment"># ====================== YOUR CODE HERE ================</span></span><br><span class="line">plot_data(X, y)</span><br><span class="line"><span class="comment"># ======================================================</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img/PRML/output_20_0.png" alt="png"></p>
<h2 id="特征变换"><a href="#特征变换" class="headerlink" title="特征变换"></a>特征变换</h2><p>创建更多的特征是充分挖掘数据中的信息的一种有效手段。在函数 map_feature 中，我们将数据映射为其六阶多项式的所有项。    </p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/c7b66b3052264cd5b49455655db18e309891b046dbcc4126aac565ec5698c597" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map_feature</span>(<span class="params">X1, X2, degree=<span class="number">6</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Feature mapping function to polynomial features.&quot;&quot;&quot;</span></span><br><span class="line">    m = <span class="built_in">len</span>(X1)</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(X1) == <span class="built_in">len</span>(X2)</span><br><span class="line">    n = <span class="built_in">int</span>((degree+<span class="number">2</span>)*(degree+<span class="number">1</span>)/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    out = np.zeros((m, n))</span><br><span class="line"></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(degree+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># print i-j, j, idx</span></span><br><span class="line">            out[:, idx] = np.power(X1, i-j)*np.power(X2, j)</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h2 id="代价函数与梯度-1"><a href="#代价函数与梯度-1" class="headerlink" title="代价函数与梯度"></a>代价函数与梯度</h2><p>逻辑回归的代价函数为</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left [ -y^{(i)} \log \left (h_{\theta}(x^{(i)}) \right) - (1-y^{(i)}) \log \left (
1- h_{\theta}(x^{(i)}) \right ) \right ] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_{j}^{2}</script><p>对应的梯度向量各分量为：</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/c6c0b9e0c80e4596b29f9df0e631998ed62dddda53974daaa3e323f22c42fe00" alt="gradient"></p>
<p>完成以下函数：</p>
<ul>
<li><code>cost_function_reg()</code></li>
<li><code>cost_gradient_reg()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_function_reg</span>(<span class="params">theta, X, y, lmb</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逻辑回归的代价函数，有正则项。&quot;&quot;&quot;</span></span><br><span class="line">    m = <span class="number">1.0</span>*<span class="built_in">len</span>(y)</span><br><span class="line">    J = <span class="number">0</span></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    hypothesis = sigmoid(np.dot(theta, X.T))</span><br><span class="line">    J = <span class="number">1</span>/m * (-y * np.log(hypothesis) - (<span class="number">1</span>-y) * np.log(<span class="number">1</span>-hypothesis)).<span class="built_in">sum</span>() <span class="comment"># 由于log(0)的偶尔存在，这里会爆warning，但似乎并不影响结果</span></span><br><span class="line">    J = J + lmb/(<span class="number">2</span>*m) * np.dot(theta, theta) <span class="comment"># 加入正则项</span></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_gradient_reg</span>(<span class="params">theta, X, y, lmb</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;逻辑回归的代价函数的梯度，有正则项。&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    m = <span class="number">1.0</span>*<span class="built_in">len</span>(y)</span><br><span class="line">    grad = np.zeros_like(theta)</span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    hypothesis = sigmoid(np.dot(theta, X.T))</span><br><span class="line">    loss = hypothesis-y</span><br><span class="line">    grad[<span class="number">0</span>] = <span class="number">1</span>/m * (loss * X[:,<span class="number">0</span>]).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">28</span>):</span><br><span class="line">        grad[i] = <span class="number">1</span>/m * (loss * X[:,i]).<span class="built_in">sum</span>() + lmb/m * theta[i]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>如果将参数$\theta$ 初始化为全零值，相应的代价函数约为 0.693。可以使用与前述无正则化项类似的方法实现梯度下降，<br>获得优化后的参数 $\theta^{*}$ 。<br>你可以调用 plot_decision_boundary 函数来查看最终得到的分类面。建议你调整正则化项的系数，分析正则化对分类面的影响!</p>
<p>参考输出图像：</p>
<p><img src="https://github.com/fredqi/fredqi.github.io/raw/master/teaching/PRML/LR_data2_boundary.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression_reg</span>(<span class="params">lmb=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;针对第二组数据建立逻辑回归模型。&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    data = np.loadtxt(<span class="string">&quot;LR_data2.txt&quot;</span>, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    X, y = data[:, :<span class="number">2</span>], data[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算具有正则项的代价与梯度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注意map_feature会自动加入一列 1</span></span><br><span class="line">    X = map_feature(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">    print(X.shape)</span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    theta_initial = np.zeros_like(X[<span class="number">0</span>, :])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算并打印初始参数对应的代价与梯度</span></span><br><span class="line">    cost = cost_function_reg(theta_initial, X, y, lmb=lmb)</span><br><span class="line">    grad = cost_gradient_reg(theta_initial, X, y, lmb=lmb)</span><br><span class="line">    print(<span class="string">&quot;Cost at initial theta (zeros): &quot;</span>, cost)</span><br><span class="line">    print(<span class="string">&quot;Gradient at initial theta (zeros): \n&quot;</span>, grad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 scipy.optimize.fmin_cg 优化模型参数</span></span><br><span class="line">    args = (X, y, lmb)</span><br><span class="line">    maxiter = <span class="number">200</span></span><br><span class="line">    <span class="comment"># ====================== YOUR CODE HERE ======================</span></span><br><span class="line">    ret = op.fmin_cg(cost_function_reg,</span><br><span class="line">                     theta_initial,</span><br><span class="line">                     cost_gradient_reg,</span><br><span class="line">                     args)</span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line">    theta_opt = ret</span><br><span class="line">    <span class="comment">#print(&quot;Cost at theta found by fmin_cg: &quot;, cost_min)</span></span><br><span class="line">    print(<span class="string">&quot;theta_op: \n&quot;</span>, theta_opt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制分类面</span></span><br><span class="line">    plot_decision_boundary(theta_opt, X, y)</span><br><span class="line">    plt.title(<span class="string">&quot;lambda = &quot;</span> + <span class="built_in">str</span>(lmb))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算在训练集上的分类正确率</span></span><br><span class="line">    pred = predict(theta_opt, X)</span><br><span class="line">    print(<span class="string">&quot;Train Accuracy: &quot;</span>, np.mean(pred == y)*<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 可选：尝试不同正则化系数lmb = 0.0, 1.0, 10.0, 100.0对分类面的影响</span></span><br><span class="line">logistic_regression_reg() </span><br></pre></td></tr></table></figure>
<pre><code>(118, 28)
Cost at initial theta (zeros):  0.6931471805599454
Gradient at initial theta (zeros): 
 [8.47457627e-03 1.87880932e-02 7.77711864e-05 5.03446395e-02
 1.15013308e-02 3.76648474e-02 1.83559872e-02 7.32393391e-03
 8.19244468e-03 2.34764889e-02 3.93486234e-02 2.23923907e-03
 1.28600503e-02 3.09593720e-03 3.93028171e-02 1.99707467e-02
 4.32983232e-03 3.38643902e-03 5.83822078e-03 4.47629067e-03
 3.10079849e-02 3.10312442e-02 1.09740238e-03 6.31570797e-03
 4.08503006e-04 7.26504316e-03 1.37646175e-03 3.87936363e-02]
Warning: Desired error not necessarily achieved due to precision loss.
         Current function value: 0.535776
         Iterations: 8
         Function evaluations: 75
         Gradient evaluations: 65
theta_op: 
 [ 1.22008591  0.6174169   1.18134764 -1.96102914 -0.84518957 -1.23644428
  0.09323171 -0.35130743 -0.35278837 -0.19758548 -1.46793288 -0.09276645
 -0.57816353 -0.25825218 -1.18239765 -0.27467132 -0.21615072 -0.06863601
 -0.25974267 -0.28202028 -0.56424266 -1.07964814 -0.0090027  -0.28241083
 -0.00667624 -0.31162077 -0.13714068 -1.03061176]
</code></pre><p><img src="/img/PRML/output_26_1.png" alt="png"></p>
<pre><code>Train Accuracy:  49.87072680264292
</code></pre>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>使用神经网络进行机器学习</title>
    <url>/2020/04/13/35.network/</url>
    <content><![CDATA[<h2 id="数据介绍"><a href="#数据介绍" class="headerlink" title="数据介绍"></a>数据介绍</h2><p>本次练习所用的数据集有5000个训练样本，每个样本对应于20x20大小的灰度图像。这些训练样本包括了9-0共十个数字的手写图像。这些样本中每个像素都用浮点数表示。加载得到的数据中，每幅图像都被展开为一个400维的向量，构成了数据矩阵中的一行。完整的训练数据是一个5000x400的矩阵，其每一行为一个训练样本（数字的手写图像）。数据中，对应于数字”0”的图像被标记为”10”，而数字”1”到”9”按照其自然顺序被分别标记为”1”到”9”。数据集保存在<code>NN_data.mat</code>.<br><img src="/img/PRML/data-array.png" alt="image"></p>
<a id="more"></a>
<h2 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h2><p>我们准备训练的神经网络是一个三层的结构，一个输入层，一个隐层以及一个输出层。由于我们训练样本（图像）是20x20的，所以输入层单元数为400（不考虑额外的偏置项，如果考虑单元个数需要+1）。在我们的程序中，数据会被加载到变量<span> $X$ </span> 和<span> $y$ </span>里。</p>
<p>本项练习提供了一组训练好的网络参数<span> $(\Theta^{(1)}, \Theta^{(2)})$ </span>。这些数据存储在数据文件 <code>NN_weights.mat</code>，在程序中被加载到变量 <code>Theta1</code> 与 <code>Theta2</code> 中。参数的维度对应于第二层有25个单元、10个输出单元（对应于10个数字 的类别）的网络。</p>
<p><img src="/img/PRML/nn-representation.png" alt="image"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> fmin_cg</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_data</span>(<span class="params">data, img_width=<span class="number">20</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将图像数据 data 按照矩阵形式显示出来&quot;&quot;&quot;</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="comment"># 计算数据尺寸相关数据</span></span><br><span class="line">    n_rows, n_cols = data.shape</span><br><span class="line">    img_height = n_cols // img_width</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算显示行数与列数</span></span><br><span class="line">    disp_rows = <span class="built_in">int</span>(np.sqrt(n_rows))</span><br><span class="line">    disp_cols = (n_rows + disp_rows - <span class="number">1</span>) // disp_rows</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 图像行与列之间的间隔</span></span><br><span class="line">    pad = <span class="number">1</span></span><br><span class="line">    disp_array = np.ones((pad + disp_rows*(img_height + pad),</span><br><span class="line">                          pad + disp_cols*(img_width + pad)))</span><br><span class="line"></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(disp_rows):</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(disp_cols):</span><br><span class="line">            <span class="keyword">if</span> idx &gt; m:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 复制图像块</span></span><br><span class="line">            rb = pad + row*(img_height + pad)</span><br><span class="line">            cb = pad + col*(img_width + pad)</span><br><span class="line">            disp_array[rb:rb+img_height, cb:cb+img_width] = data[idx].reshape((img_height, -<span class="number">1</span>), order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">            <span class="comment"># 获得图像块的最大值，对每个训练样本分别归一化</span></span><br><span class="line">            max_val = np.<span class="built_in">abs</span>(data[idx].<span class="built_in">max</span>())</span><br><span class="line">            disp_array[rb:rb+img_height, cb:cb+img_width] /= max_val</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.imshow(disp_array)</span><br><span class="line"></span><br><span class="line">    plt.gray()</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;data-array.png&#x27;</span>, dpi=<span class="number">150</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="前向传播与代价函数"><a href="#前向传播与代价函数" class="headerlink" title="前向传播与代价函数"></a>前向传播与代价函数</h2><p>现在你需要实现神经网络的代价函数及其梯度。首先需要使得函数 <code>nn_cost_function</code> 能够返回正确的代价值。</p>
<p>神经网络的代价函数（不包括正则化项）的定义为：</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left[-y_k^{(i)} \log\left((h_{\theta}(x^{(i)}))_k\right) -(1 - y_k^{(i)}) \log\left(1 - (h_{\theta}(x^{(i)}))_k\right) \right]</script><p>其中<span> $h_{\theta}(x^{(i)})$ </span> 的计算如神经网络结构图所示，<span> $K=10$ </span>是 所有可能的类别数。这里的<span> $y$ </span>使用了one-hot 的表达方式。</p>
<p>运行程序，使用预先训练好的网络参数，确认你得到的代价函数是正确的。（正确的代价约为0.287629）。</p>
<h2 id="代价函数的正则化"><a href="#代价函数的正则化" class="headerlink" title="代价函数的正则化"></a>代价函数的正则化</h2><p>神经网络包括正则化项的代价函数为: &lt;/br&gt;</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{m}\sum_{i=1}^{m} \sum_{k=1}^{K} \left[-y_k^{(i)} \log\left((h_{\theta}(x^{(i)}))_k\right) -(1 - y_k^{(i)}) \log\left(1 - (h_{\theta}(x^{(i)}))_k\right) \right] + \frac{\lambda}{2m} \left[\sum_{j=1}^{25} \sum_{k=1}^{400} (\Theta_{j,k}^{(1)})^2 +\sum_{j=1}^{10} \sum_{k=1}^{25} (\Theta_{j,k}^{(2)})^2 \right]</script><p>注意在上面式子中，正则化项的加和形式与练习中设定的网络结构一致。但是你的代码实现要保证能够用于任意大小的神经网络。<br>此外，还需要注意，对应于偏置项的参数不能包括在正则化项中。对于矩阵 <code>Theta1</code> 与 <code>Theta2</code> 而言，这些项对应于矩阵的第一列。</p>
<p>运行程序，使用预先训练好的权重数据，设置正则化系数$\lambda=1$ (<code>lmb</code>) 确认你得到的代价函数是正确的。（正确的代价约为0.383770）。</p>
<p>此步练习需要你补充实现 <code>nn_cost_function</code> 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_to_one_hot</span>(<span class="params">y,c</span>):</span></span><br><span class="line">    v = np.eye(c+<span class="number">1</span>)[y.reshape(-<span class="number">1</span>)].T  <span class="comment"># 11是因为有个非常讨厌的编码10出现</span></span><br><span class="line">    v = np.delete(v, <span class="number">0</span>, axis = <span class="number">0</span>)    <span class="comment"># 编码后是11位制的，第一行都是0删掉</span></span><br><span class="line">    <span class="keyword">return</span> v.T</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_cost_function</span>(<span class="params">nn_params, *args</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;神经网络的损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Unpack parameters from *args</span></span><br><span class="line">    input_layer_size, hidden_layer_size, num_labels, lmb, X, y = args</span><br><span class="line">    <span class="comment"># Unroll weights of neural networks from nn_params</span></span><br><span class="line">    Theta1 = nn_params[:hidden_layer_size*(input_layer_size + <span class="number">1</span>)]</span><br><span class="line">    Theta1 = Theta1.reshape((hidden_layer_size, input_layer_size + <span class="number">1</span>))</span><br><span class="line">    Theta2 = nn_params[hidden_layer_size*(input_layer_size + <span class="number">1</span>):]</span><br><span class="line">    Theta2 = Theta2.reshape((num_labels, hidden_layer_size + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 设置变量</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># You need to return the following variable correctly</span></span><br><span class="line">    J = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># ====================== 你的代码 ======================</span></span><br><span class="line">    a_1 = np.hstack((np.ones((m, <span class="number">1</span>)), X))   <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_2 = np.dot(Theta1, a_1.T) </span><br><span class="line">    a_2 = sigmoid(Z_2)</span><br><span class="line">    a_2 = a_2.T</span><br><span class="line">    a_2 = np.hstack((np.ones((m, <span class="number">1</span>)), a_2)) <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_3 = np.dot(Theta2, a_2.T)</span><br><span class="line">    hypothesis = sigmoid(Z_3)</span><br><span class="line">   </span><br><span class="line">    one_hot_y = convert_to_one_hot(y, num_labels)  <span class="comment"># 这里需要对y进行one-hot编码</span></span><br><span class="line"></span><br><span class="line">    Theta1_without_bias = np.delete(Theta1, <span class="number">0</span>, axis = <span class="number">1</span>) <span class="comment"># 删除 input add bias as 1 在第一列</span></span><br><span class="line">    Theta2_without_bias = np.delete(Theta2, <span class="number">0</span>, axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    Regular = lmb / (<span class="number">2</span>*m) * (np.square(Theta1_without_bias).<span class="built_in">sum</span>() </span><br><span class="line">                           + np.square(Theta2_without_bias).<span class="built_in">sum</span>() ) <span class="comment"># 正则项, 偏置项不要正则</span></span><br><span class="line">    loss = <span class="number">0.0</span>    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m): <span class="comment"># 样本数量</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_labels): <span class="comment"># 标签类别</span></span><br><span class="line">            loss += (-one_hot_y[i,j] * np.log(hypothesis[j,i]) - (<span class="number">1</span> - one_hot_y[i,j]) * np.log(<span class="number">1</span> - hypothesis[j,i]))</span><br><span class="line"></span><br><span class="line">    J = <span class="number">1</span>/m * loss + Regular</span><br><span class="line">    <span class="comment">#print(&#x27;current cost: &#x27;, J)</span></span><br><span class="line">    <span class="comment"># ======================================================</span></span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="误差反传训练算法-Backpropagation"><a href="#误差反传训练算法-Backpropagation" class="headerlink" title="误差反传训练算法 (Backpropagation)"></a>误差反传训练算法 (Backpropagation)</h2><p><img src="/img/PRML/nn-backpropagation.png" alt="image"></p>
<p>现在你需要实现误差反传训练算法。误差反传算法的思想大致可以描述如下。对于一个训练样本<span> $(x^{(t)}, y^{(t)})$ </span>，我们首先使用前向传播计算网络中所有单元（神经元）的激活值（activation），包括假设输出<span> $h_{\Theta}(x)$ </span>。那么，对于第<span> $l$ </span>层的第<span> $j$ </span>个节点，我们期望计算出一个“误差项”<span> $\delta_{j}^{(l)}$ </span>用于衡量该节点对于输出的误差的“贡献”。</p>
<p>对于输出节点，我们可以直接计算网络的激活值与真实目标值之间的误差。对于我们所训练的第3层为输出层的网络，这个误差定义了<span> $\delta_{j}^{(3)}$ </span>。对于隐层单元，需要根据第<span> $l+1$ </span>层的节点的误差的加权平均来计算<span> $\delta_{j}^{(l)}$ </span>。</p>
<p>下面是误差反传训练算法的细节（如图3所示）。你需要在一个循环中实现步骤1至4。循环的每一步处理一个训练样本。第5步将累积的梯度除以<span> $m$ </span>以得到神经网络代价函数的梯度。</p>
<ol>
<li>设输入层的值<span> $a^{(1)}$ </span>为第<span> $t$ </span>个训练样本<span> $x^{(t)}$ </span>。执行前向传播，计算第2层与第3层各节点的激活值(<span> $z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)}$ </span>)。注意你需要在<span> $a^{(1)}$ </span>与<span> $a^{(2)}$ </span>增加一个全部为 +1 的向量，以确保包括了偏置项。在 <code>numpy</code> 中可以使用函数 <code>ones</code> ， <code>hstack</code>, <code>vstack</code> 等完成（向量化版本）。</li>
<li><p>对第3层中的每个输出单元<span> $k$ </span>，计算</p>
<script type="math/tex; mode=display">\delta_{k}^{(3)} = a_{k}^{(3)} - y_k</script><p>其中<span> $y_k \in {0, 1}$ </span>表示当前训练样本是否是第<span> $k$ </span>类。</p>
</li>
<li><p>对隐层<span> $l=2$ </span>, 计算</p>
<script type="math/tex; mode=display">\delta^{(2)} = \left( \Theta^{(2)} \right)^T \delta^{(3)} .* g^{\prime} (z^{(2)})</script><p>其中$g^{\prime}$ 表示 Sigmoid 函数的梯度， <code>.*</code> 在 <code>numpy</code> 中是通 常的逐个元素相乘的乘法，矩阵乘法应当使用 <code>numpy.dot</code> 函数。</p>
</li>
<li><p>使用下式将当前样本梯度进行累加：</p>
<script type="math/tex; mode=display">\Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T</script><p>在 <code>numpy</code> 中，数组可以使用 <code>+=</code> 运算。</p>
</li>
<li><p>计算神经网络代价函数的（未正则化的）梯度，</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)}</script></li>
</ol>
<p>这里，你需要（部分）完成函数 <code>nn_grad_function</code> 。程序将使用函数 <code>check_nn_gradients</code> 来检查你的实现是否正确。在使用循环的方式完成函数 <code>nn_grad_function</code> 后，建议尝试使用向量化的方式重新实现这个函数。</p>
<h2 id="神经网络的正则化"><a href="#神经网络的正则化" class="headerlink" title="神经网络的正则化"></a>神经网络的正则化</h2><p>你正确实现了误差反传训练算法之后，应当在梯度中加入正则化项。</p>
<p>假设你在误差反传算法中计算了<span> $\Delta_{ij}^{(l)}$ </span>，你需要增加的正则化项为</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)} \qquad \text{for } j = 0
\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)} + \frac{\lambda}{m} \Theta_{ij}^{(l)} \qquad \text{for } j \geq 1</script><p>注意你不应该正则化<span> $\Theta^{(l)}$ </span>的第一列，因其对应于偏置项。</p>
<p>此步练习需要你补充实现函数 <code>nn_grad_function</code> 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_grad_function</span>(<span class="params">nn_params, *args</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;神经网络的损失函数梯度计算 &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获得参数信息</span></span><br><span class="line">    input_layer_size, hidden_layer_size, num_labels, lmb, X, y = args</span><br><span class="line">    <span class="comment"># 得到各个参数的权重值</span></span><br><span class="line">    Theta1 = nn_params[:hidden_layer_size*(input_layer_size + <span class="number">1</span>)]</span><br><span class="line">    Theta1 = Theta1.reshape((hidden_layer_size, input_layer_size + <span class="number">1</span>))</span><br><span class="line">    Theta2 = nn_params[hidden_layer_size*(input_layer_size + <span class="number">1</span>):]</span><br><span class="line">    Theta2 = Theta2.reshape((num_labels, hidden_layer_size + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置变量</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== 你的代码 =====================</span></span><br><span class="line">    a_1 = np.hstack((np.ones((m, <span class="number">1</span>)), X)) <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_2 = np.dot(Theta1, a_1.T)</span><br><span class="line">    a_2 = sigmoid(Z_2)</span><br><span class="line">    a_2 = a_2.T</span><br><span class="line">    a_2 = np.hstack((np.ones((m, <span class="number">1</span>)), a_2)) <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_3 = np.dot(Theta2, a_2.T)</span><br><span class="line">    a_3 = sigmoid(Z_3)</span><br><span class="line">    a_3 = a_3.T</span><br><span class="line"></span><br><span class="line">    one_hot_y = convert_to_one_hot(y, num_labels) <span class="comment"># 这里需要对y进行one-hot编码</span></span><br><span class="line"></span><br><span class="line">    Delta_2 = np.zeros_like(Theta2)</span><br><span class="line">    Delta_1 = np.zeros_like(Theta1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m): <span class="comment"># 对每一个样本</span></span><br><span class="line">        delta_3 = a_3[i] - one_hot_y[i]</span><br><span class="line">        grad = np.dot(Theta2.T, delta_3)</span><br><span class="line">        part_grad = np.delete(grad, <span class="number">0</span>)    <span class="comment"># 把偏置项删掉, 我不确定是否应该这样做</span></span><br><span class="line">        delta_2 = part_grad * sigmoid_gradient(Z_2[:,i])</span><br><span class="line">        Delta_2 = Delta_2 + np.dot(delta_3.reshape(-<span class="number">1</span>, <span class="number">1</span>), np.matrix(a_2[i]))</span><br><span class="line">        Delta_1 = Delta_1 + np.dot(delta_2.reshape(-<span class="number">1</span>, <span class="number">1</span>), np.matrix(a_1[i]))</span><br><span class="line">    </span><br><span class="line">    Theta2_grad = <span class="number">1</span>/m * Delta_2 + lmb/m * Theta2</span><br><span class="line">    Theta1_grad = <span class="number">1</span>/m * Delta_1 + lmb/m * Theta1</span><br><span class="line">    <span class="comment"># =====================================================</span></span><br><span class="line">    </span><br><span class="line">    grad = np.hstack((Theta1_grad.flatten(), Theta2_grad.flatten()))</span><br><span class="line">    grad = np.array(grad).flatten() <span class="comment"># 如果不加这一行，会在optimize.py等多处报错，例如deltak = numpy.dot(gfk, gfk)，error：(dim 1) != 1 (dim 0)</span></span><br><span class="line">    <span class="keyword">return</span> grad</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="误差反传训练算法"><a href="#误差反传训练算法" class="headerlink" title="误差反传训练算法"></a>误差反传训练算法</h2><h3 id="Sigmoid-函数及其梯度"><a href="#Sigmoid-函数及其梯度" class="headerlink" title="Sigmoid 函数及其梯度"></a><code>Sigmoid</code> 函数及其梯度</h3><p>Sigmoid 函数定义为</p>
<script type="math/tex; mode=display">\text{sigmoid}(z) = g(z) = \frac{1}{1+\exp(-z)}</script><p>Sigmoid 函数的梯度可以按照下式进行计算</p>
<script type="math/tex; mode=display">g^{\prime}(z) = \frac{d}{dz} g(z) = g(z)(1-g(z))</script><p>为验证你的实现是正确的，以下事实可供你参考。当<span> $z=0$ </span>是，梯度的精确值为 0.25 。当<span> $z$ </span>的值很大（可正可负）时，梯度值接近于0。</p>
<p>这里，你需要补充完成函数 <code>sigmoid</code> 与 <code>sigmoid_gradient</code> 。 你需要保证实现的函数的输入参数可以为矢量和矩阵( <code>numpy.ndarray</code>)。</p>
<h3 id="网络参数的随机初始化"><a href="#网络参数的随机初始化" class="headerlink" title="网络参数的随机初始化"></a>网络参数的随机初始化</h3><p>训练神经网络时，使用随机数初始化网络参数非常重要。一个非常有效的随机初始化策略为，在范围<span> $[ -\epsilon<em>{init}, \epsilon</em>{init} ]$ </span>内按照均匀分布随机选择参数<span> $\Theta^{(l)}$ </span>的初始值。这里你需要设置<span> $\epsilon_{init} = 0.12$ </span>。这个范围保证了参数较小且训练过程高效。</p>
<p>你需要补充实现函数 <code>rand_initialize_weigths</code> 。</p>
<p>对于一般的神经网络，如果第<span> $l$ </span>层的输入单元数为<span> $L_{in}$ </span>，输出单元数为<span> $L_{out}$ </span>，则<span> $\epsilon<em>{init} = {\sqrt{6}}/{\sqrt{L</em>{in} + L_{out}}}$ </span>可以做为有效的指导策略。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Sigmoid 函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-np.asarray(z)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算Sigmoid 函数的梯度&quot;&quot;&quot;</span></span><br><span class="line">    g = np.zeros_like(z)</span><br><span class="line">    <span class="comment"># ======================　你的代码 ======================</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算Sigmoid 函数的梯度g的值</span></span><br><span class="line">    g = sigmoid(z)*(<span class="number">1.0</span>-sigmoid(z))</span><br><span class="line">    <span class="comment"># =======================================================</span></span><br><span class="line">    <span class="keyword">return</span> g</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rand_initialize_weights</span>(<span class="params">L_in, L_out</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 初始化网络层权重参数&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    W = np.zeros((L_out, <span class="number">1</span> + L_in))</span><br><span class="line">    <span class="comment"># ====================== 你的代码 ======================</span></span><br><span class="line">    <span class="comment">#epsilon_init = 0.12</span></span><br><span class="line">    epsilon_init = np.sqrt(<span class="number">6.0</span>) / np.sqrt(L_in + L_out)</span><br><span class="line">    print(<span class="string">&#x27;epsilon_init: &#x27;</span>, epsilon_init)</span><br><span class="line">    <span class="comment">#初始化网络层的权重参数</span></span><br><span class="line">    M, N = W.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            W[i,j] = np.random.uniform(-epsilon_init, epsilon_init)</span><br><span class="line">    <span class="comment">#print(W)</span></span><br><span class="line">    <span class="comment"># ======================================================</span></span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug_initialize_weights</span>(<span class="params">fan_out, fan_in</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initalize the weights of a layer with</span></span><br><span class="line"><span class="string">    fan_in incoming connections and</span></span><br><span class="line"><span class="string">    fan_out outgoing connection using a fixed strategy.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    W = np.linspace(<span class="number">1</span>, fan_out*(fan_in+<span class="number">1</span>), fan_out*(fan_in+<span class="number">1</span>))</span><br><span class="line">    W = <span class="number">0.1</span>*np.sin(W).reshape(fan_out, fan_in + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_numerical_gradient</span>(<span class="params">cost_func, theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the numerical gradient of the given cost_func</span></span><br><span class="line"><span class="string">    at parameter theta&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    numgrad = np.zeros_like(theta)</span><br><span class="line">    perturb = np.zeros_like(theta)</span><br><span class="line">    eps = <span class="number">1.0e-4</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(theta)):</span><br><span class="line">        perturb[idx] = eps</span><br><span class="line">        loss1 = cost_func(theta - perturb)</span><br><span class="line">        loss2 = cost_func(theta + perturb)</span><br><span class="line">        numgrad[idx] = (loss2 - loss1)/(<span class="number">2</span>*eps)</span><br><span class="line">        perturb[idx] = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">return</span> numgrad</span><br></pre></td></tr></table></figure>
<h2 id="检查梯度"><a href="#检查梯度" class="headerlink" title="检查梯度"></a>检查梯度</h2><p>在神经网络中，需要最小化代价函数<span> $J(\Theta)$ </span>。为了检查梯度计算是否正确，考虑把参数<span> $\Theta^{(1)}$ </span>和<span> $\Theta^{(2)}$ </span>展开为一个长的向量<span> $\theta$ </span>。假设函数<span> $f_i(\theta)$ </span>表示<span> $\frac{\partial}{\partial \theta_i} J(\theta)$ </span>。</p>
<p>令</p>
<script type="math/tex; mode=display">\theta^{(i+)} = \theta + \begin{bmatrix} 0 \\ 0 \\ \vdots \\ \epsilon \\ \vdots \\ 0 \end{bmatrix} \qquad
  \theta^{(i-)} = \theta - \begin{bmatrix} 0 \\ 0 \\ \vdots \\ \epsilon \\ \vdots \\ 0 \end{bmatrix}</script><p>上式中，<span> $\theta^{(i+)}$ </span>除了第<span> $i$ </span>个元素增加了<span> $\epsilon$ </span>之 外，其他元素均与<span> $\theta$ </span>相同。类似的，<span> $\theta^{(i-)}$ </span>中仅第<span> $i$ </span>个元素减少了<span> $\epsilon$ </span>。可以使用数值近似验证<span> $f_i(\theta)$ </span>计算是否正确：</p>
<script type="math/tex; mode=display">f_i(\theta) \approx \frac{J(\theta^{(i+)}) - J(\theta^{(i-)})}{2\epsilon}</script><p>如果设<span> $\epsilon=10^{-4}$ </span>，通常上式左右两端的差异出现于第4位有效数字之后（经常会有更高的精度）。</p>
<p>在练习的程序代码中，函数 <code>compute_numerical_gradient</code> 已经实现，建议你认真阅读该函数并理解其实现原理与方案。</p>
<p>之后，程序将执行 <code>check_nn_gradients</code> 函数。该函数将创建一个较小的神经网络用于检测你的误差反传训练算法所计算得到的梯度是否正确。如果你的实现是正确的，你得到的 梯度与数值梯度之后的绝对误差（各分量的绝对值差之和）应当小于<span> $10^{-9}$ </span>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_nn_gradients</span>(<span class="params">lmb=<span class="number">0.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates a small neural network to check the backgropagation</span></span><br><span class="line"><span class="string">    gradients.&quot;&quot;&quot;</span></span><br><span class="line">    input_layer_size, hidden_layer_size = <span class="number">3</span>, <span class="number">5</span></span><br><span class="line">    num_labels, m = <span class="number">3</span>, <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    Theta1 = debug_initialize_weights(hidden_layer_size, input_layer_size)</span><br><span class="line">    Theta2 = debug_initialize_weights(num_labels, hidden_layer_size)</span><br><span class="line"></span><br><span class="line">    X = debug_initialize_weights(m, input_layer_size - <span class="number">1</span>)</span><br><span class="line">    y = np.array([<span class="number">1</span> + (t % num_labels) <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(m)])</span><br><span class="line">    nn_params = np.hstack((Theta1.flatten(), Theta2.flatten()))</span><br><span class="line"></span><br><span class="line">    cost_func = <span class="keyword">lambda</span> x: nn_cost_function(x,</span><br><span class="line">                                           input_layer_size,</span><br><span class="line">                                           hidden_layer_size,</span><br><span class="line">                                           num_labels, lmb, X, y)</span><br><span class="line">    grad = nn_grad_function(nn_params,</span><br><span class="line">                            input_layer_size, hidden_layer_size,</span><br><span class="line">                            num_labels, lmb, X, y)</span><br><span class="line">    numgrad = compute_numerical_gradient(cost_func, nn_params)</span><br><span class="line">    print(np.vstack((numgrad, grad)).T, np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(numgrad - grad)))</span><br><span class="line">    print(<span class="string">&#x27;The above two columns you get should be very similar.&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;(Left-Your Numerical Gradient, Right-Analytical Gradient)&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">Theta1, Theta2, X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;模型预测&quot;&quot;&quot;</span></span><br><span class="line">   </span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># num_labels = Theta2.shape[0]</span></span><br><span class="line"></span><br><span class="line">    p = np.zeros((m,<span class="number">1</span>), dtype=<span class="built_in">int</span>)</span><br><span class="line">    <span class="comment"># ====================== 你的代码============================</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 神经网络模型预测</span></span><br><span class="line">    </span><br><span class="line">    a_1 = np.hstack((np.ones((m, <span class="number">1</span>)), X))     <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_2 = np.dot(Theta1, a_1.T) </span><br><span class="line">    a_2 = sigmoid(Z_2)</span><br><span class="line">    a_2 = a_2.T</span><br><span class="line">    a_2 = np.hstack((np.ones((m, <span class="number">1</span>)), a_2)) <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_3 = np.dot(Theta2, a_2.T)</span><br><span class="line">    hypothesis = sigmoid(Z_3)</span><br><span class="line">    hypothesis = hypothesis.T</span><br><span class="line"></span><br><span class="line">    one_hot_to_val = np.argmax(hypothesis, axis=<span class="number">1</span>) + <span class="number">1.0</span></span><br><span class="line">    p = one_hot_to_val.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    ok = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">if</span> p[i] == y[i]:</span><br><span class="line">            ok = ok + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;ok&quot;</span>,ok)</span><br><span class="line"></span><br><span class="line">    acc_rate = ok * <span class="number">1.0</span> / m</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line">    <span class="keyword">return</span> acc_rate</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">input_layer_size = <span class="number">400</span>          <span class="comment"># 20x20 大小的输入图像，图像内容为手写数字</span></span><br><span class="line">hidden_layer_size = <span class="number">25</span>          <span class="comment"># 25 hidden units</span></span><br><span class="line">num_labels = <span class="number">10</span>                 <span class="comment"># 10 类标号 从1到10</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># =========== 第一部分 ===============</span></span><br><span class="line"><span class="comment"># 加载训练数据</span></span><br><span class="line">print(<span class="string">&quot;Loading and Visualizing Data...&quot;</span>)</span><br><span class="line">data = sio.loadmat(<span class="string">&#x27;data/NN_data.mat&#x27;</span>)</span><br><span class="line">X, y = data[<span class="string">&#x27;X&#x27;</span>], data[<span class="string">&#x27;y&#x27;</span>]</span><br><span class="line"></span><br><span class="line">m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机选取100个数据显示</span></span><br><span class="line">rand_indices = np.array(<span class="built_in">range</span>(m))</span><br><span class="line">np.random.shuffle(rand_indices)</span><br><span class="line">X_sel = X[rand_indices[:<span class="number">100</span>]]</span><br><span class="line"></span><br><span class="line">display_data(X_sel)</span><br></pre></td></tr></table></figure>
<pre><code>Loading and Visualizing Data...
</code></pre><p><img src="/img/PRML/output_23_1.png" alt="png"></p>
<h2 id="加载神经网络模型的权重"><a href="#加载神经网络模型的权重" class="headerlink" title="加载神经网络模型的权重"></a>加载神经网络模型的权重</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># =========== 第二部分 ===============</span></span><br><span class="line">print(<span class="string">&#x27;Loading Saved Neural Network Parameters ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the weights into variables Theta1 and Theta2</span></span><br><span class="line">data = sio.loadmat(<span class="string">&#x27;data/NN_weights.mat&#x27;</span>)</span><br><span class="line">Theta1, Theta2 = data[<span class="string">&#x27;Theta1&#x27;</span>], data[<span class="string">&#x27;Theta2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># print Theta1.shape, (hidden_layer_size, input_layer_size + 1)</span></span><br><span class="line"><span class="comment"># print Theta2.shape, (num_labels, hidden_layer_size + 1)</span></span><br></pre></td></tr></table></figure>
<pre><code>Loading Saved Neural Network Parameters ...
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ================ Part 3: Compute Cost (Feedforward) ================</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;\nFeedforward Using Neural Network ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight regularization parameter (we set this to 0 here).</span></span><br><span class="line">lmb = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">nn_params = np.hstack((Theta1.flatten(), Theta2.flatten()))</span><br><span class="line">J = nn_cost_function(nn_params,</span><br><span class="line">                     input_layer_size, hidden_layer_size,</span><br><span class="line">                     num_labels, lmb, X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Cost at parameters (loaded from PRML_NN_weights): %f &#x27;</span> % J)</span><br><span class="line">print(<span class="string">&#x27;(this value should be about 0.287629)&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Feedforward Using Neural Network ...
Cost at parameters (loaded from PRML_NN_weights): 0.287629 
(this value should be about 0.287629)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># =============== Part 4: Implement Regularization ===============</span></span><br><span class="line">print(<span class="string">&#x27;Checking Cost Function (w/ Regularization) ... &#x27;</span>)</span><br><span class="line">lmb = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">J = nn_cost_function(nn_params,</span><br><span class="line">                     input_layer_size, hidden_layer_size,</span><br><span class="line">                     num_labels, lmb, X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Cost at parameters (loaded from PRML_NN_weights): %f &#x27;</span> % J)</span><br><span class="line">print(<span class="string">&#x27;(this value should be about 0.383770)&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from PRML_NN_weights): 0.383770 
(this value should be about 0.383770)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ================ Part 5: Sigmoid Gradient  ================</span></span><br><span class="line">print(<span class="string">&#x27;Evaluating sigmoid gradient...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">g = sigmoid_gradient([<span class="number">1</span>, -<span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">&#x27;Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:  &#x27;</span>, g)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:   [0.19661193 0.23500371 0.25       0.23500371 0.19661193]
</code></pre><h2 id="神经网络参数初始化"><a href="#神经网络参数初始化" class="headerlink" title="神经网络参数初始化"></a>神经网络参数初始化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  ================ Part 6: Initializing Pameters ================</span></span><br><span class="line">print(<span class="string">&#x27;Initializing Neural Network Parameters ...&#x27;</span>)</span><br><span class="line">initial_Theta1 = rand_initialize_weights(input_layer_size, hidden_layer_size)</span><br><span class="line">initial_Theta2 = rand_initialize_weights(hidden_layer_size, num_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Unroll parameters</span></span><br><span class="line">initial_nn_params = np.hstack((initial_Theta1.flatten(),</span><br><span class="line">                               initial_Theta2.flatten()))</span><br></pre></td></tr></table></figure>
<pre><code>Initializing Neural Network Parameters ...
epsilon_init:  0.1188177051572009
epsilon_init:  0.4140393356054125
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># =============== Part 7: Implement Backpropagation ===============</span></span><br><span class="line">print(<span class="string">&#x27;Checking Backpropagation... &#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check gradients by running checkNNGradients</span></span><br><span class="line">check_nn_gradients()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Checking Backpropagation... 
[[ 1.27220311e-02  1.27220311e-02]
 [ 1.58832809e-04  1.58832809e-04]
 [ 2.17690452e-04  2.17690455e-04]
 [ 7.64045027e-05  7.64045009e-05]
 [ 6.46352264e-03  6.46352265e-03]
 [ 2.34983744e-05  2.34983735e-05]
 [-3.74199116e-05 -3.74199098e-05]
 [-6.39345021e-05 -6.39345006e-05]
 [-5.74199923e-03 -5.74199923e-03]
 [-1.34052016e-04 -1.34052019e-04]
 [-2.59146269e-04 -2.59146269e-04]
 [-1.45982635e-04 -1.45982634e-04]
 [-1.26792390e-02 -1.26792390e-02]
 [-1.67913183e-04 -1.67913187e-04]
 [-2.41809017e-04 -2.41809017e-04]
 [-9.33867517e-05 -9.33867522e-05]
 [-7.94573534e-03 -7.94573535e-03]
 [-4.76254503e-05 -4.76254501e-05]
 [-2.64923861e-06 -2.64923844e-06]
 [ 4.47626736e-05  4.47626708e-05]
 [ 1.09347722e-01  1.09347722e-01]
 [ 5.67965185e-02  5.67965185e-02]
 [ 5.25298306e-02  5.25298306e-02]
 [ 5.53542907e-02  5.53542907e-02]
 [ 5.59290833e-02  5.59290833e-02]
 [ 5.23534682e-02  5.23534682e-02]
 [ 1.08133003e-01  1.08133003e-01]
 [ 5.67319602e-02  5.67319602e-02]
 [ 5.14442931e-02  5.14442931e-02]
 [ 5.48296085e-02  5.48296085e-02]
 [ 5.56926532e-02  5.56926532e-02]
 [ 5.11795651e-02  5.11795651e-02]
 [ 3.06270372e-01  3.06270372e-01]
 [ 1.59463135e-01  1.59463135e-01]
 [ 1.45570264e-01  1.45570264e-01]
 [ 1.56700533e-01  1.56700533e-01]
 [ 1.56043968e-01  1.56043968e-01]
 [ 1.45771544e-01  1.45771544e-01]] 9.96691174908528e-11
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># =============== Part 8: Implement Regularization ===============</span></span><br><span class="line">print(<span class="string">&#x27;Checking Backpropagation (w/ Regularization) ... &#x27;</span>)</span><br><span class="line"><span class="comment"># Check gradients by running checkNNGradients</span></span><br><span class="line">lmb = <span class="number">3.0</span></span><br><span class="line">check_nn_gradients(lmb)</span><br></pre></td></tr></table></figure>
<pre><code>Checking Backpropagation (w/ Regularization) ... 
[[ 0.01272203  0.06321029]
 [ 0.05471668  0.05471668]
 [ 0.00868489  0.00868489]
 [-0.04533175 -0.04533175]
 [ 0.00646352 -0.05107193]
 [-0.01674143 -0.01674143]
 [ 0.03938178  0.03938178]
 [ 0.05929756  0.05929756]
 [-0.005742    0.01898511]
 [-0.03277532 -0.03277532]
 [-0.06025856 -0.06025856]
 [-0.03234036 -0.03234036]
 [-0.01267924  0.01253078]
 [ 0.05926853  0.05926853]
 [ 0.03877546  0.03877546]
 [-0.01736759 -0.01736759]
 [-0.00794574 -0.06562958]
 [-0.04510686 -0.04510686]
 [ 0.00898998  0.00898998]
 [ 0.05482148  0.05482148]
 [ 0.10934772  0.15983598]
 [ 0.11135436  0.11135436]
 [ 0.06099703  0.06099703]
 [ 0.00994614  0.00994614]
 [-0.00160637 -0.00160637]
 [ 0.03558854  0.03558854]
 [ 0.108133    0.1475522 ]
 [ 0.11609346  0.11609346]
 [ 0.0761714   0.0761714 ]
 [ 0.02218834  0.02218834]
 [-0.00430676 -0.00430676]
 [ 0.01898519  0.01898519]
 [ 0.30627037  0.33148039]
 [ 0.21889958  0.21889958]
 [ 0.18458753  0.18458753]
 [ 0.13942633  0.13942633]
 [ 0.09836012  0.09836012]
 [ 0.10071231  0.10071231]] 0.33076217369064975
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)
</code></pre><h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># =================== Part 8: Training NN ===================</span></span><br><span class="line">print(<span class="string">&#x27;Training Neural Network...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">lmb, maxiter = <span class="number">1.0</span>, <span class="number">50</span></span><br><span class="line">args = (input_layer_size, hidden_layer_size, num_labels, lmb, X, y)</span><br><span class="line">nn_params, cost_min, _, _, _ = fmin_cg(nn_cost_function,</span><br><span class="line">                                       initial_nn_params,</span><br><span class="line">                                       fprime=nn_grad_function,</span><br><span class="line">                                       args=args,</span><br><span class="line">                                       maxiter=maxiter,</span><br><span class="line">                                       full_output=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Theta1 = nn_params[:hidden_layer_size*(input_layer_size + <span class="number">1</span>)]</span><br><span class="line">Theta1 = Theta1.reshape((hidden_layer_size, input_layer_size + <span class="number">1</span>))</span><br><span class="line">Theta2 = nn_params[hidden_layer_size*(input_layer_size + <span class="number">1</span>):]</span><br><span class="line">Theta2 = Theta2.reshape((num_labels, hidden_layer_size + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training Neural Network...
Warning: Maximum number of iterations has been exceeded.
         Current function value: 0.449704
         Iterations: 50
         Function evaluations: 99
         Gradient evaluations: 99
</code></pre><h2 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ================= Part 9: Implement Predict =================</span></span><br><span class="line"></span><br><span class="line">pred = predict(Theta1, Theta2, X)</span><br><span class="line"><span class="comment"># print(pred.shape, y.shape)</span></span><br><span class="line"><span class="comment"># print(np.hstack((pred, y)))</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Training Set Accuracy:&#x27;</span>, pred)</span><br></pre></td></tr></table></figure>
<pre><code>ok 4796
Training Set Accuracy: 0.9592
</code></pre>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Modern C++ 学习笔记</title>
    <url>/2018/07/18/04.modern_cpp/</url>
    <content><![CDATA[<p>总结C++11、C++14的学习笔记。<br><a id="more"></a> </p>
<h1 id="类别推导"><a href="#类别推导" class="headerlink" title="类别推导"></a>类别推导</h1><hr>
<p>C++98有用于函数模板的推导规则，C++11和C++14增加了用于auto和deltype的推导规则。</p>
<h2 id="模板类型推导"><a href="#模板类型推导" class="headerlink" title="模板类型推导"></a>模板类型推导</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 函数模板</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(ParamType Param)</span></span>;</span><br><span class="line"><span class="comment">// 函数调用</span></span><br><span class="line">f(expr);</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">27</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> cx = x;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>&amp; rx = x;</span><br></pre></td></tr></table></figure>
<p>编译器会根据expr推导ParamType和T，两者有所差别。</p>
<p>情况1：ParamType是指针or引用<br>T的推导结果会忽略expr的指针or引用，其他类型修饰如<code>const</code>会被保留。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(T&amp; Param)</span></span>;</span><br><span class="line">f(x);  <span class="comment">// T: int        param: int&amp;</span></span><br><span class="line">f(cx); <span class="comment">// T: const int  param: const int&amp;</span></span><br><span class="line">f(rc); <span class="comment">// T: const int  param: const int&amp;</span></span><br></pre></td></tr></table></figure></h2><p>情况2：ParamType是万能引用<br>如果expr是左值，T和Param都会被推导为<code>左值引用</code>(唯一情况)。<br>如果expr是右值，和<code>情况1</code>相同。</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(T&amp;&amp; Param)</span></span>;</span><br><span class="line">f(x);  <span class="comment">// T: int&amp;        param: int&amp;</span></span><br><span class="line">f(cx); <span class="comment">// T: const int&amp;  param: const int&amp;</span></span><br><span class="line">f(rc); <span class="comment">// T: const int&amp;  param: const int&amp;</span></span><br><span class="line">f(<span class="number">27</span>); <span class="comment">// T: int         param: int&amp;&amp;</span></span><br></pre></td></tr></table></figure></h2><p>情况3：按值传递<br>param是一个全新的对象。引用、const、volatile性质都会忽略。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(T Param)</span></span>;</span><br><span class="line">f(x);  <span class="comment">// T: int  param: int</span></span><br><span class="line">f(cx); <span class="comment">// T: int  param: int</span></span><br><span class="line">f(rc); <span class="comment">// T: int  param: int</span></span><br></pre></td></tr></table></figure><br>考虑一个特殊情况，const指针在传递中，自身const会被忽略，指向的对象const会被保留。</p>
<h2 id="-2"><a href="#-2" class="headerlink" title=""></a><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>* <span class="keyword">const</span> ptr = &amp;x;</span><br></pre></td></tr></table></figure></h2><p>情况4：数组实参与函数实参<br>数组指针在传参过程中会退化成指向数组首元素的指针，可以通过把形参声明成数组的引用，得到实际的数组类别。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="built_in">std</span>::<span class="keyword">size_t</span> N&gt;</span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="built_in">std</span>::<span class="keyword">size_t</span> <span class="title">arraySize</span><span class="params">(T (&amp;)[N])</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> N;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> keyVals[] = &#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>&#125;;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">array</span>&lt;<span class="keyword">int</span>, arraySize(keyVals)&gt; mappedVals;</span><br></pre></td></tr></table></figure><br>函数类型也会退化成指针。处理方式和上面一样。</p>
<h2 id="auto推导"><a href="#auto推导" class="headerlink" title="auto推导"></a>auto推导</h2><p>首先，auto处理数组实参与函数实参也会退化。<br>其次，auto在初始化时，如果表达式是<code>&#123;&#125;</code>括起来的，会按照<code>std::initializer_list&lt;T&gt;</code>推导，如果<code>T</code>推导失败，模板推导也会失败。<br>在函数返回值使用<code>auto</code>，不会推导<code>std::initializer_list&lt;T&gt;</code>而是常规的模板推导。</p>
<h2 id="decltype"><a href="#decltype" class="headerlink" title="decltype"></a>decltype</h2><p><code>decltype</code>主要用在声明那些返回值依赖形参类型的函数。<br>C++14允许对一切<code>lambda式</code>和一切函数进行推导，不过会有隐患，所以需要<code>decltype</code>。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Container, <span class="keyword">typename</span> Index&gt;</span><br><span class="line"><span class="keyword">auto</span> </span><br><span class="line">authAndAccess(Container&amp;&amp; c, Index i)</span><br><span class="line">-&gt; <span class="keyword">decltype</span>(<span class="built_in">std</span>::forward&lt;Container&gt;(c)[i])</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::forward&lt;Container&gt;(c)[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><code>decltype(x)</code>结果为<code>int</code>，<code>decltype((x))</code>结果为<code>int&amp;</code>，小心<code>decltype(auto)</code>，为了保证推导完全没有隐患，可以看第四节。</p>
<h2 id="类型推导结果"><a href="#类型推导结果" class="headerlink" title="类型推导结果"></a>类型推导结果</h2><p>这算是奇技淫巧吧，通过编译器诊断信息。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="class"><span class="keyword">class</span> <span class="title">TD</span>;</span>  <span class="comment">// 只声明</span></span><br><span class="line">TD&lt;<span class="keyword">decltype</span>(x)&gt; xType;         <span class="comment">// 诱发编译器产生类型错误</span></span><br></pre></td></tr></table></figure><br>运行时输出类型，涉及到<code>std::type_info::name</code>，不保证输出任何有意义的内容。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="keyword">typeid</span>(x).name() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><br><code>std::type_info::name</code>处理类型的方式类似于函数模板按值传递，因此得到的类型可能不准确。<br><code>Boost.TypeIndex</code>可以产生精确的类型信息，函数模板<code>boost::typeindex::type_id_with_cvr</code>接受一个类型实参，而且不会移除<code>const</code>、<code>volatile</code>和引用，返回一个<code>boost::typeindex::type_index</code>对象，最后调用<code>pretty_name()</code>。</p>
<h1 id="auto"><a href="#auto" class="headerlink" title="auto"></a>auto</h1><hr>
<h2 id="使用auto"><a href="#使用auto" class="headerlink" title="使用auto"></a>使用auto</h2><ol>
<li>使用<code>std::function</code>声明、储存一个闭包的变量是<code>std::function</code>的一个实例，占有固定内存，空间不够的时候会分配堆上内存。</li>
<li>使用<code>auto</code>声明、储存一个闭包的变量和该闭包是一个类型，要求的内存一样。比<code>std::function</code>更优。</li>
<li>像<code>std::vector&lt;int&gt;::size_type</code>这样的类型跟平台有关，建议<code>auto</code></li>
<li>像<code>std::unordered_map&lt;const std::string, int&gt;</code>这样的类型，显式指定容易引起不想要的类型转换，建议<code>auto</code></li>
</ol>
<h2 id="显式初始化"><a href="#显式初始化" class="headerlink" title="显式初始化"></a>显式初始化</h2><p><code>auto</code>的结果不能总是满足期望，会有意外。<br><strong>例子：</strong><br><code>std::vector&lt;bool&gt;</code>对象执行<code>std::vector::operator[]</code>后，返回<code>std::vector&lt;bool&gt;::reference</code>类型，这是嵌套在<code>std::vector&lt;bool&gt;</code>里的类，然后做了一个向<code>bool</code>的隐式转换。<br><strong>原理：</strong><br>因为过特化，<code>bool</code>被压缩形式表示，<code>std::vector::operator[]</code>返回<code>T&amp;</code>，但是C++不允许比特引用。<br><code>std::vector&lt;bool&gt;::reference</code>要保证能用到<code>bool&amp;</code>的地方它也能用，所以做了一个向<code>bool</code>的隐式转换，但不是<code>bool&amp;</code>。<br><strong>意外：</strong><br>使用auto会导致容器元素被推导成<code>std::vector&lt;bool&gt;::reference</code>，这样再使用下标<code>[]</code>就是返回第几个比特，而不是第几个元素。<br><strong>后果：</strong><br><code>std::vector&lt;bool&gt;::reference</code>对象的一种实现是含有一个指针，指向一个机器字，该Word有那个被引用的比特，再加上基于那个比特对应的字的偏移量。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">processWidget(w, highPriority); <span class="comment">// Error, highPriority含有悬空指针</span></span><br></pre></td></tr></table></figure><br>“隐形”代理类（还有表达式模板）和<code>auto</code>无法和谐相处，这种类的对象往往会设计成仅仅维持到单个语句之内存在。<br>使用显式的强制转换，得到想要的类型，避开代理类的暗坑。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> highPriority = <span class="keyword">static_cast</span>&lt;<span class="keyword">bool</span>&gt;(feature(w)[<span class="number">5</span>]);</span><br><span class="line"><span class="keyword">auto</span> sum = <span class="keyword">static_cast</span>&lt;Matrix&gt;(m1+m2+m3+m4);</span><br></pre></td></tr></table></figure></p>
<h1 id="modern-C"><a href="#modern-C" class="headerlink" title="modern C++"></a>modern C++</h1><hr>
<h2 id="关于大括号"><a href="#关于大括号" class="headerlink" title="关于大括号"></a>关于大括号</h2><ol>
<li>大括号禁止内建类型之间进行隐式窄化转换。</li>
<li>C++任何能够解析为声明的都会解析为声明，用<code>&#123;&#125;</code>调用默认构造函数初始化对象可以避免被当成函数声明。</li>
<li>在构造函数被调用时，形参中没有<code>std::initialier_list</code>，那么大小括号没有区别；如果有，则<code>&#123;&#125;</code>会优先使用带<code>std::initialier_list</code>的构造函数。</li>
<li>空的大括号表示“没有实参”，而不是空的<code>std::initialier_list</code>。如果要调用一个带有<code>std::initialier_list</code>的构造函数，并且传入一个空的<code>std::initialier_list</code>，可以这样写:<code>&#123; &#123; &#125; &#125;</code>。</li>
<li>在设计构造函数的时候，<code>std::vector</code>是个反例，不要学它。</li>
<li>更具有弹性的设计，允许调用者自行决定使用大括号还是小括号，<span class="exturl" data-url="aHR0cHM6Ly9ha3J6ZW1pMS53b3JkcHJlc3MuY29tLzIwMTMvMDYvMDUvaW50dWl0aXZlLWludGVyZmFjZS1wYXJ0LWkv">Intuitive interface, Andrzej<i class="fa fa-external-link-alt"></i></span></li>
</ol>
<h2 id="使用nullptr"><a href="#使用nullptr" class="headerlink" title="使用nullptr"></a>使用nullptr</h2><ol>
<li><code>0</code>和<code>NULL</code>都不具备指针的类别，在指针型和整型之间进行重载时容易发生意外。</li>
<li><code>nullptr</code>的实际类型的<code>std::nullptr_t</code>，而<code>std::nullptrd</code>的类型被指定为<code>nullptr</code>，<code>nulllptr</code>可以隐式转换到所有的裸指针上。</li>
</ol>
<p>将nullptr用于模板，适当的互斥量锁定，调用，解锁<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> FuncType,</span><br><span class="line">         <span class="keyword">typename</span> MuxType,</span><br><span class="line">         <span class="keyword">typename</span> PtrType&gt;</span><br><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">lockAndCall</span><span class="params">(FuncType func,</span></span></span><br><span class="line"><span class="function"><span class="params">                 MuxType&amp; mutex,</span></span></span><br><span class="line">                 PtrType ptr) -&gt; decltype(func(ptr))</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function">MuxGuard <span class="title">g</span><span class="params">(mutex)</span></span>;</span><br><span class="line">    <span class="keyword">return</span> func(ptr);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//调用</span></span><br><span class="line"><span class="keyword">auto</span> result = lockAndCall(f,fm,<span class="literal">nullptr</span>);</span><br></pre></td></tr></table></figure></p>
<h2 id="使用using代替typedef"><a href="#使用using代替typedef" class="headerlink" title="使用using代替typedef"></a>使用using代替typedef</h2><ol>
<li><code>typedef</code>不支持模板化，但别名声明支持。</li>
<li>别名模板可以免写<code>::type</code>后缀，在模板内，对于内嵌<code>typedef</code>的引用经常要加上<code>typename</code>前缀。</li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 处理函数指针</span></span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">void</span> <span class="params">(*FP)</span><span class="params">(<span class="keyword">int</span>, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp;)</span></span>;</span><br><span class="line"><span class="keyword">using</span> FP = <span class="keyword">void</span> (*)(<span class="keyword">int</span>, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在处理模板时</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt; <span class="class"><span class="keyword">struct</span> <span class="title">MyAllocList</span>&#123;</span></span><br><span class="line">    <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">list</span>&lt;T, MyAlloc&lt;T&gt;&gt; type;</span><br><span class="line">&#125;;</span><br><span class="line">MyAllocList&lt;Widget&gt;::type lw;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">using</span> MyAllocList = <span class="built_in">std</span>::<span class="built_in">list</span>&lt;T, MyAlloc&lt;T&gt;&gt;;</span><br><span class="line">MyAllocList&lt;Widget&gt; lw;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 模板内使用typedef</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line">    <span class="keyword">typename</span> MyAllocList&lt;T&gt;::type <span class="built_in">list</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line">    MyAllocList&lt;T&gt; <span class="built_in">list</span>;          <span class="comment">// 使用了using</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>对编译器来讲，<code>MyAllocList&lt;T&gt;</code>别名模板命名了一个类型，是非依赖性的，所以<code>typename</code>不要求也不允许。而<code>MyAllocList&lt;Widget&gt;::type</code>不能确定是否是一个类型，在某个特化中，代表并非类型而是其他什么的东西，所以要加<code>typename</code>。</p>
<p>Note:从模板类型形参出发创建其修正类型<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;type_traits&gt;</span></span></span><br><span class="line"><span class="comment">//c++11</span></span><br><span class="line"><span class="built_in">std</span>::remove_const&lt;T&gt;::type</span><br><span class="line"><span class="built_in">std</span>::remoeve_reference&lt;T&gt;::type</span><br><span class="line"><span class="built_in">std</span>::add_lvalue_reference&lt;T&gt;::type</span><br><span class="line"><span class="comment">//c++14</span></span><br><span class="line"><span class="built_in">std</span>::<span class="keyword">remove_const_t</span>&lt;T&gt;</span><br><span class="line"><span class="built_in">std</span>::<span class="keyword">remoeve_reference_t</span>&lt;T&gt;</span><br><span class="line"><span class="built_in">std</span>::<span class="keyword">add_lvalue_reference_t</span>&lt;T&gt;</span><br><span class="line"><span class="comment">// using</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">remove_const_t</span> = <span class="built_in">std</span>::remove_const&lt;T&gt;::type;</span><br></pre></td></tr></table></figure></p>
<h2 id="限定作用域的枚举"><a href="#限定作用域的枚举" class="headerlink" title="限定作用域的枚举"></a>限定作用域的枚举</h2><p>关于C++98的枚举：</p>
<ol>
<li>容易造成命名空间的污染。</li>
<li>可以隐式转换到<code>int</code>，甚至可以进一步转换到<code>float</code>，算个隐患。</li>
<li>不能前置声明（在C++11中可以了），增加了编译依赖性。</li>
<li>为了节约使用内存，编译器通常会为枚举分配刚好够用的最小底层类型。</li>
</ol>
<p>关于C++11的枚举：</p>
<ol>
<li>通过<code>enum class</code>声明，枚举类。</li>
<li>枚举类型更强，不允许隐式转换。</li>
<li>可以前置声明了，而且可以指定枚举的底层类型，比如<code>enum class Color: std::uint8_t;</code></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> UserInfo = <span class="built_in">std</span>::tuple&lt;<span class="built_in">std</span>::<span class="built_in">string</span>, <span class="built_in">std</span>::<span class="built_in">string</span>, <span class="built_in">std</span>::<span class="keyword">size_t</span>&gt;;</span><br><span class="line"><span class="comment">// 使用限定作用域的枚举，缺点是需要强制转换</span></span><br><span class="line"><span class="class"><span class="keyword">enum</span> <span class="keyword">class</span> <span class="title">userInfoFields</span> &#123;</span>uiName, uiEmail, uiReputation &#125;;</span><br><span class="line">UserInfo uInfo;</span><br><span class="line"><span class="keyword">auto</span> val = </span><br><span class="line"><span class="built_in">std</span>::get&lt;<span class="keyword">static_cast</span>&lt;<span class="built_in">std</span>::<span class="keyword">size_t</span>&gt;(UserInfoFields::uiEmail)&gt;(uInfo);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 用std::underly_type得到枚举的底层类型，type_traits</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> E&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">typename</span> <span class="built_in">std</span>::underly_type&lt;E&gt;::type</span><br><span class="line">    toUType(E enumerator) <span class="keyword">noexcept</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;<span class="keyword">typename</span></span><br><span class="line">           <span class="built_in">std</span>::underlying_type&lt;E&gt;::type&gt;(enumerator);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 用函数代替强制转换</span></span><br><span class="line"><span class="built_in">std</span>::get&lt;toUType(UserInfoFields::uiEmail)&gt;(uInfo);</span><br></pre></td></tr></table></figure>
<h2 id="删除函数"><a href="#删除函数" class="headerlink" title="删除函数"></a>删除函数</h2><ol>
<li>声明<code>private</code>函数，用<code>delete</code>代替，无法通过任何方式访问。</li>
<li>任何函数都能成为删除函数，在函数重载中可以避免不想要的重载，在函数模板中，可以避免不想要的具体化。</li>
<li>模板特化必须在命名空间作用域，在类作用于不允许。因此，类内部的函数模板不想要的特化用<code>delete</code>。</li>
</ol>
<h2 id="override声明"><a href="#override声明" class="headerlink" title="override声明"></a>override声明</h2><p>在派生类声明一个函数，意在重写基类虚函数时，加上<code>override</code>声明。<br>C++对重写有严格要求，很容易就声明了一个新函数：</p>
<ol>
<li>基类的函数必须是虚函数。</li>
<li>函数名字必须完全一样（析构函数除外）。</li>
<li>形参类型必须完全一样</li>
<li>函数的后缀性质完全一样。</li>
<li>函数返回值和一场规格必须兼容。</li>
</ol>
<p>C++11新增成员函数引用特性，为了给<code>*this</code>加一些区分度，原理同<code>const</code>。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> DataType = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt;;</span><br><span class="line">    <span class="function">DataType&amp; <span class="title">data</span><span class="params">()</span> &amp; </span>&#123;<span class="keyword">return</span> values;&#125;</span><br><span class="line">    <span class="function">DataType <span class="title">data</span><span class="params">()</span> &amp;&amp; </span>&#123;<span class="keyword">return</span> <span class="built_in">std</span>::move(values);&#125; <span class="comment">// 移动语义</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h2 id="const-iterator"><a href="#const-iterator" class="headerlink" title="const_iterator"></a>const_iterator</h2><p>在C++98中，很难从一个<code>非const</code>容器得到对应的<code>const</code>容器，插入删除只能以<code>iterator</code>指定，而不接受<code>const_iterator</code>。从<code>const_iterator</code>到<code>iterator</code>不存在可移植的类型转换。C++11解决了这些问题，并且指示位置的迭代器都更换成了<code>const_iterator</code>。<br>写最通用化的库代码，需要考虑以非成员函数提供接口的情况，对于非成员函数版本的支持：</p>
<ol>
<li><code>begin</code>、<code>end</code> (c++11)</li>
<li><code>cbegin</code>、<code>cend</code>、<code>rbegin</code>、<code>rend</code>、<code>crbegin</code>、<code>crend</code> (c++14)</li>
</ol>
<p>写一个<code>cbegin</code>的实现<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">C</span>&gt;</span></span><br><span class="line">auto cbegin(const C&amp; container)-&gt;decltype(std::begin(container))</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::begin(container);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>这里通过<code>const引用</code>类型产生一个类似<code>const_iterator</code>的效果。</p>
<h2 id="noexcept声明"><a href="#noexcept声明" class="headerlink" title="noexcept声明"></a>noexcept声明</h2><p>如果函数f运行期出发了异常，<br>C++98：调用栈会开解到f的调用者，然后执行一些瞎操作，程序执行终止。<br>C++11：程序终止之前，栈只是<code>可能会</code>开解。<br>在带有<code>noexcept</code>声明的函数中，优化器不需要将执行期栈保持在可以开解的状态，也不需要在异常溢出的前提下，保证里面的对象按照构造顺序逆序析构。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">ReType <span class="title">func</span><span class="params">(params)</span> <span class="keyword">noexcept</span></span>; <span class="comment">// 最优化</span></span><br><span class="line"><span class="function">ReType <span class="title">func</span><span class="params">(params)</span> <span class="title">throw</span><span class="params">()</span></span>;  <span class="comment">// 优化不够</span></span><br></pre></td></tr></table></figure><br>std::vector的push_back操作是异常安全保证的（遗留代码会依赖这样的特性），<code>std::vector::push_back</code>调用<code>std::move_if_noexcept</code>，接着向<code>std::is_nothrow_move_constructible</code>（模板特征）求助。<br>类似这样的接口都使用“能移动则移动，必须复制才复制”的策略，也就是<code>push_back</code>是否<code>noexcept</code>取决于push对象的移动构造函数是否是<code>noexcept</code>的。<br>另一个例子是<code>swap</code>，这些函数带有条件式的<code>noexcept</code>声明，高阶数据结构的<code>swap</code>行为要依赖低阶数据结构的<code>swap</code>行为，以此类推。</p>
<p>大多数函数都是异常中立的，自身不抛出异常，但内部调用的函数可能会发生异常，发生异常时，会允许异常经由它传递到调用栈更深的一层，就像路过一样。不具备<code>noexcept</code>。</p>
<p>C++98：允许内存释放函数（operator、delete、析构）触发异常，允许但是糟糕。<br>C++11：默认所有的内存释放函数和析构函数都是<code>noexcept</code>，除非显式声明<code>noexcept(false)</code>。</p>
<p>宽约束函数：没有调用的限制条件，也不会出现未定义行为。<br>窄约束函数：对调用有条件限制，就能异常。</p>
<h2 id="使用constexpr"><a href="#使用constexpr" class="headerlink" title="使用constexpr"></a>使用constexpr</h2><p><code>constexpr</code>是对象和函数接口的组成部分，<code>constexpr</code>对象比<code>const</code>对象更“常量”，符合编译期常量的语境。<br><code>constexpr</code>函数在调用时若传入的是编译器常量，则返回的也是常量，如果传入的是直到运行期才知晓的值，就和普通函数一样，但如果所有实参都在编译期未知，那么代码无法通过编译。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">int</span> <span class="title">pow</span><span class="params">(<span class="keyword">int</span> base, <span class="keyword">int</span> <span class="built_in">exp</span>)</span> <span class="keyword">noexcept</span> </span>&#123;...&#125;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">auto</span> num = <span class="number">5</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">array</span>&lt;<span class="keyword">int</span>, <span class="built_in">pow</span>(3,num)&gt; result;</span><br></pre></td></tr></table></figure><br>在C++11中，<code>constexpr</code>函数不得包含多于一个可执行语句，C++14解除了限制。<br><code>constexpr</code>可以让更多运行期进行的工作在编译期完成。</p>
<h2 id="const成员函数的线程安全"><a href="#const成员函数的线程安全" class="headerlink" title="const成员函数的线程安全"></a>const成员函数的线程安全</h2><p>对于单个要求同步的变量或内存区域，使用<code>std::atomic</code>就足够了。如果有更多的内存区域需要同步，就要使用<code>std::mutex</code>。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Polynomial</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> RootsType = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt;;</span><br><span class="line">    <span class="function">RootsType <span class="title">roots</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">    	<span class="function"><span class="built_in">std</span>::lock_guard&lt;<span class="built_in">std</span>::mutex&gt; <span class="title">g</span><span class="params">(m)</span></span>; <span class="comment">// 加上互斥量</span></span><br><span class="line">    	<span class="keyword">if</span> (!rootsAreValid)</span><br><span class="line">    	&#123;</span><br><span class="line">    		rootsAreValid = <span class="literal">true</span>;</span><br><span class="line">    	&#125;</span><br><span class="line">    	<span class="keyword">return</span> rootVals;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">mutable</span> <span class="built_in">std</span>::mutex m;</span><br><span class="line">    <span class="keyword">mutable</span> <span class="keyword">bool</span> rootsAreValid&#123; <span class="literal">false</span> &#125;;</span><br><span class="line">    <span class="keyword">mutable</span> RootsType rootVals&#123;&#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h2 id="成员函数的生成机制"><a href="#成员函数的生成机制" class="headerlink" title="成员函数的生成机制"></a>成员函数的生成机制</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Widget(Widget&amp;&amp; rhs);            <span class="comment">// 移动构造</span></span><br><span class="line">    Widget&amp; <span class="keyword">operator</span>=(Widget&amp;&amp; rhs); <span class="comment">// 移动赋值</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>对于移动操作：</p>
<ol>
<li>移动操作和复制操作一样，仅作用于非静态成员，同时也会相应地构造/赋值基类部分。</li>
<li>移动操作不一定真的成功，而是一种请求。对于不可移动得类型，会按照复制操作实现“移动”。</li>
<li>移动操作的核心在于把<code>std::move</code>应用于每个对象，其返回值被用于函数重载，最终决定执行移动还是复制。</li>
</ol>
<p>两种复制操作彼此独立：</p>
<ol>
<li>声明了一个，并不会阻止编译器生成另一个。</li>
<li>一旦显式声明了移动操作，编译器就会废除复制操作，通过<code>=delete</code>。</li>
</ol>
<p>两种移动操作不独立：</p>
<ol>
<li>声明了其中一个，编译器就不会生成另一个。因为只要声明了移动操作，就表示移动操作的实现方式会和编译器默认生成的行为多少有些不同。</li>
<li>一旦显式声明了复制操作，这个类也不会默认生成移动操作了，理由同上。</li>
</ol>
<p>大三律原则：</p>
<ol>
<li>如果有改写复制操作的需求，往往意味着该类需要执行某种资源管理。默认生成的操作不适用，而且需要正确的析构。</li>
<li>标准库中用以管理内存的类都会遵从原则。</li>
<li>声明了析构函数，那么默认生成的复制可能不适用，或者说此时复制操作就不该默认生成，但是从C++98到C++11，保留了这一特性。</li>
<li>只要声明了析构函数，就不会生成移动操作。</li>
</ol>
<p>移动操作生成的条件：该类没有任何复制/移动/析构操作。这些标准可以延伸到复制操作上，在已经存在复制/析构条件下，仍然自动生成复制操作已经成为被废弃的特性，在代码中应该尽可能消除这样的依赖。<br>C++11可以通过<code>=default</code>来显式表达这个想法：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ~Widget(); </span><br><span class="line">    Widget(<span class="keyword">const</span> Widget&amp;) = <span class="keyword">default</span>;</span><br><span class="line">    Widget&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Widget&amp;) = <span class="keyword">default</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br>这种写法在多态基类中常见。<br>一旦声明了析构函数，移动操作的生成就会被抑制，加上<code>=default</code>能够再给编译器一次机会，声明移动又会废除复制，可以再加一轮<code>=default</code>。这个没啥用，但是啥都不写可能一不注意就引发性能问题。</p>
<p>函数模板不会影响到成员函数的生成。</p>
<h1 id="智能指针"><a href="#智能指针" class="headerlink" title="智能指针"></a>智能指针</h1><hr>
<h2 id="std-unique-ptr"><a href="#std-unique-ptr" class="headerlink" title="std::unique_ptr"></a>std::unique_ptr</h2><p><code>std::unique_ptr</code>和裸指针拥有相同的尺寸，只能移动不能复制，移动一个std::unique_ptr会移动所有权，原指针自动挂空。执行析构时，由非空的<code>std::unique-ptr</code>内部的裸指针完成。<br><code>std::unique_ptr</code>随对象主体的析构而被析构，如果是异常或非典型流程，最终调用该资源的析构函数析构。析构默认通过delete，也可以自定义析构器。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自定义析构器</span></span><br><span class="line"><span class="keyword">auto</span> delInvmt = [](Investment* pInvestment)</span><br><span class="line">                &#123;</span><br><span class="line">                    makeLogEntry(pInvestment); <span class="comment">// 删除前先写入日志</span></span><br><span class="line">                    <span class="keyword">delete</span> pInvestment;</span><br><span class="line">                &#125;;</span><br><span class="line"><span class="comment">// 工厂函数</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;Investment, <span class="keyword">decltype</span>(delInvmt)&gt;</span><br><span class="line">makeInvestment(Ts&amp;... params)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;Investment, <span class="keyword">decltype</span>(delInvmt)&gt;</span><br><span class="line">        pInv(<span class="literal">nullptr</span>, delInvmt);</span><br><span class="line">    <span class="comment">// 一些创建对象的操作</span></span><br><span class="line">    pInv.reset(<span class="keyword">new</span> something(<span class="built_in">std</span>::forward&lt;Ts&gt;(params)...));</span><br><span class="line">    <span class="keyword">return</span> pInv;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 创建对象</span></span><br><span class="line"><span class="keyword">auto</span> pInvestment = makeInvestment(arguments); <span class="comment">// std::unique_ptr指针</span></span><br></pre></td></tr></table></figure><br>将一个裸指针赋给<code>std::unique_ptr</code>不会编译通过，因为裸指针到智能指针的隐式类型转换有问题，所以需要<code>reset</code>。在C++14中，自定义析构器可以定义在<code>makeInvestment</code>内部了。<br>对于自定义析构器：</p>
<ol>
<li>指定为<code>std::unique_ptr</code>的第二个实参。</li>
<li>接受Investment*的形参最后删除，等价于通过一个基类指针删除派生类对象，因此<code>Investment</code>要有虚析构函数。</li>
<li>若析构器为函数指针，则<code>std::unique_ptr</code>长度一般会增长一到两个字长。</li>
<li>若析构器为函数对象，则<code>std::unique_ptr</code>长度增长取决于函数对象中储存了多少状态。</li>
<li>无捕获的lambda表达式属于无状态的函数对象。</li>
</ol>
<p><code>std::unique_ptr</code>区分<code>std::unique_ptr&lt;T&gt;</code>和<code>std::unique_ptr&lt;T[]&gt;</code>，这种区分对指向的对象类型没有二义性。对单个对象没有<code>operator[]</code>，对数组形式没有<code>opeartor*</code>和<code>operator-&gt;</code>。</p>
<p><code>std::unique_ptr</code>可以很轻松转换成<code>std::shared_ptr</code>。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Investment&gt; sp = makeInvestment(arguments);</span><br></pre></td></tr></table></figure></p>
<h2 id="std-shared-ptr"><a href="#std-shared-ptr" class="headerlink" title="std::shared_ptr"></a>std::shared_ptr</h2><p><code>std::shared_ptr</code>可以通过访问某资源的引用计数来确定自己是否是最后一个指针，例如<code>sp1 = sp2</code>代表<code>sp2</code>引用计数递增，而<code>sp1</code>引用计数递减，递减为零就会释放。</p>
<ol>
<li><code>std::shared_ptr</code>尺寸是裸指针的两倍（指针资源的裸指针+指向引用计数的裸指针）</li>
<li>引用计数与资源关联，但是不知道对象是谁（内建类型也可以用<code>std::shared_ptr</code>）。需要动态分配，若由<code>std::make_ptr</code>分配，可以避免动态分配的成本。</li>
<li>引用计数的递增递减必须是原子操作，因为可能会有并发读写。</li>
<li>移动操作会把原指针置空，当前新指针不需要计数。只有复制操作会增加引用计数。</li>
<li>支持自定义析构器，但不是类型的一部分，析构器不同会影响<code>std::unique_ptr</code>但不会影响<code>std::shared_ptr</code>。</li>
<li><code>std::shared_ptr</code>的尺寸不会受到自定义析构器的影响。</li>
</ol>
<p>析构器可能是函数对象有更多数据，这时<code>std::shared_ptr</code>不得不使用更多内存，但这并不属于自身的一部分，而是把这些内存放在堆上。每个由<code>std::shared_ptr</code>管理的对象都有一个控制块。</p>
<p><img src="/img/share_ptr.png" alt="std::shared_ptr指针的内存示意图">在控制块上，如果自定义析构器被指定，就会包含一份它的复制。如果自定义内存分配器被指定，也会有一份复制。还包括很多附加数据。<br>控制块中的引用计数会跟踪有多少个<code>std::shared_ptr</code>指向该控制块，控制块还包含第二个引用计数，对<code>std::weak_ptr</code>进行计数（弱计数）。<code>std::weak_ptr</code>通过检查控制块内的引用计数来校验自己是否失效，假设引用计数为0，没有<code>std::shared_ptr</code>指向对象，对象已经被析构，则<code>std::weak_ptr</code>会失效，如果是使用<code>std::make_shared</code>创建的内存块，此时<code>std::shared_ptr</code>已经析构，但是<code>std::weak_ptr</code>依然存在并会指向到控制块（弱计数大于零），所以控制块会持续存在，包含它的内存也会持续存在。</p>
<p>一般情况，对象的控制块由首个创建指向它的<code>std::shared_ptr</code>的函数来确定。但是正在创建指向某对象的<code>std::shared_ptr</code>的函数是不知道是否由其他的<code>std::shared_ptr</code>已经指向了该对象的。因此：</p>
<ol>
<li><code>std::make_shared</code>总是创建一个控制块。</li>
<li>从<code>std::unique_ptr</code>或<code>std::auto_ptr</code>出发构造<code>std::shared_ptr</code>时，会创建一个控制块。</li>
<li>当<code>std::shared_ptr</code>构造函数使用裸指针作为实参来调用时，会创建一个控制块。</li>
<li>如果从已经有控制块的对象出发，传入<code>std::shared_ptr</code>或者<code>std::weak_ptr</code>就不会创建新的控制块。</li>
</ol>
<p>从同一个裸指针出发构造不止一个<code>std::shared_ptr</code>就会多重的控制块，多重的引用计数，也会多重的析构，Boom~！<br>但是可以这样写：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; <span class="title">spw</span><span class="params">(<span class="keyword">new</span> Widget, loggingDel)</span></span>; <span class="comment">// 直接new</span></span><br><span class="line"><span class="comment">// 容易出现上面错误的时this指针，例如：</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt;&gt; processWidgets;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Widget::procsee</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    processWidgets.emplace_back(<span class="keyword">this</span>); </span><br><span class="line">    <span class="comment">// this是裸指针，传入`std::shared_ptr`容器会创建内存块</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>不过也有解决方法：<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvJUU1JUE1JTg3JUU1JUJDJTgyJUU5JTgwJTkyJUU1JUJEJTkyJUU2JUE4JUExJUU2JTlEJUJGJUU2JUE4JUExJUU1JUJDJThG">The Curiously Recurring Template Pattern<i class="fa fa-external-link-alt"></i></span><br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>:</span> <span class="keyword">public</span> <span class="built_in">std</span>::enable_shared_form_this&lt;Widget&gt;&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">()</span></span>&#123;</span><br><span class="line">        processWidgets.emplace_back(shared_form_this()); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br><code>std::enable_shared_form_this&lt;T&gt;</code>是一个基类，它有一个成员函数是<code>shared_form_this()</code>会从this创建一个<code>std::shared_ptr</code>，这样的设计依赖于当前对象已有一个关联的<code>std::shared_ptr</code>控制块，否则该行为未定义，通常会<code>shared_form_this()</code>抛出异常。<br>为了解决这个顺序问题，继承自<code>std::enable_shared_form_this&lt;T&gt;</code>的类可以把构造函数声明为私有，只允许通过工厂函数创建对象。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>:</span> <span class="keyword">public</span> <span class="built_in">std</span>::enable_shared_form_this&lt;Widget&gt;&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; <span class="title">create</span><span class="params">(Ts&amp;&amp;... params)</span></span>;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 构造函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br>一个控制块通常只有几个字节，但自定义析构器和内存分配器可能会使其变得更大。控制块的实现原理涉及到继承，甚至还会有虚函数（仅在析构的时候使用一次），而进行一项引用计数需要一个或两个原子化操作，映射到单个机器指令，这些都是<code>std::shared_ptr</code>性能上的成本。<br>在使用一切默认+<code>std::shared_ptr</code>和<code>std::make_shared</code>时，控制块就三个字长，分配操作零成本。这就是C++动态分配资源，自动生存期管理的温和成本。<br>最后，不存在<code>std::shared_ptr&lt;T[]&gt;</code>，这和<code>std::unique_ptr</code>不同。</p>
<h2 id="std-weak-ptr"><a href="#std-weak-ptr" class="headerlink" title="std::weak_ptr"></a>std::weak_ptr</h2><p><code>std::weak_ptr</code>并不是一种独立的智能指针，而是<code>std::shared_ptr</code>的扩充。它可以像<code>std::shared_ptr</code>一样运作，同时不影响其指向对象的引用计数，而且要能跟踪指针何时悬空。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> spw = <span class="built_in">std</span>::make_shared&lt;Widget&gt;(); <span class="comment">// spw构造完成，Widget引用计数为1</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::weak_ptr&lt;Widget&gt; <span class="title">wpw</span><span class="params">(spw)</span></span>;        <span class="comment">// wpw和spw指向同一个widget，引用计数保持为1</span></span><br><span class="line">spw = <span class="literal">nullptr</span>;                         <span class="comment">// 引用计数0，widget被析构，wpw悬空</span></span><br></pre></td></tr></table></figure><br>关于<code>std::weak_ptr</code>的使用场景</p>
<ol>
<li><code>std::weak_ptr</code>是否失效的校验（为了线程安全需要原子操作），以及在未失效的条件下提供所指涉到的对象的访问<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; spw1 = wpw.lock(); <span class="comment">// 若wpw失效，则spw为空</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; <span class="title">spw2</span><span class="params">(wpw)</span></span>;         <span class="comment">// 若wpw失效，抛出std::bad_weak_ptr异常</span></span><br></pre></td></tr></table></figure></li>
<li>带缓存的工厂函数，缓存管理器<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> Widget&gt; <span class="title">fastLoadWidget</span><span class="params">(WidgetID id)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// C++11散列表容器，缓存</span></span><br><span class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::unorderd_map&lt;WidgetID, <span class="built_in">std</span>::weak_ptr&lt;<span class="keyword">const</span> Widget&gt;&gt; cache;</span><br><span class="line">    <span class="keyword">auto</span> objPtr = cache[id].lock(); <span class="comment">// 如果对象不在缓存中，返回空指针</span></span><br><span class="line">    <span class="keyword">if</span>(!objPtr)&#123;</span><br><span class="line">        objPtr = loadWidget(id);    <span class="comment">// 加载</span></span><br><span class="line">        cache[id] = objPtr;         <span class="comment">// 缓存</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> objPtr; <span class="comment">// 缓存中失效的std::weak_ptr会不断积累，可以优化</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>观察者模式——可以改变状态的对象，观察者（对象状态发生改变后通知的对象）。</li>
<li>避免<code>std::shared_ptr</code>的指针环路，如果A和B相互指向对方，这种环路会阻止析构，资源得不到回收。</li>
</ol>
<p>最后，<code>std::weak_ptr</code>和<code>std::shared_ptr</code>对象尺寸相同，也使用同样的控制块。</p>
<h2 id="std-make-unique和std-make-shared"><a href="#std-make-unique和std-make-shared" class="headerlink" title="std::make_unique和std::make_shared"></a>std::make_unique和std::make_shared</h2><p><code>std::make_shared</code>来自C++11，<code>std::make_unique</code>来自C++14，不过可以用C++11简易实现，参考<span class="exturl" data-url="aHR0cHM6Ly9pc29jcHAub3JnL2Jsb2cvMjAxMy8wNC9uMzY1Ni1tYWtlLXVuaXF1ZS1yZXZpc2lvbi0x">创建make_unique, N3656, Stephan T.Lavavej, 2013-4-18<i class="fa fa-external-link-alt"></i></span><br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这个简易实现不支持数组和自定义析构器</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;T&gt; <span class="title">make_unique</span><span class="params">(Ts&amp;&amp;... params)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;T&gt;(<span class="keyword">new</span> T(<span class="built_in">std</span>::forward&lt;Ts&gt;(params)...));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>make系列函数会把一个任意实参集合完美转发给动态分配内存对象的构造函数，并返回一个指向该对象的智能指针，分别是<code>std::make_unique</code>、<code>std::make_shared</code>、<code>std::allocate_shared</code>（动态分配器）。</p>
<p>优先使用make的原因之一与异常安全有关，例如：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">processWidget</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; spw, <span class="keyword">int</span> priority)</span></span>;</span><br><span class="line">processWidget(<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt;(<span class="keyword">new</span> Widget), computePriority()); <span class="comment">// 潜在的资源泄露</span></span><br><span class="line">processWidget(<span class="built_in">std</span>::make_shared&lt;Widget&gt;(), computePriority()); <span class="comment">// 安全</span></span><br></pre></td></tr></table></figure><br>这里的风险来自编译器从源代码到目标代码的翻译过程，在运行时，传递给函数的实参必须在函数调用被发起之前完成评估求值。因此在这里：</p>
<ol>
<li>表达式<code>new Widget</code>必须完成评估求值，在堆上创建</li>
<li>由new产生的裸指针的托管对象<code>std::shared_ptr&lt;Widget&gt;</code>的构造函数必须执行。</li>
<li><code>computeProprity()</code>必须运行</li>
</ol>
<p>但是编译器不必要按照这个顺序生成代码，最糟糕的情况是先运行new后运行<code>computeProprity</code>最后运行<code>std::shared_ptr</code>构造函数，这样如果<code>computeProprity</code>产生异常，第一步创建的new永远不会被储存到第三步才接管的<code>std::shared_ptr</code>，但是使用<code>std::make_shared</code>就没有这个问题。</p>
<p>优先使用make的原因之二是性能提升<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; <span class="title">spw</span><span class="params">(<span class="keyword">new</span> Widget)</span></span>;   <span class="comment">// 1</span></span><br><span class="line"><span class="keyword">auto</span> spw = <span class="built_in">std</span>::make_shared_ptr&lt;Widget&gt;(); <span class="comment">// 2</span></span><br></pre></td></tr></table></figure><br>情况1会多一次内存分配，第一次分配是new分配，第二次是<code>std::share_ptr</code>的构造函数对控制块的分配。<br>情况2只有1次内存分配，对象+控制块，会分配在一个同一块内存上（单块内存）。</p>
<p>但是make函数有许多限制：</p>
<ol>
<li>不允许使用自定义析构器，只能用构造函数实现。<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> widgetDeleter = [](Wiget* pw)&#123;...&#125;;</span><br><span class="line">std::unique_ptr&lt;Widget, decltype(widgetDeleter)&gt; upw(new Widget, widgetDleter);</span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; <span class="title">spw</span><span class="params">(<span class="keyword">new</span> Widget, widgetDeleter)</span></span>;</span><br></pre></td></tr></table></figure></li>
<li>make函数对形参进行完美转发使用的是圆括号，大括号会优先匹配<code>std::initializer_list</code>类型的构造函数，因此假如要使用大括号初始化就必须使用new了。不能够完美转发大括号初始化物，但是可以尝试auto推导创建一个<code>std::initializer_list</code>对象。<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> initList = &#123;<span class="number">10</span>,<span class="number">20</span>&#125;;</span><br><span class="line"><span class="keyword">auto</span> spv = <span class="built_in">std</span>::make_shared&lt;<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;(initList);</span><br></pre></td></tr></table></figure>
对<code>std::unique</code>而言，仅在上面两种情景下会存在问题。而对<code>std::shared_ptr</code>和其他make函数而言，还有其他两种更边缘的场景：</li>
<li>有些类会定义自身版本的<code>operator new</code>和<code>operator delete</code>，全局版本的内存分配策略不适用于这些类。通常，类自定义的这两种函数被设计成仅用来分配和是犯法该类精确尺寸的内存块，就不适于用<code>std::shared_ptr</code>所用的自定义分配器（通过<code>std::allocate_shared</code>）和自定义析构器了。因为<code>std::allocate_shared</code>所需要的内存并不等于动态分配对象的尺寸，所以这种情况推荐new。</li>
<li>使用<code>std::make_shared</code>创建的内存块，此时<code>std::shared_ptr</code>已经析构，但是<code>std::weak_ptr</code>依然存在并会指向到控制块（弱计数大于零），所以控制块会持续存在，包含它的内存也会持续存在。这么一来，假设对象的尺寸很大，且最后一个<code>std::shared_ptr</code>和<code>std::weak_ptr</code>析构之间的时间间隔不能忽略，在对象的析构和内存的释放之间就会产生延迟。<br>如果是使用new表达式，则对象内存可以在最后一个指向它的<code>std::shared_ptr</code>析构时就释放。而使用new表达式时，要避开之前提到的异常安全问题。<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">processWidget</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; spw, <span class="keyword">int</span> priority)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cusDel</span><span class="params">(Widget *ptr)</span></span>;  <span class="comment">// 自定义析构器</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Widget&gt; <span class="title">spw</span><span class="params">(<span class="keyword">new</span> Widget, cusDel)</span></span>;</span><br><span class="line">processWidget(<span class="built_in">std</span>::move(spw), computePriority());  <span class="comment">// std::move 右值传递，节省开销</span></span><br></pre></td></tr></table></figure>
<h2 id="使用Pimpl习惯用法"><a href="#使用Pimpl习惯用法" class="headerlink" title="使用Pimpl习惯用法"></a>使用Pimpl习惯用法</h2>Pimpl习惯用法是一种可以在类实现和类使用之间减少编译依赖性的方法。<br>对采用<code>std::unique_ptr</code>来实现的Pimpl指针，需在头文件中声明特种成员函数，但在实现文件中实现他们，即使默认函数实现有着正确的行为，必须要这样做，这对<code>std::shared_ptr</code>并不适用。</li>
</ol>
<h1 id="移动语义"><a href="#移动语义" class="headerlink" title="移动语义"></a>移动语义</h1><hr>
<p>右值引用是把移动语义和完美转发两种语言特性粘合的底层语言机制。一开始看山是山，看水是水；了解的越多，看山不是山，看水不是水，最后山还是山，水还是水。</p>
<h2 id="std-move和std-forward"><a href="#std-move和std-forward" class="headerlink" title="std::move和std::forward"></a>std::move和std::forward</h2><p>这两者在运行期都无所作为，不会生成任何可执行代码。<code>std::move</code>并不进行任何移动，而是把实参强制转换成右值。在一个对象上实施<code>std::move</code>是告诉编译器这个对象具备可移动的属性。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// C++11中std::move的示例实现</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">typename</span> remove_reference&lt;T&gt;::type&amp;&amp;  <span class="comment">// 确保返回右值引用</span></span><br><span class="line">move(T&amp;&amp; param)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">using</span> ReturnType =</span><br><span class="line">        <span class="keyword">typename</span> remove_reference&lt;T&gt;::type&amp;&amp;;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;ReturnType&gt;(param);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// C++14中std::move的示例实现</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">decltype</span>(<span class="keyword">auto</span>) <span class="title">move</span><span class="params">(T&amp;&amp; param)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">using</span> ReturnType = <span class="keyword">remove_reference_t</span>&lt;T&gt;&amp;&amp;;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">static_cast</span>&lt;ReturnType&gt;(param);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>移动操作不能违反维持常量正确性的原则，所以不允许常量对象进行移动。如果想取得某个对象执行移动操作的能力，不要将其声明为常量。考虑以下情况：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">string</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">string</span>(<span class="keyword">const</span> <span class="built_in">string</span>&amp; rhs);  <span class="comment">// 无法std::move</span></span><br><span class="line">    <span class="built_in">string</span>(<span class="built_in">string</span>&amp;&amp; rhs);       <span class="comment">// 只能接受非常量</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><code>std::move</code>无条件将实参强制转换为右值，而std::forward则仅在某个特定条件满足时才执行同一个强制转换。<br><code>std::forward</code>最常见的一个使用场景是某个函数模板有万能引用的形参，随后将其传递给另一个函数。因为一切函数形参皆左值，所以为了避免这种结果，就需要一种机制，左值保持不变，传递左值；右值传递右值。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(<span class="keyword">const</span> Widget&amp; lvalArg)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(Widget&amp;&amp; rvalArg)</span></span>;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">logAndProcess</span><span class="params">(T&amp;&amp; param)</span> </span>&#123; process(<span class="built_in">std</span>::forward&lt;T&gt;(param)); &#125;</span><br></pre></td></tr></table></figure><br>这里，<code>std::forward</code>可以分辨出param是通过左值还是右值完成初始化的，该信息被编码到模板形参T中。详细参见“引用折叠”。<br><code>std::move</code>只取用一个实参，而<code>std::forward</code>需要同时取用类型+实参，两者的含义也有很大的不同。</p>
<h2 id="万能引用和右值引用"><a href="#万能引用和右值引用" class="headerlink" title="万能引用和右值引用"></a>万能引用和右值引用</h2><p>万能引用可以绑定到右值引用、左值引用、const、volatile，一般指出现在函数模板的形参和auto声明，也就是说需要涉及到类型推导才行。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 万用引用</span></span><br><span class="line"><span class="keyword">auto</span>&amp;&amp; var2 = var1;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt; <span class="keyword">void</span> <span class="title">f</span><span class="params">(T&amp;&amp; param)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;class... Args&gt; <span class="keyword">void</span> <span class="title">emplace_back</span><span class="params">(Args&amp;&amp;... args)</span></span>;</span><br><span class="line"><span class="keyword">auto</span> timeFuncInvocation =</span><br><span class="line">    [](<span class="keyword">auto</span>&amp;&amp; func, <span class="keyword">auto</span>&amp;&amp;... params)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::forward&lt;<span class="keyword">decltype</span>(func)&gt;(func)(           <span class="comment">// func</span></span><br><span class="line">        <span class="built_in">std</span>::forward&lt;<span class="keyword">decltype</span>(params)&gt;(params)..  <span class="comment">// params</span></span><br><span class="line">    );</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 右值引用</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt; <span class="keyword">void</span> <span class="title">f</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;T&gt;&amp;&amp; param)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt; <span class="keyword">void</span> <span class="title">f</span><span class="params">(cosnt T&amp;&amp; param)</span></span>;</span><br></pre></td></tr></table></figure></p>
<h2 id="对右值引用实施std-move，对万能引用实施std-forward"><a href="#对右值引用实施std-move，对万能引用实施std-forward" class="headerlink" title="对右值引用实施std::move，对万能引用实施std::forward"></a>对右值引用实施std::move，对万能引用实施std::forward</h2><p>右值引用仅会绑定到可供移动的对象上，所以需要<code>std::move</code>把对象转换为右值。万能引用只有在使用右值初始化才会是右值，对应<code>std::forward</code>。如果对万能引用施加<code>std::move</code>就可能有问题：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="function"><span class="keyword">void</span> <span class="title">setName</span><span class="params">(T&amp;&amp; newName)</span> </span>&#123;name = <span class="built_in">std</span>::move(newName);&#125;</span><br><span class="line">&#125;;</span><br><span class="line">w.setName(n);  <span class="comment">// n的值移入了w，n的值未知</span></span><br></pre></td></tr></table></figure><br>如果不使用万能引用，分别写成两个函数可能会遇到效率问题。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setName</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; newName)</span></span>&#123; name = newName; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setName</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span>&amp;&amp; newName)</span></span>&#123; name = <span class="built_in">std</span>::move(newName); &#125;</span><br><span class="line">&#125;;</span><br><span class="line">w.setName(<span class="string">&quot;Adela Novak&quot;</span>); <span class="comment">// 对这个调用，重载版本会比万能引用多创建一次临时对象用来传参</span></span><br></pre></td></tr></table></figure><br>分成函数写，当参数变多甚至可变形参时，就显得不太现实。因此，万能引用+<code>std::forward</code>是解决问题的唯一方法。<br>有些情况，在单一函数内一个对象会不止一次地绑定到右值引用或万能引用，这时仅在最后一次使用引用即可。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 对矩阵加法</span></span><br><span class="line">Matrix opeartor+(MAtrix&amp;&amp; lhs, <span class="keyword">const</span> Matrix&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    lhs += rhs;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::move(lhs);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 约分</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">Function <span class="title">reduceAndCopy</span><span class="params">(T&amp;&amp; frac)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    frac.reduce();</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::forward&lt;T&gt;(frac);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>RVO（return value optimization）<br>编译器对函数返回值自带优化，需要满足两个条件：</p>
<ol>
<li>局部对象类型和函数返回值类型相同。</li>
<li>返回的就是局部对象本身。<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">Widget <span class="title">makeWidget</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Widget w;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::move(w);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
这里返回的不是局部对象，而是局部对象的引用，因此编译器无法实施<code>RVO</code>，但编译器不选择执行<code>RVO</code>的时候，返回对象必须作为右值处理。即要么发生复制忽略，要么<code>std::move</code>被隐式实施于返回的局部对象。</li>
</ol>
<h2 id="万能引用的重载"><a href="#万能引用的重载" class="headerlink" title="万能引用的重载"></a>万能引用的重载</h2><p>一旦万能引用作为重载候选，它就会吸走大批的实参类型，完美转发构造函数尤其严重，因为对于非常量的左值类型，它们一般都会形成相对于复制构造函数的更加匹配，并且还会劫持派生类中对基类的复制和移动构造函数的调用。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">template</span>&lt;tyepname T&gt; <span class="keyword">explicit</span> <span class="title">Person</span><span class="params">(T&amp;&amp; n)</span>: <span class="title">name</span><span class="params">(<span class="built_in">std</span>::forward&lt;T&gt;(n))</span></span>&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpecialPerson</span>:</span> <span class="keyword">public</span> Person&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 都是调用基类的完美转发构造函数</span></span><br><span class="line">    SpecialPerson(<span class="keyword">const</span> SpecialPerson&amp; rhs): Person(rhs) &#123;...&#125;</span><br><span class="line">    SpecialPerson(SpecialPerson&amp;&amp; rhs): Person(<span class="built_in">std</span>::move(rhs) &#123;...&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br>为了解决这些问题，要么就直接弃用重载，要么通过区分传递<code>const T&amp;</code>，要么通过传值操作（把按引用传递换成按值传递，尽管这反直觉，当知道肯定需要复制形参时，考虑按值传递）</p>
<p>兼顾特性的做法是标签分派，以下是示例：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;<span class="function"><span class="keyword">void</span> <span class="title">logAndAdd</span><span class="params">(T&amp;&amp; name)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    logAndAddImpl(</span><br><span class="line">        <span class="built_in">std</span>::forward&lt;T&gt;(name),</span><br><span class="line">        <span class="built_in">std</span>::is_intergal&lt;<span class="keyword">typename</span> <span class="built_in">std</span>::remove_reference&lt;T&gt;::type()</span><br><span class="line">    );</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">logAndAddImpl</span><span class="params">(T&amp;&amp; name, <span class="built_in">std</span>::false_type)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> now = <span class="built_in">std</span>::chrono::system_clock::now();</span><br><span class="line">    <span class="built_in">log</span>(now, <span class="string">&quot;logAndAdd&quot;</span>);</span><br><span class="line">    names.emplace(<span class="built_in">std</span>::forward&lt;T&gt;(name));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">nameFormIdx</span><span class="params">(<span class="keyword">int</span> idx)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">logAndAddImpl</span><span class="params">(<span class="keyword">int</span> idx, <span class="built_in">std</span>::true_type)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    logAndAdd(nameFromIdx(idx));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>标签分派的思想是这样的，如果万能引用仅是形参列表的一部分，列表中还有其他非万能引用的形参，那么只要这个非万能引用不匹配，这个重载函数就不会匹配。<br>通用的做法是重写万能引用的函数，函数内部再委托给另外两个函数，比如上面用<code>std::is_intergal</code>区分整型or非整型，因为推导以及引用类型int&amp;不是int，所以还要加上<code>std::remove_reference</code>。然后<code>std::is_intergal</code>会得到<code>std::true_type</code>和<code>std::false_type</code>两种结果。<br>true和false都是运行期的值，这里我们需要利用的是重载决议（处于编译期）来选择正确的重载版本，所以需要<code>std::true_type</code>和<code>std::false_type</code>。这就是所谓的“<code>标签</code>”。<br>标签分派能够发挥作用的关键在于，存在一个单版本函数作为API，该函数会把待完成的任务分派到实现函数，创建无重载的分派函数并不难。但是这解决不了劫持派生类中对基类的复制和移动构造函数的调用的问题。</p>
<p><code>std::enable_if</code>可以强制编译器禁用模板，默认时所有的模板都是启用的，但是施加了<code>std::enable_if</code>的模板只会在满足了<code>std::enable_if</code>指定的条件才会启用，更深入的可以参考：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8yMTMxNDcwOA==">C++SHINAE机制 — 知乎<i class="fa fa-external-link-alt"></i></span><br>以下是一个示例：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">template</span>&lt;</span><br><span class="line">        <span class="keyword">typename</span> T,</span><br><span class="line">        <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">enable_if_t</span>&lt;</span><br><span class="line">        !<span class="built_in">std</span>::is_base_of&lt;Person, <span class="built_in">std</span>::<span class="keyword">decay_t</span>&lt;T&gt;&gt;::value</span><br><span class="line">        &amp;&amp;</span><br><span class="line">        <span class="built_in">std</span>::is_integral&lt;<span class="built_in">std</span>::<span class="keyword">remove_reference_t</span>&lt;T&gt;&gt;::value</span><br><span class="line">        &gt;</span><br><span class="line">    &gt;</span><br><span class="line">    <span class="keyword">explicit</span> Person(T&amp;&amp; n): name(<span class="built_in">std</span>::forward&lt;T&gt;(n))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">static_assert</span>(</span><br><span class="line">            <span class="built_in">std</span>::is_constructible&lt;<span class="built_in">std</span>::<span class="built_in">string</span>, T&gt;::value,</span><br><span class="line">            <span class="string">&quot;Parameter n can&#x27;t be used to construct a std::string&quot;</span></span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><br>这里想指定的条件是T不是Person类型时才启用这个模板函数，通过<code>!std::is_same&lt;Person,T&gt;::value</code>，深入思考，在得到T时，要移除它是否为一个引用（这个简单），也要移除它是否带有const或volatile，这时需要使用<code>std::decay&lt;T&gt;::type</code>或者<code>std::decay_t&lt;T&gt;(c++14)</code>，这么一来就成了<code>!std::is_same&lt;Person,typename std::decay&lt;T&gt;::type&gt;::value</code>。<br>在最开始的例子，派生类会给基类的构造函数传递对象，因为派生类和基类不同，所以这里的构造函数仍会被启用。<code>std::is_same</code>要换成<code>std::is_base_of</code>，来判断是否类型有继承。</p>
<p>万能引用转发次数越多，某些地方出错时给出的错误信息就越难懂。可以通过写一些断言来缓解这个问题。参考：<span class="exturl" data-url="aHR0cHM6Ly96aC5jcHByZWZlcmVuY2UuY29tL3cvY3BwL2hlYWRlci90eXBlX3RyYWl0cw==">type_traits<i class="fa fa-external-link-alt"></i></span></p>
<h2 id="引用折叠"><a href="#引用折叠" class="headerlink" title="引用折叠"></a>引用折叠</h2><p>当初始化形参为万能引用时，实参传递给函数模板时，推导出来的模板形参会将实参时左值还是右值的信息编码到结果类型中。<br>如果传递的实参是个左值，T推到结果为左值引用。<br>如果传递的实参是个右值，T推导结果是个非引用。<br>因为C++禁用引用的引用，所以折叠。</p>
<blockquote>
<p>A&amp; &amp; 变成 A&amp;<br>A&amp; &amp;&amp; 变成 A&amp;<br>A&amp;&amp; &amp; 变成 A&amp;<br>A&amp;&amp; &amp;&amp; 变成 A&amp;&amp;</p>
</blockquote>
<p>引用折叠会在四种语境中发生：模板实例化、auto类型推导、创建和运用typedef和别名声明、decltype。<br>万能引用并不是新的引用，而是满足条件的右值引用：</p>
<ol>
<li>类型推导会区别左值和右值。</li>
<li>会发生引用折叠。</li>
</ol>
<h2 id="假定移动操作不存在、成本高、未使用"><a href="#假定移动操作不存在、成本高、未使用" class="headerlink" title="假定移动操作不存在、成本高、未使用"></a>假定移动操作不存在、成本高、未使用</h2><p>C++98的代码原封不动地在C++11编译器上编译，也会有性能优化。但是有很多场景移动操作并不高效。在这几个场景，C++11移动语义不会带来任何好处：</p>
<ol>
<li>std::array是STL数组，数据直接存在对象上而不是堆上。</li>
<li>std::string有SSO（small string optimization），即小型字符串会储存在缓存区而不是堆上。</li>
<li>一些看似万无一失的移动场景，没有加上noexcept的话，编译器会强制调用复制。要求移动不可发射异常，必须加上noexcept声明。</li>
<li>没有移动操作，移动请求就变成了复制请求。</li>
<li>移动还不如复制更快。</li>
<li>原对象是个左值，除了极少数例外，只有右值可以移动。</li>
</ol>
<h2 id="完美转发的失败情况"><a href="#完美转发的失败情况" class="headerlink" title="完美转发的失败情况"></a>完美转发的失败情况</h2><p>完美转发的失败，是源自模板推导的失败，或者推导结果错误。会导致完美转发失败的实参种类有<code>大括号初始化物</code>、<code>以0或NULL表达的空指针</code>、<code>仅有声明的整型static const成员变量、模板或重载函数名字</code>、以及<code>位域</code>。</p>
<p>完美转发不仅转发对象、还会转发类型、左值右值、是否嗲有const、volatile。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;<span class="function"><span class="keyword">void</span> <span class="title">fwd</span><span class="params">(Ts&amp;&amp;... params)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    f(<span class="built_in">std</span>::forward&lt;Ts&gt;(param)...);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>常规情况下，编译器先得到调用端的实参类型，再得到f所声明的形参类型，比较两者是否兼容，（之后通过隐式类型转换）来调用成功。而经由完美转发，编译器采用推导的手法得到调用端的实参类型与所声明的形参类型比较，会在以下任何一种情况成立时失败：</p>
<ol>
<li>编译器无法为一个或多个fwd的形参推导出结果。</li>
<li>编译器为一个或多个fwd的形参推导出了“错误”的结果。</li>
</ol>
<p><code>大括号初始化物</code>的问题在于向未声明<code>std::initializer_list</code>类型的函数模板传递了大括号，叫作“非推导语境”，所以会被编译器禁止。但是可以先用<code>auto</code>推导，然后传递给完美转发函数。</p>
<p>若尝试把0或NULL传给模板，类型推导的结果就是整型，传递<code>nullptr</code>即可。</p>
<p><code>static cosnt</code>成员变量仅需声明，不必保留内存。一般调用直接当作常数处理，完美转发会失败，因为隐含了取地址，毕竟引用和指针实现类似。</p>
<p>重载的函数名字和模板名字，因为没有任何关于类型的信息，编译器不知道应该传递哪个版本。</p>
<p>位域<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">IPv4Header</span> &#123;</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="keyword">uint32_t</span> version:<span class="number">4</span>,</span><br><span class="line">                  IHL:<span class="number">4</span>,</span><br><span class="line">                  DSCP:<span class="number">6</span>,</span><br><span class="line">                  ECN:<span class="number">2</span>,</span><br><span class="line">                  totalLength:<span class="number">16</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 可以这么做</span></span><br><span class="line">IPv4Header h;</span><br><span class="line"><span class="keyword">auto</span> length = <span class="keyword">static_cast</span>&lt;<span class="built_in">std</span>::<span class="keyword">uint16_t</span>&gt;(h.totalLength);</span><br><span class="line">fwd(length);</span><br></pre></td></tr></table></figure><br>C++标准禁止非const引用绑定到位域。位域是由机器字的若干任意部分组成的，没办法对其直接去地址。指针指向的最小实体是一个字节。</p>
<h1 id="lambda表达式"><a href="#lambda表达式" class="headerlink" title="lambda表达式"></a>lambda表达式</h1><hr>
<p>lambda是表达式的一种，闭包是lambda式创建的运行期对象，根据不同的捕获模式，闭包会持有数据的副本或引用。<br>闭包类就是实例化闭包的类，每个lambda都会触发编译器生成独一无二的闭包类，而闭包中的语句会变成成员函数可执行语句。</p>
<h2 id="避免默认捕获模式"><a href="#避免默认捕获模式" class="headerlink" title="避免默认捕获模式"></a>避免默认捕获模式</h2><p>C++11有两种默认捕获方式：按引用或按值。</p>
<p>按引用的默认捕获模式可能会导致空悬引用，一旦lambda式所创建的闭包越过了生存周期，引用就会空悬。该局部变量或形参<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> divisor = computeDivisor();</span><br><span class="line">filters.emplace_back([&amp;divisor](<span class="keyword">int</span> value) &#123; <span class="keyword">return</span> value % divisor == <span class="number">0</span>; &#125; );</span><br></pre></td></tr></table></figure><br>闭包会被立即使用（例如STL算法）并且不会被复制的场景，引用比原对象的生命期更长就不存在风险。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="built_in">std</span>::all_of(<span class="built_in">std</span>::begin(container), <span class="built_in">std</span>::end(container),</span><br><span class="line">    [&amp;](<span class="keyword">const</span> ContElemT&amp; value)&#123;<span class="keyword">return</span> value % divisor == <span class="number">0</span>;&#125;))</span><br><span class="line"></span><br><span class="line"><span class="comment">// c++14 已经可以用auto了 </span></span><br><span class="line"><span class="keyword">if</span>(<span class="built_in">std</span>::all_of(<span class="built_in">std</span>::begin(container), <span class="built_in">std</span>::end(container),</span><br><span class="line">    [&amp;](<span class="keyword">auto</span>&amp; value)&#123;<span class="keyword">return</span> value % divisor == <span class="number">0</span>;&#125;))</span><br></pre></td></tr></table></figure><br>另一种是按值<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">filters.emplace_back([=](<span class="keyword">int</span> value) &#123; <span class="keyword">return</span> value % divisor == <span class="number">0</span>; &#125; );</span><br></pre></td></tr></table></figure><br>按值并不能避免空悬，问题在于经过复制，闭包中得到是副本，如果是指针什么的还是可能空悬的。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addFilter</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        filters.emplace_back([=](<span class="keyword">int</span> value) &#123; <span class="keyword">return</span> value % divisor == <span class="number">0</span>; &#125; );</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> divisor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>lambda式只能捕获作用域内可见的非静态局部变量和形参，以上代码无法编译通过。这里，lambda捕获的其实是this指针，lambda闭包的存活与它含有其this指针副本的对象的生命期式绑定的。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> FilterContainer = <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::function&lt;<span class="keyword">bool</span>&gt;&gt;;</span><br><span class="line">FilterContainer filters;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">doSomeWork</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> pw = <span class="built_in">std</span>::make_unique&lt;Widget&gt;();</span><br><span class="line">    pw-&gt;addFilter();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Widget::addFilter</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> divisorCopy = divisor;</span><br><span class="line">    filters.emplace_back([divisorCopy](<span class="keyword">int</span> value)&#123;<span class="keyword">return</span> value % divisorCopy == <span class="number">0</span>;&#125;);</span><br><span class="line">    filters.emplace_back([=](<span class="keyword">int</span> value)&#123;<span class="keyword">return</span> value % divisorCopy == <span class="number">0</span>;&#125;); <span class="comment">// 这样也行</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// c++14，lambda广义捕获</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Widget::addFilter</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> divisorCopy = divisor;</span><br><span class="line">    filters.emplace_back(</span><br><span class="line">        [divisor = divisor](<span class="keyword">int</span> value)       <span class="comment">// 将divisor复制入闭包</span></span><br><span class="line">        &#123;<span class="keyword">return</span> value % divisorCopy == <span class="number">0</span>; &#125;</span><br><span class="line">    );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>使用默认值捕获另一缺点是，给人感觉lambda与闭包外数据绝缘，但其实并不是。除了依赖作用域内可见的非静态局部变量和形参，其实还会依赖静态存储期对象，这样的对象定义在全局或命名空间作用域中，又或在类中、在函数中、在文件中以static声明，这些玩意儿都不能被捕获。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">addDivisorFilter</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">auto</span> calc1 = computeSomeValue1();</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">auto</span> calc2 = computeSomeValue2();</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">auto</span> divisor = computeDivisor(calc1, calc2);</span><br><span class="line">    filters.emplace_back(</span><br><span class="line">        [=](<span class="keyword">int</span> value)                    <span class="comment">// 没有捕获到任何东西，看上去是按值，其实是按引用</span></span><br><span class="line">        &#123; <span class="keyword">return</span> value % divisor == <span class="number">0</span>; &#125;  <span class="comment">// 指涉到static对象</span></span><br><span class="line">    );</span><br><span class="line">    ++divisor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="使用初始化捕获将对象移入闭包"><a href="#使用初始化捕获将对象移入闭包" class="headerlink" title="使用初始化捕获将对象移入闭包"></a>使用初始化捕获将对象移入闭包</h2><p>C++11没有办法移动对象到闭包，C++14则有云泥之别，即通过初始化捕获来弥补C++11移动捕获的缺失。这样就可以在lambda使用只移对象以及大部分的标准库（移动廉价、复制昂贵）。<br>使用初始化捕获，可以得到机会指定：</p>
<ol>
<li>由lambda生成的闭包类中的成员变量的名字。</li>
<li>一个表达式，用以初始化该成员变量。</li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isValidated</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isArchived</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">auto</span> pw = <span class="built_in">std</span>::make_unique&lt;Widget&gt;();</span><br><span class="line"><span class="keyword">auto</span> func = [pw = <span class="built_in">std</span>::move(pw)]       <span class="comment">// &quot;=&quot;左右，即初始化捕获</span></span><br><span class="line">            &#123; <span class="keyword">return</span> pw-&gt;isValidated()</span><br><span class="line">                  &amp;&amp; pw-&gt;isArchived(); &#125;;</span><br><span class="line"><span class="comment">// 这样也可以</span></span><br><span class="line"><span class="keyword">auto</span> func = [pw = <span class="built_in">std</span>::make_unique&lt;Widget&gt;()] </span><br><span class="line">            &#123; <span class="keyword">return</span> pw-&gt;isValidated()</span><br><span class="line">                  &amp;&amp; pw-&gt;isArchived(); &#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以上，初始化捕获也就是广义lambda捕获（<code>generalized lambda capture</code>）。<br>假如编译器只支持到C++11，多敲键盘也能达到目的，以下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IsValAndArch</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> DataType = <span class="built_in">std</span>::<span class="built_in">unique_ptr</span>&lt;Widget&gt;;</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">isValAndArch</span><span class="params">(DataType&amp;&amp; ptr)</span>: <span class="title">pw</span><span class="params">(<span class="built_in">std</span>::move(ptr))</span></span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">()</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">    </span>&#123; <span class="keyword">return</span> pw-&gt;isValidated() &amp;&amp; pw-&gt;isArchisved(); &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    DataType pw;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">auto</span> func = IsValAndArch(<span class="built_in">std</span>::make_unique&lt;Widget&gt;());</span><br></pre></td></tr></table></figure><br>如果非要使用lambda式，按移动捕获可以在C++11中模拟做到：</p>
<ol>
<li>把需要捕获的对象移动到std::bind产生的函数对象中。</li>
<li>给到lambda式一个指向欲“捕获”的对象的引用。</li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// c++14</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; data;</span><br><span class="line"><span class="keyword">auto</span> func = [data = <span class="built_in">std</span>::move(data)]&#123; <span class="comment">/*对数据加以运用*/</span> &#125;;</span><br><span class="line"><span class="comment">// c++11</span></span><br><span class="line"><span class="keyword">auto</span> func = <span class="built_in">std</span>::bind(</span><br><span class="line">    [](<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt;&amp; data)</span><br><span class="line">    &#123; <span class="comment">/*对数据加以运用*/</span> &#125;,</span><br><span class="line">    <span class="built_in">std</span>::move(data)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>和lambda式类似，<code>std::bind</code>返回函数对象并成为绑定对象（bind ojbect）。<code>std::bind</code>的第一个实参是一个可调用对象，接下来所有的实参都表示传给该对象的值。绑定对象含有传递给<code>std::bind</code>所有实参的副本，对于每个左值实参都会复制，每个右值实参都会移动。<br>默认情况下，lambda生成的闭包类中的operator()会带有const，结果闭包里的所有成员变量在lambda式的函数体内都会带有const。但是绑定对象里移动构造得到的data副本并不带有const，所以为了防止该data副本在lambda内被意外修改，形参需要为const T。但如果lambda声明带有mutable，闭包里的operator()就不会带const了，相应的形参应该略去const：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> func = <span class="built_in">std</span>::bind(</span><br><span class="line">    [](<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt;&amp; data) <span class="keyword">mutable</span></span><br><span class="line">    &#123; <span class="comment">/*对数据加以运用*/</span> &#125;,</span><br><span class="line">    <span class="built_in">std</span>::move(data)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><br>使用std::bind模拟移动捕获，再举一例：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// c++14</span></span><br><span class="line"><span class="keyword">auto</span> func = [pw = <span class="built_in">std</span>::make_unique&lt;Widget&gt;()]</span><br><span class="line">            &#123; <span class="keyword">return</span> pw-&gt;isValidated() &amp;&amp; pw-&gt;isArchived(); &#125;</span><br><span class="line"><span class="comment">// c++11</span></span><br><span class="line"><span class="keyword">auto</span> func = <span class="built_in">std</span>::bind([](<span class="keyword">const</span> <span class="built_in">std</span>::unqiue_ptr&lt;Widget&gt;&amp; pw)</span><br><span class="line">                      &#123; <span class="keyword">return</span> pw-&gt;isValidated() &amp;&amp; pw-&gt;isArchived(); &#125;,</span><br><span class="line">                      <span class="built_in">std</span>::make_unique&lt;Widget&gt;()</span><br><span class="line">                     );</span><br></pre></td></tr></table></figure></p>
<h2 id="对auto-amp-amp-形参使用dectltype"><a href="#对auto-amp-amp-形参使用dectltype" class="headerlink" title="对auto&amp;&amp;形参使用dectltype"></a>对auto&amp;&amp;形参使用dectltype</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> f = [](<span class="keyword">auto</span> x)&#123; <span class="keyword">return</span> func(normalize(x)); &#125;;</span><br><span class="line"><span class="comment">// 对应的闭包类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SomeCompilerGeneratedClassName</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function"><span class="keyword">auto</span> <span class="title">operator</span><span class="params">()</span><span class="params">(T x)</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> func(normalize(x)); &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这里应该把x完美转发给normalize()，但是泛型lambda式却没有可用的T可以用。改进后的代码：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> f = [](<span class="keyword">auto</span>&amp;&amp; param)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> func(normalize(<span class="built_in">std</span>::forward&lt;<span class="keyword">decltype</span>(param)&gt;(param)));</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">auto</span> f = [](<span class="keyword">auto</span>&amp;&amp;... params)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> func(normalize(<span class="built_in">std</span>::forward&lt;<span class="keyword">decltype</span>(params)&gt;(params)...));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="std-bind"><a href="#std-bind" class="headerlink" title="std::bind"></a>std::bind</h2><p><span class="exturl" data-url="aHR0cHM6Ly96aC5jcHByZWZlcmVuY2UuY29tL3cvY3BwL3V0aWxpdHkvZnVuY3Rpb25hbC9iaW5k">std::bind<i class="fa fa-external-link-alt"></i></span>是C++98中<code>std::bind1st</code>和<code>std::bind2nd</code>的后继特性。作为非标准特性，在05年就成为标准库的组成部分（那时标准委员会刚接受了<span class="exturl" data-url="aHR0cHM6Ly96aC53aWtpcGVkaWEub3JnL3dpa2kvQyUyQiUyQl9UZWNobmljYWxfUmVwb3J0XzE=">C++ Technical Report 1 (TR1)文档<i class="fa fa-external-link-alt"></i></span>）。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> Time = <span class="built_in">std</span>::chrono::steady_clock::time_point;</span><br><span class="line">emum <span class="class"><span class="keyword">class</span> <span class="title">Sound</span> &#123;</span>Beep, Siren, Whisstle&#125;;</span><br><span class="line"><span class="keyword">using</span> Duration = <span class="built_in">std</span>::chrono::steady_clock::duration;</span><br><span class="line"><span class="comment">// 在时刻t，发出声音s，持续d</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">setAlarm</span><span class="params">(Time t, Sound s, Duration d)</span></span>;</span><br><span class="line"><span class="comment">// lambda</span></span><br><span class="line"><span class="keyword">auto</span> setSoundL = [](Sound s)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>::chrono;</span><br><span class="line">    setAlarm(steady_clock::now() + hours(<span class="number">1</span>),  <span class="comment">// 一小时后</span></span><br><span class="line">             s,                               <span class="comment">// 发出声音</span></span><br><span class="line">             seconds(<span class="number">30</span>));                    <span class="comment">// 响30秒</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// c++14提供了ms,s,h</span></span><br><span class="line"><span class="keyword">auto</span> setSoundL = [](Sound s)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>::chrono;</span><br><span class="line">    setAlarm(steady_clock::now() + <span class="number">1</span>h, s, <span class="number">30</span>s);                 </span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// std::bind</span></span><br><span class="line"><span class="keyword">using</span> namepsace <span class="built_in">std</span>::chrono;</span><br><span class="line"><span class="keyword">using</span> namepsace <span class="built_in">std</span>::literals;</span><br><span class="line"><span class="keyword">using</span> namepsace <span class="built_in">std</span>::placeholders;</span><br><span class="line"><span class="keyword">auto</span> setSoundB = <span class="built_in">std</span>::bind(setAlarm, steady_clock::now() + <span class="number">1</span>h, _1, <span class="number">30</span>s);</span><br></pre></td></tr></table></figure><br>使用<code>std::bind</code>存在一些问题，我们想要的是在setAlarm被调用的时刻之后1小时报警，但是这里是调用<code>std::bind</code>一小时后报警，为了解决这个问题需要延迟表达式的评估求值调用setAlarm的时刻。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// c++14: 标准运算符的模板实参大多数情况下可以省略不写</span></span><br><span class="line"><span class="keyword">auto</span> setSoundB = <span class="built_in">std</span>::bind(setAlarm, </span><br><span class="line">                           <span class="built_in">std</span>::bind(<span class="built_in">std</span>::plus&lt;&gt;(), steady_clock::now(), <span class="number">1</span>h),</span><br><span class="line">                           _1, <span class="number">30</span>s);</span><br><span class="line"><span class="comment">// c++11 还不支持这样的特性</span></span><br><span class="line"><span class="keyword">auto</span> setSoundB = <span class="built_in">std</span>::bind(setAlarm, </span><br><span class="line">                           <span class="built_in">std</span>::bind(<span class="built_in">std</span>::plus&lt;steady_clock::time_point&gt;(), </span><br><span class="line">                           steady_clock::now(), <span class="number">1</span>h),</span><br><span class="line">                           _1, <span class="number">30</span>s);</span><br></pre></td></tr></table></figure><br>一旦函数进行重载，新的问题又会出现，之前的lambda式没有问题，但是<code>std::bind</code>会无法编译通过。为了使得<code>std::bind</code>的调用能够通过编译，需要强制转换类型到合适的函数指针。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> SetAlarm3ParamType = <span class="keyword">void</span>(*)(Time t, Sound s, Duration d);</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> setSoundB = <span class="built_in">std</span>::bind(<span class="keyword">static_cast</span>&lt;SetAlarm3ParamType&gt;(setAlarm),</span><br><span class="line">                           <span class="built_in">std</span>::bind(<span class="built_in">std</span>::plus&lt;steady_clock::time_point&gt;(), </span><br><span class="line">                           steady_clock::now()</span><br><span class="line">                           <span class="number">1</span>h),</span><br><span class="line">                           _1, <span class="number">30</span>s );</span><br></pre></td></tr></table></figure><br>这样又带出来lambda式和<code>std::bind</code>的另一个不同之处，lambda式式常规的函数唤起方式，编译器可以用惯用手法将其内联。可是，<code>std::bind</code>的调用传递了一个函数指针，几乎无法内联。此外，随着想做的事情越来越复杂，使用lambda式的好处会扩大。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> betweenL = [lowVal, highVal](<span class="keyword">const</span> <span class="keyword">auto</span>&amp; val)</span><br><span class="line">                &#123; <span class="keyword">return</span> lowVal &lt;= val &amp;&amp; val &lt;= highVal; &#125;;</span><br><span class="line"><span class="comment">// std::bind</span></span><br><span class="line"><span class="keyword">auto</span> betweenB = </span><br><span class="line">    <span class="built_in">std</span>::bind(<span class="built_in">std</span>::logical_and&lt;<span class="keyword">bool</span>&gt;(),</span><br><span class="line">              <span class="built_in">std</span>::bind(<span class="built_in">std</span>::less_equal&lt;<span class="keyword">int</span>&gt;(), lowVal, <span class="built_in">std</span>::placeholders::_1),</span><br><span class="line">              <span class="built_in">std</span>::bind(<span class="built_in">std</span>::less_equal&lt;<span class="keyword">int</span>&gt;(), <span class="built_in">std</span>::placeholders::_1, highVal));</span><br></pre></td></tr></table></figure><br><code>std::bind</code>总是按值复制，不过可以通过<code>std::ref()</code>达成按引用传递，lambda式要更直观一些。在C++11中，仍需要<code>std::bind</code>的场景：</p>
<ol>
<li>移动捕获。C++11的lambda式不能移动捕获，可以通过std::bind和lambda模拟移动捕获。</li>
<li>多态函数对象。因为绑定对象的函数调用运算符利用了完美转发，呀就可以接受任何类型的实参。<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolyWidget</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T&amp; param)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 使用std::bind绑定PolyWidget对象</span></span><br><span class="line">PolyWidget pw;</span><br><span class="line"><span class="keyword">auto</span> boundPW = <span class="built_in">std</span>::bind(pw, _1);</span><br></pre></td></tr></table></figure>
这样，boundPW就可以通过任意类型的实参加以调用，C++11 lambda做不到这一点，但是C++14可以。因此，std::bind在C++14已经没啥用处了。<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> boundPW = [pw](<span class="keyword">const</span> <span class="keyword">auto</span>&amp; param) &#123; pw(param); &#125;;</span><br></pre></td></tr></table></figure>
<h1 id="并发API"><a href="#并发API" class="headerlink" title="并发API"></a>并发API</h1><h2 id="基于任务的程序设计"><a href="#基于任务的程序设计" class="headerlink" title="基于任务的程序设计"></a>基于任务的程序设计</h2></li>
</ol>
<h2 id="异步，指定std-launch-async"><a href="#异步，指定std-launch-async" class="headerlink" title="异步，指定std::launch::async"></a>异步，指定std::launch::async</h2><h2 id="使std-thread在所有路径不可联结"><a href="#使std-thread在所有路径不可联结" class="headerlink" title="使std::thread在所有路径不可联结"></a>使std::thread在所有路径不可联结</h2><h2 id="关注线程句柄的析构函数"><a href="#关注线程句柄的析构函数" class="headerlink" title="关注线程句柄的析构函数"></a>关注线程句柄的析构函数</h2><h2 id="针对一次性时间通信使用以void为模板"><a href="#针对一次性时间通信使用以void为模板" class="headerlink" title="针对一次性时间通信使用以void为模板"></a>针对一次性时间通信使用以void为模板</h2><h2 id="并发std-atomic，特种内存volatile"><a href="#并发std-atomic，特种内存volatile" class="headerlink" title="并发std::atomic，特种内存volatile"></a>并发std::atomic，特种内存volatile</h2>]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Review</title>
    <url>/2018/11/20/21.Algorithm/</url>
    <content><![CDATA[<p>Algorithm review of M.H.Alsuwaiyel, Algorithms design techniques and Analysis, Publishing House of Electronics Industry.</p>
<a id="more"></a>
<h1 id="分析-Algorithmic-Analysis"><a href="#分析-Algorithmic-Analysis" class="headerlink" title="分析 Algorithmic Analysis"></a>分析 Algorithmic Analysis</h1><h2 id="Binary-Search"><a href="#Binary-Search" class="headerlink" title="Binary Search"></a>Binary Search</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">low←1; high←n; j←0;</span><br><span class="line">while (low &lt;&#x3D; high) and (j←0)</span><br><span class="line">    mid←(low+high)&#x2F;2;</span><br><span class="line">    if x &#x3D; A[mid] then j←mid;</span><br><span class="line">    else if x &lt; A[mid] then high←mid–1;</span><br><span class="line">    else low←mid+1;</span><br><span class="line">end while;</span><br><span class="line">return j;</span><br></pre></td></tr></table></figure>
<p>The number of comparisons performed by the algorithm Binary Search on a sorted array of size n is at most $\lfloor log n \rfloor+1$</p>
<h2 id="Merging-Two-Sorted-Lists"><a href="#Merging-Two-Sorted-Lists" class="headerlink" title="Merging Two Sorted Lists"></a>Merging Two Sorted Lists</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">s←p; t←q+1; k←p;</span><br><span class="line">while s &lt;= q and t &lt;= r</span><br><span class="line">    if A[s] &lt;= A[t] then </span><br><span class="line">        B[k]←A[s];</span><br><span class="line">        s←s+1;</span><br><span class="line">    else</span><br><span class="line">        B[k]←A[t];</span><br><span class="line">        t←t+1;</span><br><span class="line">     end if;</span><br><span class="line">   k←k+1;</span><br><span class="line"> end while;</span><br><span class="line"> if s=q+1 then B[k…r]←A[t…r]</span><br><span class="line"> else B[k…r]←A[s…q]</span><br><span class="line"> end if</span><br><span class="line"> A[p…r]←B[p…r]</span><br></pre></td></tr></table></figure>
<p>此处需要分析</p>
<h2 id="Selection-Sort"><a href="#Selection-Sort" class="headerlink" title="Selection Sort"></a>Selection Sort</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for i←1 to n-1</span><br><span class="line">    k←i;</span><br><span class="line">    for j←i+1 to n</span><br><span class="line">        if A[j]&lt;A[k] then k←j;</span><br><span class="line">    end for;</span><br><span class="line">    if k != i then interchange A[i] and A[k];</span><br><span class="line">end for;</span><br></pre></td></tr></table></figure>
<h2 id="Insertion-Sort"><a href="#Insertion-Sort" class="headerlink" title="Insertion Sort"></a>Insertion Sort</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for i←2 to n</span><br><span class="line">    x←A[i];</span><br><span class="line">    j←i-1;</span><br><span class="line">    while (j&gt;0) and (A[j]&gt;x)</span><br><span class="line">        A[j+1]←A[j];</span><br><span class="line">        j←j-1;</span><br><span class="line">    end while;</span><br><span class="line">    A[j+1]←x;</span><br><span class="line">end for;</span><br></pre></td></tr></table></figure>
<h2 id="Bottom-Up-Merge-Sorting"><a href="#Bottom-Up-Merge-Sorting" class="headerlink" title="Bottom-Up Merge Sorting"></a>Bottom-Up Merge Sorting</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">t←1;</span><br><span class="line">while t&lt;n</span><br><span class="line">    s←t; t←2s; i←0;</span><br><span class="line">    while i+tn</span><br><span class="line">        MERGE(A, i+1, i+s, i+t);</span><br><span class="line">        i←i+t;</span><br><span class="line">    end while;</span><br><span class="line">    if i+s&lt;n then MERGE(A, i+1, i+s, n);</span><br><span class="line">end while;</span><br></pre></td></tr></table></figure>
<h1 id="递归-Techniques-Based-on-Recursion"><a href="#递归-Techniques-Based-on-Recursion" class="headerlink" title="递归 Techniques Based on Recursion"></a>递归 Techniques Based on Recursion</h1><h2 id="归纳-induction"><a href="#归纳-induction" class="headerlink" title="归纳 induction"></a>归纳 induction</h2><p>Given a problem with parameter n, designing an algorithm by induction is based on the fact that if we know how to solve the problem when presented with a parameter less than n, called the induction hypothesis, then our task reduces to extending that solution to include those instances with parameter n.</p>
<h3 id="Radix-Sort"><a href="#Radix-Sort" class="headerlink" title="Radix Sort"></a>Radix Sort</h3><p>Let $L={a<em>1, a_2, …, a_n}$ be a list of n numbers each consisting of exactly $k$ digits. That is, each number is of the form $d_kd</em>{k-1}…d_1$, where each $d_i$ is a digit between 0 and 9.<br>In this problem, instead of applying induction on n, the number of objects, we use induction on k, the size of each integer.</p>
<p>If the numbers are first distributed into the lists by their <strong>least significant digit</strong>, then a very efficient algorithm results.<br>Suppose that the numbers are sorted lexicographically according to their least <em>k-1</em> digits, i.e., digits $d<em>{k-1}, d</em>{k-2}, …, d_1$.<br>After sorting them on their <em>k</em>th digits, they will eventually be sorted.</p>
<p><strong>First</strong>, distribute the numbers into 10 lists <em>$L_0, L_1, …, L_9$</em> according to digit <em>$d_1$</em> so that those numbers with <em>$d_1=0$</em> constitute list <em>$L_0$</em>, those with <em>$d_1=1$</em> constitute list <em>$L_1$</em> and so on.<br><strong>Next</strong>, the lists are coalesced in the order <em>$L_0, L_1, …, L_9$</em>.<br><strong>Then</strong>, they are distributed into 10 lists according to digit <em>$d_2$</em>, coalesced in order, and so on.</p>
<p><strong>After</strong> distributing them according to <em>$d_k$</em> and collecting them in order, all numbers will be sorted.</p>
<p><strong>Example</strong>: Sort A nondecreasingly. A[1…5]=7467，3275，6792，9134，1239</p>
<p><strong>Input</strong>: A linked list of numbers $L={a_1, a_2, …, a_n}$ and <em>k</em>, the number of digits.<br><strong>Output</strong>: <em>L</em> sorted in nondecreasing order.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for j←1 to k</span><br><span class="line">    Prepare 10 empty lists L0, L1, …, L9;</span><br><span class="line">    while L is not empty</span><br><span class="line">        a←next element in L;</span><br><span class="line">        Delete a from L;</span><br><span class="line">        i←jth digit in a;</span><br><span class="line">        Append a to list Li;</span><br><span class="line">    end while;</span><br><span class="line">    L ←L0;</span><br><span class="line">  for i ←1 to 9</span><br><span class="line">      L←L, Li   //Append list Li to L</span><br><span class="line">  end for;</span><br><span class="line">end for;</span><br><span class="line">return L;</span><br></pre></td></tr></table></figure><br>Time Complexity: $\Theta(n)$<br>Space Complexity: $\Theta(n)$</p>
<h3 id="Generating-permutations"><a href="#Generating-permutations" class="headerlink" title="Generating permutations"></a>Generating permutations</h3><p>Generating all permutations of the numbers 1, 2, …, n.<br>Based on the assumption that if we can generate all the permutations of n-1 numbers, then we can get algorithms for generating all the permutations of n numbers.</p>
<p>Generate all the permutations of the numbers 2, 3, …, n and add the number 1 to the beginning of each permutation.<br>Generate all permutations of the numbers 1, 3, 4, …, n and add the number 2 to the beginning of each permutation.<br>Repeat this procedure until finally the permutations of 1, 2, …, n-1 are generated and the number n is added at the beginning of each permutation.</p>
<p>Input: A positive integer n;<br>Output: All permutations of the numbers 1, 2, …, n;</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> for j←1 to n</span><br><span class="line">     P[j]←j;</span><br><span class="line"> end for;</span><br><span class="line"> perm(1);</span><br><span class="line"></span><br><span class="line">perm(m)</span><br><span class="line"> if m=n then output P[1…n]</span><br><span class="line"> else</span><br><span class="line">     for j←m to n</span><br><span class="line">         interchange P[j] and P[m];</span><br><span class="line">            //Add one number at the beginning of the permutation</span><br><span class="line">         perm(m+1);</span><br><span class="line">            //Generate permutations for the left numbers</span><br><span class="line">         interchange P[j] and P[m];</span><br><span class="line">     end for;</span><br><span class="line"> end if;</span><br></pre></td></tr></table></figure>
<p>Time Complexity: $\Theta(nn!)$<br>Space Complexity: $\Theta(n)$</p>
<h3 id="Find-Majority"><a href="#Find-Majority" class="headerlink" title="Find Majority"></a>Find Majority</h3><p>Let A[1…n] be a sequence of integers. An integer a in A is called the majority if it appears more than $\lfloor n/2 \rfloor$ times in A.<br>For example:<br>Sequence 1, 3, 2, 3, 3, 4, 3: 3 is the majority element since 3 appears 4 times which is more than $\lfloor n/2 \rfloor$<br>Sequence 1, 3, 2, 3, 3, 4: 3 is not the majority element since 3 appears three times which is equal to $\lfloor n/2 \rfloor$, but not more than $\lfloor n/2 \rfloor$</p>
<p>If two different elements in the original sequence are removed, then the majority in the original sequence remains the majority in the new sequence.<br>The above observation suggests the following procedure for finding an element that is candidate for being the majority.</p>
<p>Let x=A[1] and set a counter to 1.<br>Starting from A[2], scan the elements one by one increasing the counter by one if the current element is equal to x and decreasing the counter by one if the current element is not equal to x.<br>If all the elements have been scanned and the counter is greater than zero, then return x as the candidate.<br>If the counter becomes 0 when comparing x with A[j], 1&lt; j &lt; n, then call procedure candidate recursively on the elements A[j+1…n].</p>
<p>Input: An array A[1…n] of n elements;<br>Output: The majority element if it exists; otherwise none;<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> x←candidate(1);</span><br><span class="line"> count←0;</span><br><span class="line"> for j←1 to n</span><br><span class="line">     if A[j]=x then count←count+1;</span><br><span class="line"> end for;</span><br><span class="line"> if count&gt; [n/2] then return x;</span><br><span class="line"> else return none;</span><br><span class="line"></span><br><span class="line">candidate(m)</span><br><span class="line"> j←m; x←A[m]; count←1;</span><br><span class="line"> while j&lt;n and count&gt;0</span><br><span class="line">     j ←j+1;</span><br><span class="line">     if A[j]=x then count ←count+1;</span><br><span class="line">     else count ←count-1;</span><br><span class="line"> end while;</span><br><span class="line"> if j=n then return x;</span><br><span class="line"> else return candidate(j+1);</span><br></pre></td></tr></table></figure></p>
<h2 id="分而治之-Divide-and-Conquer"><a href="#分而治之-Divide-and-Conquer" class="headerlink" title="分而治之 Divide and Conquer"></a>分而治之 Divide and Conquer</h2><p>A divide-and-conquer algorithm divides the problem instance into a number of subinstances (in most cases 2), recursively solves each subinsance separately, and then combines the solutions to the subinstances to obtain the solution to the original problem instance.</p>
<h3 id="The-Divide-and-Conquer-Paradigm"><a href="#The-Divide-and-Conquer-Paradigm" class="headerlink" title="The Divide and Conquer Paradigm"></a>The Divide and Conquer Paradigm</h3><p>The divide step: the input is partitioned into $p\geq1$ parts, each of size strictly less than n.<br>The conquer step: performing p recursive call(s) if the problem size is greater than some predefined threshold n0.<br>The combine step: the solutions to the p recursive call(s) are combined to obtain the desired output.</p>
<ol>
<li>If the size of the instance I is “small”, then solve the problem using a straightforward method and return the answer. Otherwise, continue to the next step;</li>
<li>Divide the instance I into p subinstances I1, I2, …, Ip of approximately the same size;</li>
<li>Recursively call the algorithm on each subinstance Ij, $1\leq j\leq p$, to obtain p partial solutions;</li>
<li>Combine the results of the p partial solutions to obtain the solution to the original instance I. Return the solution of instance I.</li>
</ol>
<h3 id="Finding-kth-Smallest-Element"><a href="#Finding-kth-Smallest-Element" class="headerlink" title="Finding kth Smallest Element"></a>Finding kth Smallest Element</h3><p>The media of a sequence of n sorted numbers A[1…n] is the “middle” element.<br>If n is odd, then the middle element is the (n+1)/2th element in the sequence.<br>If n is even, then there are two middle elements occurring at positions n/2 and n/2+1. In this case, we will choose the n/2th smallest element.<br>Thus, in both cases, the median is the $\lceil n/2 \rceil$th smallest element.<br>The kth smallest element is a general case.</p>
<p>Input: An array A[1…n] of n elements and an integer k, $1\leq k\leq n$;<br>Output: The kth smallest element in A;<br>select(A, 1, n, k);</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">select(A, low, high, k)</span><br><span class="line"> p←high-low+1;</span><br><span class="line"> if p&lt;44 then sort A and return (A[k]);</span><br><span class="line"> Let q=[p/5](向下取整). Divide A into q groups of 5 elements each. </span><br><span class="line">    If 5 does not divide p, then discard the remaining elements;</span><br><span class="line"> Sort each of the q groups individually and extract its media. </span><br><span class="line">    Let the set of medians be M.</span><br><span class="line"> mm←select(M, 1, q, [q/2](向上取整));</span><br><span class="line"> Partition A[low…high] into three arrays: </span><br><span class="line">    A1=&#123;a|a&lt;mm&#125;, A2=&#123;a|a=mm&#125;, A3=&#123;a|a&gt;mm&#125;;</span><br><span class="line"> case</span><br><span class="line">    |A1|&gt;=k: return select (A1, 1, |A1|, k);</span><br><span class="line">    |A1|+|A2|&gt;=k: return mm;</span><br><span class="line">    |A1|+|A2|&lt;k: return select(A3, 1, |A3|, k-|A1|-|A2|);</span><br><span class="line"> end case;</span><br></pre></td></tr></table></figure>
<p>The kth smallest element in a set of n elements drawn from a linearly ordered set can be found in $\Theta(n)$ time.</p>
<h3 id="Quicksort"><a href="#Quicksort" class="headerlink" title="Quicksort"></a>Quicksort</h3><p>Let A[low…high] be an array of n numbers, and x=A[low].<br>We consider the problem of rearranging the elements in A so that all elements less than or equal to x precede x which in turn precedes all elements greater than x.<br>After permuting the element in the array, x will be A[w] for some w, low&lt;=w&lt;=high. The action of rearrangement is also called splitting or partitioning around x, which is called the pivot or splitting element.</p>
<p>We say that an element A[j] is in its proper position or correct position if it is neither smaller than the elements in A[low…j-1] nor larger than the elements in A[j+1…high].<br>After partitioning an array A using $x\in A$ as a pivot, x will be in its correct position.</p>
<p>Input: An array of elements A[low…high];<br>Output: A with its elements rearranged, if necessary; w, the new position of the splitting element A[low];<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">split(A[...], w)</span><br><span class="line"> i←low;</span><br><span class="line"> x←A[low];</span><br><span class="line"> for j←low+1 to high</span><br><span class="line">     if A[j]&lt;=x then</span><br><span class="line">        i←i+1;</span><br><span class="line">         if i≠j then interchange A[i] and A[j];</span><br><span class="line">     end if;</span><br><span class="line"> end for;</span><br><span class="line"> interchange A[low] and A[i];</span><br><span class="line"> w←i;</span><br><span class="line">return A and w;</span><br></pre></td></tr></table></figure><br>The number of element comparisons performed by Algorithm SPLIT is exactly n-1. Thus, its time complexity is $\Theta(n)$.</p>
<p>The only extra space used is that needed to hold its local variables. Therefore, the space complexity is $\Theta(1)$.</p>
<p>Input: An array A[1…n] of n elements;<br>Output: The elements in A sorted in nondecreasing order;<br>quicksort(A, 1, n);<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">quicksort(A, low, high)</span><br><span class="line"> if low&lt;high then</span><br><span class="line">     SPLIT(A[low…high], w) //w is the new position of A[low];</span><br><span class="line">     quicksort(A, low, w-1);</span><br><span class="line">     quicksort(A, w+1, high);</span><br><span class="line"> end if;</span><br></pre></td></tr></table></figure><br>The average number of comparisons performed by Algorithm QUICKSORT to sort an array of n elements is $\Theta(nlogn)$.</p>
<h2 id="动态规划-Dynamic-Programming"><a href="#动态规划-Dynamic-Programming" class="headerlink" title="动态规划 Dynamic Programming"></a>动态规划 Dynamic Programming</h2><p>An algorithm that employs the dynamic programming technique is not recursive by itself, but the underlying solution of the problem is usually stated in the form of a recursive function.<br>This technique resorts to evaluating the recurrence in a bottom-up manner, saving intermediate results that are used later on to compute the desired solution.<br>This technique applies to many combinatorial optimization problems to derive efficient algorithms.</p>
<h3 id="Longest-Common-Subsequence"><a href="#Longest-Common-Subsequence" class="headerlink" title="Longest Common Subsequence"></a>Longest Common Subsequence</h3><p>Given two strings A and B of lengths n and m, respectively, over an alphabet $\sum$, determine the length of the longest subsequence that is common to both A and B.<br>A subsequence of $A=a<em>1a_2…a_n$ is a string of the form $a</em>{i1}a<em>{i2}…a</em>{ik}$, where each $i_j$ is between 1 and n and $1\leq i_1&lt;i_2&lt;…&lt;i_k\leq n$.</p>
<p>Let $A=a_1a_2…a_n$ and $B=b_1b_2…b_m$.<br>Let L[i, j] denote the length of a longest common subsequence of $a_1a_2…a_i$ and $b_1b_2…b_j$. $0\leq i\leq n, 0\leq j\leq m$. When i or j be 0, it means the corresponding string is empty.<br>Naturally, if i=0 or j=0; the L[i, j]=0</p>
<p>Suppose that both i and j are greater than 0. Then<br>If $a_i=b_j，L[i,j]=L[i-1,j-1]+1$<br>If $a_i\ne b_j，L[i,j]=max{L[i,j-1],L[i-1,j]}$<br>We get the following recurrence for computing the length of the longest common subsequence of A and B:</p>
<script type="math/tex; mode=display">
L[i,j]= 
\begin{equation} 
\left\{ 
\begin{array}{lc} 
0, && if\ i = 0 \ or \ j = 0\\
L[i-1,j-1]+1, && if\ i>0,j>0, and\ a_i=b_j\\
max\{L[i,j-1],L[i-1,j]\}, && if\ i>0,j>0, and\ a_i\ne b_j
\end{array} 
\right. 
\end{equation}</script><p>We use an $(n+1)\times(m+1)$ table to compute the values of $L[i, j]$ for each pair of values of i and j, $0\leq i\leq n, 0\leq j\leq m$.<br>We only need to fill the table $L[0…n, 0…m]$ row by row using the previous formula.<br>Example: A=zxyxyz, B=xyyzx</p>
<p><strong>Input</strong>: Two strings A and B of lengths n and m, respectively, over an alphabet $\sum$;<br><strong>Output</strong>: The length of the longest common subsequence of A and B.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for i←0 to n</span><br><span class="line">    L[i, 0]←0;</span><br><span class="line">end for;</span><br><span class="line">for j←0 to m</span><br><span class="line">    L[0, j]←0;</span><br><span class="line">end for;</span><br><span class="line">for i←1 to n</span><br><span class="line">    for j←1 to m</span><br><span class="line">        if ai=bj then L[i, j]←L[i-1, j-1]+1;</span><br><span class="line">      else L[i, j]←max&#123;L[i, j-1], L[i-1, j]&#125;;</span><br><span class="line">      end if;</span><br><span class="line">   end for;</span><br><span class="line">end for;</span><br><span class="line">return L[n, m];</span><br></pre></td></tr></table></figure><br>Time Complexity: $\Theta(nm)$<br>Space Complexity: $\Theta(\min{n,m})$</p>
<h3 id="The-Dynamic-Programming-Paradigm"><a href="#The-Dynamic-Programming-Paradigm" class="headerlink" title="The Dynamic Programming Paradigm"></a>The Dynamic Programming Paradigm</h3><p>The idea of saving solutions to subproblems in order to avoid their recomputation is the basis of this powerful method.<br>This is usually the case in many combinatorial optimization problems in which the solution can be expressed in the form of a recurrence whose direct solution causes subinstances to be computed more than once.</p>
<p>An important observation about the working of dynamic programming is that the algorithm computes an optimal solution to every subinstance of the original instance considered by the algorithm.<br>This argument illustrates an important principle in algorithm design called the principle of optimality: Given an optimal sequence of decisions, each subsequence must be an optimal sequence of decisions by itself.</p>
<h3 id="All-Pairs-Shortest-Path"><a href="#All-Pairs-Shortest-Path" class="headerlink" title="All-Pairs Shortest Path"></a>All-Pairs Shortest Path</h3><p>$d<em>{i,j}^0=l[i,j]$<br>$d</em>{i,j}^1$ is the length of a shortest path from i to j that does not pass through any vertex except possibly vertex 1<br>$d<em>{i,j}^2$ is the length of a shortest path from i to j that does not pass through any vertex except possibly vertex 1 or vertex 2 or both<br>$d</em>{i,j}^n$ is the length of a shortest path from i to j, i.e. the distance from i to j</p>
<p>We can compute $d_{i,j}^k$ recursively as follows:</p>
<script type="math/tex; mode=display">
d_{i,j}^k= 
\begin{equation} 
\left\{ 
\begin{array}{lc} 
l[i,j] && if\ k=0 \\
\min\left\{d_{i,j}^{k-1},d_{i,k}^{k-1}+d_{k,j}^{k-1}\right\} && if\ 1\leq k\leq n
\end{array} 
\right. 
\end{equation}</script><p>Here，We use <strong><em>Floyd Algorithm</em></strong> !<br>use n+1 matrices $D_0, D_1, D_2, …, D_n$ of dimension $n\times n$ to compute the lengths of the shortest constrained paths.<br>Initially, we set $D_0[i, i]=0, D_0[i, j]=l[i, j] if\ i\ne j$ and (i, j) is an edge in G; otherwise $D_0[i, j]=\infty $.<br>We then make n iterations such that after the kth iteration, $D_k[i, j]$ contains the value of a shortest length path from vertex <em>i</em> to vertex <em>j</em> that does not pass through any vertex numbered higher than k.</p>
<p>Thus, in the kth iteration, we compute $D_k[i, j]$ using the formula</p>
<script type="math/tex; mode=display">
D_k[i, j]=\min\left\{D_{k-1}[i, j], D_{k-1}[i, k]+D_{k-1}[k, j]\right\}</script><p>Example:<br><img src="/img/algorithm_review/pic.png" alt=""><br>Input: An $n\times n$ matrix $l[1…n, 1…n]$ such that $l[i, j]$ is the length of the edge(i, j) in a directed graph G=({1, 2, …, n}, E);<br>Output: A matrix D with D[i, j]=the distance from <em>i</em> to <em>j</em><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">D←l;</span><br><span class="line">for k←1 to n</span><br><span class="line">    for i←1 to n</span><br><span class="line">        for j←1 to n</span><br><span class="line">            D[i, j]=min&#123;D[i, j], D[i, k]+D[k, j]);</span><br><span class="line">        end for;</span><br><span class="line">    end for;</span><br><span class="line">end for;</span><br></pre></td></tr></table></figure><br>Time Complexity: $\Theta(n^3)$<br>Space Complexity: $\Theta(n^2)$</p>
<h3 id="Knapsack-Problem"><a href="#Knapsack-Problem" class="headerlink" title="Knapsack Problem"></a>Knapsack Problem</h3><p>Let $U={u_1, u_2, …, u_n}$ be a set of n items to be packed in a knapsack of size C. for $1\leq j\leq n$, let $s_j$ and $v_j$ be the size and value of the jth item, respectively, where C and sj, vj, 1jn, are all positive integers.</p>
<p>The objective is to fill the knapsack with some items for U whose total size is at most C and such that their total value is maximum. Assume without loss of generality that the size of each item does not exceed C.</p>
<p>More formally, given U of n items, we want to find a subset $S\in U$ such that</p>
<script type="math/tex; mode=display">
\sum_{u_i\in S}V_i</script><p>is maximized subject to the constraint</p>
<script type="math/tex; mode=display">
\sum_{u_i\in S}V_i \leq C</script><p>This version of the knapsack problem is sometimes referred to in the literature as the 0/1 knapsack problem. This is because the knapsack cannot contain more than one item of the same type.</p>
<p>Let V[i, j] denote the value obtained by filling a knapsack of size j with items taken from the first i items {u1, u2, …, ui} in an optimal way. Here the range of i is from 0 to n and the range of j is from 0 to C. Thus, what we seek is the value V[n, C].<br>Obviously, V[0, j] is 0 for all values of j, as there is nothing in the knapsack. On the other hand, V[i, 0] is 0 for all values of i since nothing can be put in a knapsack of size 0.</p>
<p>V[i, j], where i&gt;0 and j&gt;0, is the maximum of the following two quantities:<br>V[i-1, j]: The maximum value obtained by filling a knapsack of size j with items taken from ${u<em>1, u_2, …, u</em>{i-1}}$ only in an optimal way.<br>$V[i-1, j-s<em>i]+v_i$: The maximum value obtained by filling a knapsack of size $j-s_i$ with items taken from ${u_1, u_2, …, u</em>{i-1}}$ in an optimal way plus the value of item $u_i$. This case applies only if $j\geq s_i$ and it amounts to adding item $u_i$ to the knapsack.</p>
<p>Then, we got the following recurrence for finding the value in an optimal packing:</p>
<script type="math/tex; mode=display">
V[i,j]= 
\begin{equation} 
\left\{ 
\begin{array}{lc} 
0 && if\ i=0\ or\ j=0 \\
V[i-1,j] && if\ j<s_i \\
\max\{V[i-1,j],V[i-1,j-s_i]+v_i\} && if\ j\geq s_i
\end{array}
\right.
\end{equation}</script><p>Using dynamic programming to solve this integer programming problem is now straightforward. We use an $(n+1)\times (C+1)$ table to evaluate the values of V[i, j]. We only need to fill the table V[0…n, 0…C] row by row using the above formula.</p>
<p>Example:<br>C=9<br>U={u1, u2, u3, u4}<br>Si=2, 3, 4, 5<br>Vi=3, 4, 5, 7</p>
<p><strong>Input</strong>: A set of items $U={u<em>1, u_2, …, u_n}$ with sizes $s_1, s_2, …, s_n$ and values $v_1, v_2, …, v_n$ and a knapsack capacity C.<br><strong>Output</strong>: The maximum value of the function $\sum</em>{u<em>i\in S}v_i$ subject to $\sum</em>{u_i\in S}S_i\leq C$ for some subset of items $S\in U$.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for i←0 to n</span><br><span class="line">    V[i, 0]←0;</span><br><span class="line">end for;</span><br><span class="line">for j←0 to C</span><br><span class="line">    V[0, j]←0;</span><br><span class="line">end for;</span><br><span class="line">for i←1 to n</span><br><span class="line">    for j←1 to C</span><br><span class="line">        V[i, j]←V[i-1, j];</span><br><span class="line">       if si&lt;=j then V[i, j] ← max&#123;V[i, j], V[i-1, j-si]+vi&#125;;</span><br><span class="line">   end for;</span><br><span class="line">end for;</span><br><span class="line">return V[n, C];</span><br></pre></td></tr></table></figure><br>Time Complexity: $\Theta(nC)$<br>Space Complexity: $\Theta(C)$</p>
<h1 id="贪心-The-Greedy-Approach"><a href="#贪心-The-Greedy-Approach" class="headerlink" title="贪心 The Greedy Approach"></a>贪心 The Greedy Approach</h1><p>As in the case of dynamic programming algorithms, greedy algorithms are usually designed to solve optimization problems in which a quantity is to be minimized or maximized.<br>Unlike dynamic programming algorithms, greedy algorithms typically consist of a n iterative procedure that tries to find a local optimal solution.<br>In some instances, these local optimal solutions translate to global optimal solutions. In others, they fail to give optimal solutions.</p>
<p>A greedy algorithm makes a correct guess on the basis of little calculation without worrying about the future. Thus, it builds a solution step by step. Each step increases the size of the partial solution and is based on local optimization.<br>The choice make is that which produces the largest immediate gain while maintaining feasibility.<br>Since each step consists of little work based on a small amount of information, the resulting algorithms are typically efficient.</p>
<h2 id="The-Fractional-Knapsack-Problem"><a href="#The-Fractional-Knapsack-Problem" class="headerlink" title="The Fractional Knapsack Problem"></a>The Fractional Knapsack Problem</h2><p>Given n items of sizes s1, s2, …, sn, and values v1, v2, …, vn and size C, the knapsack capacity, the objective is to find nonnegative real numbers x1, x2, …, xn that maximize the sum</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{n}x_iv_i</script><p>subject to the constraint</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{n}x_is_i\leq C</script><p>This problem can easily be solved using the following greedy strategy:<br>For each item compute yi=vi/si, the ratio of its value to its size.<br>Sort the items by decreasing ratio, and fill the knapsack with as much as possible from the first item, then the second, and so forth.<br>This problem reveals many of the characteristics of a greedy algorithm discussed above: The algorithm consists of a simple iterative procedure that selects that item which produces that largest immediate gain while maintaining feasibility.</p>
<h2 id="Shortest-Path-Problem"><a href="#Shortest-Path-Problem" class="headerlink" title="Shortest Path Problem"></a>Shortest Path Problem</h2><p>Let G=(V, E) be a directed graph in which each edge has a nonnegative length, and a distinguished vertex s called the source. The single-source shortest path problem, or simply the shortest path problem, is to determine the distance from s to every other vertex in V, where the distance from vertex s to vertex x is defined as the length of a shortest path from s to x.<br>For simplicity, we will assume that V={1, 2, …, n} and s=1.<br>This problem can be solved using a greedy technique known as <strong><em>Dijkstra algorithm</em></strong>.</p>
<p>The set of vertices is partitioned into two sets X and Y so that X is the set of vertices whose distance from the source has already been determined, while Y contains the rest vertices. Thus, initially X={1} and Y={2, 3, …, n}.<br>Associated with each vertex y in Y is a label $\lambda[y]$, which is the length of a shortest path that passes only through vertices in X. Thus, initially</p>
<script type="math/tex; mode=display">
\lambda[1]=0,\ \lambda[i]= 
\begin{equation} 
\left\{ 
\begin{array}{lc} 
length(1,i) && if\ (i,i)\in E \\
\infty && if\ (1,i)\notin E
\end{array}
\right.
\end{equation}\ ,\ \
2\leq i\leq n</script><p>At each step, we select a vertex $y\in Y$ with minimum $\lambda$ and move it to X, and $\lambda$ of each vertex $w\in Y$ that is adjacent to y is updated indicating that a shorter path to w via y has been discovered.</p>
<script type="math/tex; mode=display">
\forall w\in Y\ and\ (y,w)\in E,\ \lambda[w]=\min\{\lambda[w],\lambda[y]+length(y,w)\}</script><p>The above process is repeated until Y is empty.<br>Finally, $lambda$ of each vertex in X is the distance from the source vertex to this one.<br>Example:<br><img src="/img/algorithm_review/pic2.png" alt=""><br><strong>Input</strong>: A weighted directed graph G=(V, E), where V={1, 2, …, n};<br><strong>Output</strong>: The distance from vertex 1 to every other vertex in G;<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">X=&#123;1&#125;; Y←V-&#123;1&#125;; λ[1]←0;</span><br><span class="line">for y←2 to n</span><br><span class="line">    if y is adjacent to 1 then λ[y]←length[1, y];</span><br><span class="line">    else λ[y]← ∞;</span><br><span class="line">    end if;</span><br><span class="line">end for;</span><br><span class="line">for j←1 to n-1</span><br><span class="line">    Let y∈Y be such that λ[y] is minimum;</span><br><span class="line">    X←X∪&#123;y&#125;;  // add vertex y to X</span><br><span class="line">    Y←Y-&#123;y&#125;;  // delete vertex y from Y</span><br><span class="line">   for each edge (y, w)</span><br><span class="line">       if w∈Y and λ[y]+length[y, w]&lt;λ[w] then</span><br><span class="line">           λ[w]←λ[y]+length[y, w];</span><br><span class="line">    end for;</span><br><span class="line">end for;</span><br></pre></td></tr></table></figure><br>Given a directed graph G with nonnegative weights on its edges and a source vertex s, Algorithm DIJKSTRA finds the length of the distance from s to every other vertex in $\Theta(n^2)$ time.</p>
<h2 id="MST-Minimum-Cost-Spanning-Trees-Kruskal-Prim"><a href="#MST-Minimum-Cost-Spanning-Trees-Kruskal-Prim" class="headerlink" title="MST / Minimum Cost Spanning Trees (Kruskal / Prim)"></a>MST / Minimum Cost Spanning Trees (Kruskal / Prim)</h2><p>Let G=(V, E) be a connected undirected graph with weights on its edges.<br>A spanning tree (V, T) of G is a subgraph of G that is a tree.<br>If G is weighted and the sum of the weights of the edges in T is minimum, then (V, T) is called a minimum cost spanning tree or simply a minimum spanning tree.</p>
<p>Kruskal’s algorithm works by maintaining a forest consisting of several spanning trees that are gradually merged until finally the forest consists of exactly one tree.<br>The algorithm starts by sorting the edges in nondecreasing order by weight.</p>
<p>Next, starting from the forest (V, T) consisting of the vertices of the graph and none of its edges, the following step is repeated until (V, T) is transformed into a tree: Let (V, T) be the forest constructed so far, and let $e\in E-T$ be the current edge being considered. If adding e to T does not create a cycle, then include e in T; otherwise discard e.<br>This process will terminate after adding exactly n-1 edges.<br>Example:<br><img src="/img/algorithm_review/pic3.png" alt=""></p>
<h2 id="Huffman-File-Compression"><a href="#Huffman-File-Compression" class="headerlink" title="Huffman / File Compression"></a>Huffman / File Compression</h2><p>Suppose we are given a file, which is a string of characters. We wish to compress the file as much as possible in such a way that the original file can easily be reconstructed.</p>
<p>Let the set of characters in the file be C={c1, c2, …, cn}. Let also f(ci), $1\leq i \leq n$, be the frequency of character ci in the file, i.e., the number of times ci appears in the file. </p>
<p>Using a fixed number of bits to represent each character, called the encoding of the character, the size of the file depends only on the number of characters in the file.<br>Since the frequency of some characters may be much larger than others, it is reasonable to use variable length encodings.</p>
<p>Intuitively, those characters with large frequencies should be assigned short encodings, whereas long encodings may be assigned to those characters with small frequencies.<br>When the encodings vary in length, we stipulate that the encoding of one character must not be the prefix of the encoding of another character; such codes are called prefix codes.<br>For instance, if we assign the encodings 10 and 101 to the letters “a” and “b”, there will be an ambiguity as to whether 10 is the encoding of “a” or is the prefix of the encoding of the letter “b”.</p>
<p>Once the prefix constraint is satisfied, the decoding becomes unambiguous; the sequence of bits is scanned until an encoding of some character is found.<br>One way to “parse” a given sequence of bits is to use a full binary tree, in which each internal node has exactly two branches labeled by 0 an 1. The leaves in this tree corresponding to the characters. Each sequence of 0’s and 1’s on a path from the root to a leaf corresponds to a character encoding.</p>
<p>The algorithm presented is due to Huffman.<br>The algorithm consists of repeating the following procedure until C consists of only one character.<br>Let ci and cj be two characters with minimum frequencies.<br>Create a new node c whose frequency is the sum of the frequencies of ci and cj, and make ci and cj the children of c.<br>Let C=C-{ci, cj}∪{c}.</p>
<p>Example:<br>C={a, b, c, d, e}<br>f (a)=20<br>f (b)=7<br>f (c)=10<br>f (d)=4<br>f (e)=18</p>
<h2 id="Graph-Travel"><a href="#Graph-Travel" class="headerlink" title="Graph Travel"></a>Graph Travel</h2><p>In some cases, what is important is that the vertices are visited in a systematic order, regardless of the input graph. Usually, there are two methods of graph traversal:<br>Depth-first search<br>Breadth-first search</p>
<h3 id="Depth-First-Search（DFS）"><a href="#Depth-First-Search（DFS）" class="headerlink" title="Depth-First Search（DFS）"></a>Depth-First Search（DFS）</h3><p>Let G=(V, E) be a directed or undirected graph.<br>First, all vertices are marked unvisited.<br>Next, a starting vertex is selected, say $v\in V$, and marked visited. Let w be any vertex that is adjacent to v. We mark w as visited and advance to another vertex, say x, that is adjacent to w and is marked unvisited. Again, we mark x as visited and advance to another vertex that is adjacent to x and is marked unvisited.</p>
<p>This process of selecting an unvisited vertex adjacent to the current vertex continues as deep as possible until we find a vertex y whose adjacent vertices have all been marked visited.<br>At this point, we back up to the most recently visited vertex, say z, and visit an unvisited vertex that is adjacent to z, if any.<br>Continuing this way, we finally return back to the starting vertex v.<br>The algorithm for such a traversal can be written using recursion.</p>
<p>Example:<br><img src="/img/algorithm_review/pic4.png" alt=""><br>When the search is complete, if all vertices are reachable from the start vertex, a spanning tree called the depth-first search spanning tree is constructed whose edges are those inspected in the forward direction, i.e., when exploring unvisited vertices.<br>As a result of the traversal, the edges of an undirected graph G are classified into the following two types:<br><strong>Tree edges</strong>: edges in the depth-first search tree.<br><strong>Back edges</strong>: all other edges.<br><strong>Input</strong>: An undirected graph G=(V, E);<br><strong>Output</strong>: Preordering of the vertices in the corresponding depth-first search tree.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> predfn←0;</span><br><span class="line"> for each vertex v∈V</span><br><span class="line">     Mark v unvisited;</span><br><span class="line"> end for;</span><br><span class="line"> for each vertex v∈V</span><br><span class="line">     if v is marked unvisited then dfs(v);</span><br><span class="line"> end for;</span><br><span class="line"></span><br><span class="line">dfs(v)</span><br><span class="line"> Mark v visited;</span><br><span class="line"> predfn←predfn+1;</span><br><span class="line"> for each edge (v, w)∈E</span><br><span class="line">     if w is marked unvisited then dfs(w);</span><br><span class="line"> end for;</span><br></pre></td></tr></table></figure></p>
<h3 id="Breadth-First-Search（BFS）"><a href="#Breadth-First-Search（BFS）" class="headerlink" title="Breadth-First Search（BFS）"></a>Breadth-First Search（BFS）</h3><p>When we visit a vertex v, we next visit all vertices adjacent to v.<br>This method of traversal can be implemented by a queue to store unexamined vertices.</p>
<h3 id="Finding-Articulation-Points-in-a-Graph"><a href="#Finding-Articulation-Points-in-a-Graph" class="headerlink" title="Finding Articulation Points in a Graph"></a>Finding Articulation Points in a Graph</h3><p>A vertex v in an undirected graph G with more than two vertices is called an <strong>articulation point</strong> if there exist two vertices u and w different from v such that any path between u and w must pass through v.<br>If G is connected, the removal of v and its incident edges will result in a disconnected subgraph of G.<br>A graph is called biconnected if it is connected and has no articulation points.</p>
<p>To find the set of articulation points, we perform a depth-first search traversal on G.<br>During the traversal, we maintain two labels with each vertex $v\in V: \alpha[v] \ and\ \beta[v]$.<br>$\alpha[v]$ is simply predfn in the depth-first search algorithm. $\beta[v]$ is initialized to $\alpha[v]$, but may change later on during the traversal.</p>
<p>For each vertex v visited, we let $\beta[v]$ be the minimum of the following:<br>$\alpha[v]$<br>$\alpha[u]$ for each vertex u such that (v, u) is a back edge<br>$\beta[w]$ for each vertex w such that (v, w) is a tree edge<br><strong>Thus, $\beta[v]$ is the smallest $\alpha$ that v can reach through back edges or tree edges.</strong></p>
<p>The articulation points are determined as follows:<br>The root is an articulation point if and only if it has two or more children in the depth-first search tree.<br>A vertex v other than the root is an articulation point if and only if v has a child w with $\beta[w]\geq \alpha[v]$.</p>
<p><strong>Input</strong>: A connected undirected graph G=(V, E);<br><strong>Output</strong>: Array A[1…count] containing the articulation points of G, if any.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Let s be the start vertex;</span><br><span class="line">for each vertex v∈V</span><br><span class="line">    Mark v unvisited;</span><br><span class="line">end for;</span><br><span class="line">predfn←0; count←0; rootdegree←0;</span><br><span class="line">dfs(s);</span><br></pre></td></tr></table></figure><br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dfs(v)</span><br><span class="line"> Mark v visited; artpoint←false; predfn←predfn+1;</span><br><span class="line"> α[v]←predfn; β[v]←predfn;</span><br><span class="line"> for each edge (v, w) ∈ E</span><br><span class="line">     if (v, w) is a tree edge then</span><br><span class="line">         dfs(w);</span><br><span class="line">         if v=s then</span><br><span class="line">             rootdegree ← rootdegree+1;</span><br><span class="line">             if rootdegree=2 then artpoint←true;</span><br><span class="line">         else</span><br><span class="line">           β[v]←min&#123;β[v], β[w]&#125;;</span><br><span class="line">           if β[w]&gt;=α[v] then artpoint←true;</span><br><span class="line">       end if;</span><br><span class="line">     else if (v, w) is a back edge then β[v]←min&#123;β[v], α[w]&#125;;</span><br><span class="line">     else do nothing; //w is the parent of v</span><br><span class="line">     end if;</span><br><span class="line"> end for;</span><br><span class="line"> if artpoint then </span><br><span class="line">     count←count +1;</span><br><span class="line">     A[count]←v;</span><br><span class="line"> end if;</span><br></pre></td></tr></table></figure><br>Example:<br><img src="/img/algorithm_review/pic5.png" alt=""></p>
<h1 id="回溯-Backtracking"><a href="#回溯-Backtracking" class="headerlink" title="回溯 Backtracking"></a>回溯 Backtracking</h1><p>suitable for those problems that exhibit good average time complexity. This methodology is based on a methodic examination of the implicit state space induced by the problem instance under study. In the process of exploring the state space of the instance, some pruning takes place.</p>
<p>In many real world problems, a solution can be obtained by exhaustively searching through a large but finite number of possibilities. Hence, the need arose for developing systematic techniques of searching, with the hope of cutting down the search space to possibly a much smaller space. </p>
<p>Here, we present a general technique for organizing the search known as <strong>backtracking</strong>. This algorithm design technique can be described as an organized exhaustive search which often avoids searching all possibilities.</p>
<h2 id="涂色问题-3-Color-Problem"><a href="#涂色问题-3-Color-Problem" class="headerlink" title="涂色问题 3-Color Problem"></a>涂色问题 3-Color Problem</h2><p>Given an undirected graph G=(V, E), it is required to color each vertex in V with one of three colors, say 1, 2, and 3, such that no two adjacent vertices have the same color. We call such a coloring legal; otherwise, if two adjacent vertices have the same color, it is illegal.<br>A coloring can be represented by an n-tuple (c1, c2, …, cn) such that ci∈{1, 2, 3}, $1\leq i\leq n$.<br>For example, (1, 2, 2, 3, 1) denotes a coloring of a graph with five vertices.</p>
<p>There are $3^n$ possible colorings (legal and illegal) to color a graph with n vertices.<br>The set of all possible colorings can be represented by a complete ternary tree called the <strong>search tree</strong>. In this tree, each path from the root to a leaf node represents one coloring assignment.<br>An incomplete coloring of a graph is <strong>partial</strong> if no two adjacent colored vertices have the same color.<br>Backtracking works by generating the underlying tree one node at a time.<br>If the path from the root to the current node corresponds to a legal coloring, the process is terminated (unless more than one coloring is desired).</p>
<p>If the length of this path is less than n and the corresponding coloring is partial, then one child of the current node is generated and is marked as the current node.<br>If, on the other hand, the corresponding path is not partial, then the current node is marked as a <strong>dead node</strong> and a new node corresponding to another color is generated.<br>If, however, all three colors have been tried with no success, the search backtracks to the parent node whose color is changed, and so on.</p>
<p>Example:<br><img src="/img/algorithm_review/pic6.png" alt=""><br>There are two important observations to be noted, which generalize to all backtracking algorithms:<br>(1) The nodes are generated in a depth-first-search manner.<br>(2) There is no need to store the whole search tree; we only need to store the path from the root to the current active node. In fact, no physical nodes are generated at all; the whole tree is implicit. We only need to keep track of the color assignment.</p>
<p><strong>Recursive Algorithm</strong><br><strong>Input</strong>: An undirected graph G=(V, E).<br><strong>Output</strong>: A 3-coloring c[1…n] of the vertices of G, where each c[j] is 1, 2, or 3.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> for k←1 to n</span><br><span class="line">     c[k]←0;</span><br><span class="line"> end for;</span><br><span class="line"> flag←false;</span><br><span class="line"> graphcolor(1);</span><br><span class="line"> if flag then output c;</span><br><span class="line"> else output “no solution”;</span><br><span class="line"></span><br><span class="line">graphcolor(k)</span><br><span class="line"> for color=1 to 3</span><br><span class="line">     c[k]←color;</span><br><span class="line">     if c is a legal coloring then set flag ←true and exit;</span><br><span class="line">     else if c is partial then graphcolor(k+1);</span><br><span class="line"> end for;</span><br></pre></td></tr></table></figure><br><strong>Iterative Algorithm</strong><br><strong>Input</strong>: An undirected graph G=(V, E).<br><strong>Output</strong>: A 3-coloring c[1…n] of the vertices of G, where each c[j] is 1, 2, or 3.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for k←1 to n</span><br><span class="line">    c[k]←0;</span><br><span class="line">end for;</span><br><span class="line">flag←false;</span><br><span class="line">k←1;</span><br><span class="line">while k&gt;=1</span><br><span class="line">    while c[k]&lt;=2</span><br><span class="line">        c[k]←c[k]+1;</span><br><span class="line">        if c is a legal coloring then set flag←true and exit from the two while loops;</span><br><span class="line">      else if c is partial then k←k+1;</span><br><span class="line">   end while;</span><br><span class="line">   c[k]←0;</span><br><span class="line">   k←k-1;</span><br><span class="line">end while;</span><br><span class="line">if flag then output c;</span><br><span class="line">else output “no solution”;</span><br></pre></td></tr></table></figure></p>
<h2 id="八皇后-8-Queens-Problem"><a href="#八皇后-8-Queens-Problem" class="headerlink" title="八皇后 8-Queens Problem"></a>八皇后 8-Queens Problem</h2><p>How can we arrange 8 queens on an 8x8 chessboard so that no two queens can attack each other?<br>Two queens can attack each other if they are in the same row, column or diagonal.<br>The n-queens problem is defined similarly, where in this case we have n queens and an nxn chessboard for an arbitrary value of $n\geq 1$.</p>
<p>Consider a chessboard of size 4x4. Since no two queens can be put in the same row, each queen is in a different row. Since there are four positions in each row, there are $4^4$ possible configurations.<br>Each possible configuration can be described by a vector with four components x=(x1, x2, x3, x4).<br>For example, the vector (2, 3, 4, 1) corresponds to a configuration.</p>
<p><strong>Input</strong>: none;<br><strong>Output</strong>: A vector x[1…4] corresponding to the solution of the 4-queens problem.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for k←1 to 4</span><br><span class="line">    x[k]←0;</span><br><span class="line">end for;</span><br><span class="line">flag←false;</span><br><span class="line">k←1;</span><br><span class="line">while k&gt;=1</span><br><span class="line">    while x[k]&lt;=3</span><br><span class="line">        x[k]←x[k]+1;</span><br><span class="line">        if x is a legal placement then set flag←true and exit from the two while loops;</span><br><span class="line">      else if x is partial then k←k+1;</span><br><span class="line">   end while;</span><br><span class="line">   x[k]←0;</span><br><span class="line">   k←k-1;</span><br><span class="line">end while;</span><br><span class="line">if flag then output x;</span><br><span class="line">else output “no solution”;</span><br></pre></td></tr></table></figure></p>
<h2 id="The-General-Backtracking-Method"><a href="#The-General-Backtracking-Method" class="headerlink" title="The General Backtracking Method"></a>The General Backtracking Method</h2><p>The general backtracking algorithm can be described as a systematic search method that can be applied to a class of search problems whose solution consists of a vector (x1, x2, … xi) satisfying some predefined constraints. Here, i is dependent on the problem formulation. In 3-Coloring and the 8-queens problems, i was fixed.<br>In some problems, i may vary from one solution to another.</p>
<p>Consider a variant of the PARTITION problem defined as follows. Given a set of n integers X={x1, x2, …, xn} and an integer y, find a subset Y of X whose sum is equal to y.<br>For instance if X={10, 20, 30, 40, 50, 60}, and y=60, then there are three solutions of different lengths: {10, 20, 30}, {20, 40}, and {60}.<br>Actually, this problem can be formulated in another way so that the solution is a boolean vector of length n in the obvious way. The above three solutions may be expressed by the boolean vectors {1, 1, 1, 0, 0, 0}, {0, 1, 0, 1, 0, 0}, and {0, 0, 0, 0, 0, 1}.</p>
<p>In backtracking, each xi in the solution vector belongs to a finite linearly ordered set Xi. Thus, the backtracking algorithm considers the elements of the cartesian product $X_1\times X_2\times…X_n$ in lexicographic order.<br>Initially, the algorithm starts with the empty vector. It then chooses the least element of X1 as x1. If (x1) is a partial solution, then algorithm proceeds by choosing the least element of X2 as x2. If (x1, x2) is a partial solution, then the least element of X3 is included; otherwise x2 is set to the next element in X2.<br>In general, suppose that the algorithm has detected the partial solution (x1, x2, …, xj). It then considers the vector v=(x1, x2, …, xj, xj+1). We have the following cases:</p>
<ol>
<li>If v represents a final solution to the problem, the algorithm records it as a solution and either terminates in case only one solution is desired or continues to find other solutions.</li>
<li>If v represents a partial solution, the algorithm advances by choosing the least element in the set Xj+2.</li>
<li>If v is neither a final nor a partial solution, we have two subcases:<ol>
<li>If there are still more elements to choose from in the set Xj+1, the algorithm sets xj+1 to the next member of Xj+1.</li>
<li>If there are no more elements to choose from in the set Xj+1, the algorithm backtracks by setting xj to the next member of Xj. If again there are no more elements to choose from in the set Xj, the algorithm backtracks by setting xj-1 to the next member of Xj-1, and so on.</li>
</ol>
</li>
</ol>
<h2 id="分枝界限-Branch-and-Bound-TSPs"><a href="#分枝界限-Branch-and-Bound-TSPs" class="headerlink" title="分枝界限 Branch and Bound (TSPs)"></a>分枝界限 Branch and Bound (TSPs)</h2><p>Branch and bound design technique is similar to backtracking in the sense that it generates a search tree and looks for one or more solutions.<br>However, while backtracking searches for a solution or a set of solutions that satisfy certain properties (including maximization or minimization), branch-and-bound algorithms are typically concerned with only maximization or minimization of a given function.<br>Moreover, in branch-and-bound algorithms, a bound is calculated at each node x on the possible value of any solution given by nodes that may later be generated in the subtree rooted at x. If the bound calculated is worse than the previous bound, the subtree rooted at x is blocked.</p>
<p>Henceforth, we will assume that the algorithm is to minimize a given cost function; the case of maximization is similar. In order for branch and bound to be applicable, the cost function must satisfy the following property.<br>For all partial solutions (x1, x2, …, xk-1) and their extensions (x1, x2, …, xk), we must have</p>
<script type="math/tex; mode=display">
cost(x_1, x_2, …, x_{k-1})\leq cost(x_1, x_2, …, x_k)</script><p>Given this property, a partial solution (x1, x2, …, xk) can be discarded once it is generated if its cost is greater than or equal to a previously computed solution.<br>Thus, if the algorithm finds a solution whose cost is c, and there is a partial solution whose cost is at least c, no more extensions of this partial solution are generated.</p>
<p><strong>Traveling Salesman Problems (TSPs):</strong><br>Given a set of cities and a cost function that is defined on each pair of cities, find a tour of minimum cost. Here a tour is a closed path that visits each city exactly once. The cost function may be the distance, travel time, air fare, etc.<br>An instance of the TSP is given by its cost matrix whose entries are assumed to be nonnegative.<br><img src="/img/algorithm_review/pic7.png" alt=""><br>With each partial solution (x1, x2, …, xk), we associate a lower bound y, which is the cost of any complete tour that visits the cities x1, x2, …, xk in this order must be at least y.<br><strong>Observations:</strong><br>We observe that each complete tour must contain exactly one edge and its associated cost from each row and each column of the cost matrix.<br>We also observe that if a constant r is subtracted from every entry in any row or column of the cost matrix A, the cost of any tour under the new matrix is exactly r less than the cost of the same tour under A. This motivates the idea of reducing the cost matrix so that each row or column contains at least one entry that is equal to 0. We will refer to such a matrix as the reduction of the original matrix.</p>
<p>Let (r1, r2, …, rn) and (c1, c2, …, cn) be the amounts subtracted from rows 1 to n and columns 1 to n, respectively, in an nxn cost matrix A. Then, y defined as follow is a lower bound on the cost of any complete tour.</p>
<script type="math/tex; mode=display">
y=\sum_{i=1}^{n}r_i+\sum_{i=1}^{n}c_i</script><h1 id="随机-Randomized-Algorithms"><a href="#随机-Randomized-Algorithms" class="headerlink" title="随机 Randomized Algorithms"></a>随机 Randomized Algorithms</h1><p>Based on the probabilistic notion of accuracy. </p>
<p>One form of algorithm design in which we relax the condition that an algorithm must solve the problem correctly for all possible inputs, and demand that its possible incorrectness is something that can safely be ignored due to its very low likelihood of occurrence.<br>Also, we will not demand that the output of an algorithm must be the same in every run on a particular input.</p>
<p>A <strong>randomized algorithm</strong> can be defined as one that receives, in addition to its input, a stream of random bits that it can use in the course of its action for the purpose of making random choices.<br>A randomized algorithm may give different results when applied to the same input in different rounds. It follows that the execution time of a randomized algorithm may vary from one run to another when applied to the same input.</p>
<p>Randomized algorithms can be classified into two categories:<br>The first category is referred to as <strong>Las Vegas</strong> algorithms. It constitutes those randomized algorithms that always give a correct answer, or do not give an answer at all.<br>The second category is referred to as <strong>Monte Carlo</strong> algorithms. It always gives an answer, but may occasionally produce an answer that is incorrect. However, the probability of producing an incorrect answer can be make arbitrarily small by running the algorithm repeatedly with independent random choices in each run.</p>
<h2 id="Randomized-Selection"><a href="#Randomized-Selection" class="headerlink" title="Randomized Selection"></a>Randomized Selection</h2><p><strong>Input</strong>: An array A[1…n] of n elements and an integer k, $1\leq k\leq n$;<br><strong>Output</strong>: The kth smallest element in A;<br>1.rselect(A, 1, n, k);<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rselect(A, low, high, k)</span><br><span class="line"> v←random(low, high);</span><br><span class="line"> x←A[v];</span><br><span class="line"> Partition A[low…high] into three arrays: </span><br><span class="line">    A1=&#123;a|a&lt;x&#125;, A2=&#123;a|a=x&#125;, A3=&#123;a|a&gt;x&#125;;</span><br><span class="line"> case</span><br><span class="line">    |A1|&gt;=k: return select (A1, 1, |A1|, k);</span><br><span class="line">    |A1|+|A2|&gt;=k: return x;</span><br><span class="line">    |A1|+|A2|&lt;k: return select(A3, 1, |A3|, k-|A1|-|A2|);</span><br><span class="line"> end case;</span><br></pre></td></tr></table></figure></p>
<h2 id="Testing-String-Equality"><a href="#Testing-String-Equality" class="headerlink" title="Testing String Equality"></a>Testing String Equality</h2><p>Suppose that two parties A and B can communicate over a communication channel, which we will assume to be very reliable. A has a very long string x and B has a very long string y, and they want to determine whether x=y.</p>
<p>Obviously, A can send x to B, who in turn can immediately test whether x=y. But this method would be extremely expensive, in view of the cost of using the channel.</p>
<p>Another alternative would be for A to derive from x a much shorter string that could serve as a “fingerprint” of x and send it to B.<br>B then would use the same derivation to obtain a fingerprint for y, and then compare the two fingerprints.<br>If they are equal, then B would assume that x=y; otherwise he would conclude that $x\ne y$. B than notifies A of the outcome of the test.<br>This method requires that transmission of a much shorter string across the channel.</p>
<p>For a string w, let I(w) be the integer represented by the bit string w. One method of fingerprinting is to choose a prime number p and then use the fingerprint function</p>
<script type="math/tex; mode=display">
I_p(x)=I(x) (mod\ p)</script><p>If p is not too large, then the fingerprint Ip(x) can be sent as a short string. If $I_p(x)\ne I_p(y)$, then obviously $x\ne y$. However, the converse is not true. That is, if Ip(x)=Ip(y), then it is not necessarily the case that x=y. We refer to this phenomenon as a <strong>false match</strong>.<br>In general, a false match occurs if $x\ne y$, but $I_p(x)=I_p(y)$, i.e., p divides $I(x)-I(y)$.</p>
<p>The weakness of this method is that, for fixed p, there are certain pairs of strings x and y on which the method will always fail. <strong>Then, what’s the probability?</strong></p>
<p>Let n be the number of bits of the binary strings of x and y, and p be a prime number which is smaller than $2n^2$. The probability of false matching is 1/n.</p>
<p>Thus, we choose p at random every time the equality of two strings is to be checked, rather than agreeing on p in advance. Moreover, choosing p at random allows for resending another fingerprint, and thus increasing the confidence in the case x=y.</p>
<ol>
<li>A chooses p at random from the set of primes less than M.</li>
<li>A sends p and Ip(x) to B.</li>
<li>B checks whether Ip(x)=Ip(y) and confirms the equality or inequality of the two strings x and y.</li>
</ol>
<h2 id="Pattern-Matching"><a href="#Pattern-Matching" class="headerlink" title="Pattern Matching"></a>Pattern Matching</h2><p>Given a string of text $X=x_1x_2…x_n$ and a pattern $Y=y_1y_2…y_m$, where $m\leq n$, determine whether or not the pattern appears in the text. Without loss of generality, we will assume that the text alphabet is $\sum={0, 1}.$<br>The most straightforward method for solving this problem is simply to move the pattern across the entire text, and in every position compare the pattern with the portion of the text of length m.<br>This brute-force method leads to an $O(mn)$ running time in the worst case. </p>
<p>Here we will present a simple and efficient Monte Carlo algorithm that achieves a running time of $O(n+m)$.<br>The algorithm follows the same brute-force algorithm of sliding the pattern Y across the text X, but instead of comparing the pattern with each block $X(j)=x<em>jx</em>{j+1}…x_{j+m-1}$, we will compare the fingerprint $I_p(Y)$ of the pattern with the fingerprints $I_p(X(j))$ of the blocks of text.</p>
<p>The key observations that when we shift from one block of text to the text, the fingerprint of the new block X(j+1) can easily be computed from the fingerprint of X(j).</p>
<script type="math/tex; mode=display">
I_p(X(j+1))=(2I_p(X(j))-2^mx_j+x_{j+m}) (mod\ p)</script><p>If we let $W_p=2^m$ (mod p), then we have the recurrence</p>
<script type="math/tex; mode=display">
I_p(X(j+1))=(2I_p(X(j))-W_px_j+x_{j+m}) (mod\ p)</script><p><strong>Input</strong>: A string of text X and a pattern Y of length n and m, respectively.<br><strong>Output</strong>: The first position of Y in X if Y occurs in X; otherwise 0.<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Choose p at random from the set of primes less than M;</span><br><span class="line">j←1;</span><br><span class="line">Compute Wp=2^m (mod p), Ip(Y) and Ip(Xj);</span><br><span class="line">while j&lt;=n-m+1</span><br><span class="line">    if Ip(Xj)=Ip(Y) then return j ; \\A match is found (probably)</span><br><span class="line">    Compute Ip(Xj) using previous equation;</span><br><span class="line">    j←j+1;</span><br><span class="line">end while;</span><br><span class="line">return 0; //Y does not occur in X (definitely)</span><br></pre></td></tr></table></figure><br>The time complexity of the previous pattern matching algorithm is O(m+n).<br>Let p be a prime number which is smaller than 2mn2. The probability of false matching is 1/n.</p>
<p>To convert the algorithm into a Las Vegas algorithm is easy.<br>Whenever the two fingerprints Ip(Y) and Ip(X(j)) match, the two strings are tested for equality.<br>Thus, we end with an efficient pattern matching algorithm that always gives the correct result, and the time complexity is still O(m+n).</p>
<h1 id="逼近-Approximation-Algorithms"><a href="#逼近-Approximation-Algorithms" class="headerlink" title="逼近 Approximation Algorithms"></a>逼近 Approximation Algorithms</h1><p>Compromise on the quality of solution in return for faster solutions.</p>
<p>There are many hard combinatorial optimization problems that cannot be solved efficiently using backtracking or randomization.<br>An alternative in this case for tacking some of these problems is to devise an approximation algorithm, given that we will be content with a “reasonable” solution that approximates an optimal solution.</p>
<p>Associated with each approximation algorithm, there is a performance bound that guarantees that the solution to a given instance will not be far away from the neighborhood of the exact solution.<br>A marking characteristic of (most of) approximation algorithms is that they are fast, as they are mostly greedy algorithm.</p>
<p>A combinatorial optimization problem $\Pi$ is either a minimization problem or a maximization problem. It consists of three components:<br>(1) A set $D<em>\Pi$ of instances.<br>(2) For each instance $I\in D</em>\Pi$, there is a finite set $S<em>\Pi(I)$ of candidate solutions for I.<br>(3) Associated with each solution $\sigma\in S</em>\Pi(I)$ to an instance I in $D<em>\Pi$, there is a value $f</em>\Pi(\sigma)$ called the solution value for $\sigma$.</p>
<p>If $\Pi$ is a minimization problem, then an optimal solution $\sigma’$ for an instance $I\in D<em>\Pi$ has the property that for all $\sigma\in S</em>\Pi(I)$, $f<em>\Pi(\sigma’)\leq f</em>\Pi(\sigma)$. An optimal solution for a maximization problem is defined similarly. We will denote by OPT(I) the value $f<em>\Pi(\sigma<em>)$.<br>An <em>*approximation algorithm</em></em> A for an optimization problem $\Pi$ is a (polynomial time) algorithm such that given an instance $I\in D</em>\Pi$, it outputs some solution $\sigma\in S<em>\Pi(I)$. We will denote by $A(I)$ the value $f</em>\Pi(\sigma)$.</p>
<h2 id="Difference-Bounds"><a href="#Difference-Bounds" class="headerlink" title="Difference Bounds"></a>Difference Bounds</h2><p>The most we can hope from an approximation algorithm is that the difference between the value of the optimal solution and the value of the solution obtained by the approximation algorithm is always constant.</p>
<p>In other words, for all instances I of the problem, the most desirable solution that can be obtained by an approximation algorithm A is such that $|A(I)-OPT(I)|\leq K$, for some constant K. </p>
<h3 id="Planar-Graph-Coloring"><a href="#Planar-Graph-Coloring" class="headerlink" title="Planar Graph Coloring"></a>Planar Graph Coloring</h3><p>Let G=(V, E) be a planar graph. By the Four Color Theorem, every planar graph is four-colorable. It is fairly easy to determine whether a graph is 2-colorable or not. On the other hand, to determine whether it is 3-colorable is NP-complete.</p>
<p>Given an instance I of G, an approximation algorithm A may proceed as follows:<br>Assume G is nontrivial, i.e. it has at least one edge. Determine if the graph is 2-colorable. If it is, then output 2; otherwise output 4. If G is 2-colorable, then $|A(I)-OPT(I)|=0$. If it is not 2-colorable, then $|A(I)-OPT(I)\leq 1$. This is because in the latter case, G is either 3-colorable or 4-colorable.</p>
<h3 id="Counterexample-Knapsack-Problems"><a href="#Counterexample-Knapsack-Problems" class="headerlink" title="Counterexample: Knapsack Problems"></a>Counterexample: Knapsack Problems</h3><p>There is no approximation algorithms with difference bounds for knapsack problems.</p>
<h2 id="Relative-Performance-Bounds"><a href="#Relative-Performance-Bounds" class="headerlink" title="Relative Performance Bounds"></a>Relative Performance Bounds</h2><p>Clearly, a difference bound is the best bound guaranteed by an approximation algorithm.<br>However, it turns out that very few hard problems possess such a bound. So we will discuss another performance guarantee, namely the relative performance guarantee. </p>
<p>Let $\Pi$ be a minimization problem and I an instance of $\Pi$. Let A be an approximation algorithm to solve $\Pi$. We define the approximation ratio $R_A(I)$ to be</p>
<script type="math/tex; mode=display">
R_A(I)=\frac{A(I)}{OPT(I)}</script><p>If $\Pi$ is a maximization problem, then we define $R_A(I)$ to be </p>
<script type="math/tex; mode=display">
R_A(I)=\frac{OPT(I)}{A(I)}</script><p>Thus the approximation ratio is always greater than or equal to one.</p>
<h3 id="The-Bin-Packing-Problem"><a href="#The-Bin-Packing-Problem" class="headerlink" title="The Bin Packing Problem"></a>The Bin Packing Problem</h3><p>Given a collection of items u1, u2, …, un of sizes s1, s2, …, sn, where such sj is between 0 and 1, we are required to pack these items into the minimum number of bins of unit capacity.</p>
<p>We list here one heuristic method:<br><strong>First Fit (FF):</strong> The bins are indexed as 1, 2, … All bins are initially empty. The items are considered for packing in the order u1, u2, …, un. To pack item ui, find the least index j such that bin j contains at most 1-si, and add item ui to the items packed in bin j. Then, we have</p>
<script type="math/tex; mode=display">
R_{FF}(I)=\frac{FF(I)}{OPT(I)}<2</script><h1 id="网络流-Network-Flow"><a href="#网络流-Network-Flow" class="headerlink" title="网络流 Network Flow"></a>网络流 Network Flow</h1><p>A network is a 4-tuple (G, s, t, c), where G=(V, E) is a directed graph, s and t are two distinguished vertices called, respectively, the source and sink, and c(u, v) is a capacity function defined on all pairs of vertices with c(u, v)&gt;0 if (u, v) ∈ E and c(u, v)=0 otherwise. |V|=n, |E|=m.</p>
<p>A flow in G is a real-valued function f on vertex pairs having the following four conditions:<br>(1) <strong>Skew symmetry.</strong> $\forall u, v\in V, f(u, v)=-f(v, u)$. We say there is a flow from u to v if $f(u, v)&gt;0$.<br>(2) <strong>Capacity constraints.</strong> $\forall u, v\in V, f(u, v)\leq c(u, v)$. We say edge (u, v) is saturated if $f(u, v)=c(u, v)$.<br>(3) <strong>Flow conservation.</strong> $\forall u\in V-{s, t}, \sum_{v\in V}f(u, v)=0$. In other words, the net flow (total flow out minus total flow in) at any interior vertex is 0.<br>(4) $\forall v\in V, f(v, v)=0$.</p>
<p>A cut {S, T} is a partition of the vertex set V into two subsets S and T such that $s\in S$ and $t\in T$. The capacity of the cut {S, T}, denoted by c(S, T), is</p>
<script type="math/tex; mode=display">
c(S,T)=\sum_{u\in S,v\in T}c(u,v)</script><p>The flow across the cut {S, T}, denoted by f(S, T), is</p>
<script type="math/tex; mode=display">
f(S,T)=\sum_{u\in S,v\in T}f(u,v)</script><p>Thus, the flow across the cut {S, T} is the sum of the positive flow on edges from S to T minus the sum of the positive flow on edges from T to S.</p>
<p>For any vertex u and any subset $A\subseteq V$, let f(u, A) denote f({u}, A), and f(A, u) denote f(A, {u}). For a capacity function c, c(u, A) and c(A, u) are defined similar.<br>The <strong>value of a flow f</strong>, denoted by |f|, is defined to be </p>
<script type="math/tex; mode=display">
|f|=f(s,v)=\sum_{v\in V}f(s,v)</script><p><strong>Lemma</strong>: For any cut {S, T} and a flow f, |f|=f(S, T).<br><strong>Max-Flow Problems</strong>: Design a function f for the network (G, s, t, c), so that |f| is the maximum.</p>
<p>Given a flow f on G with capacity function c, the <strong>residual capacity function</strong> for f on the set of pairs of vertices is defined as follows.<br>For each pair of vertices, $u, v\subseteq V$, the residual capacity $r(u, v)=c(u, v)-f(u, v)$. The <strong>residual graph</strong> for the flow f is the directed graph $R=(V, E_f)$, with capacities defined by r and </p>
<script type="math/tex; mode=display">
E_f=\{(u, v)|r(u, v)>0\}</script><p>The residual capacity r(u, v) represents the amount of additional flow that can be pushed along the edge (u, v) without violating the capacity constraints.<br>If $f(u, v)&lt;c(u, v)$, then both (u, v) and (v, u) are present in R. If there is no edge between u and v in G, then neither (u, v) nor (v, u) are in $E_f$. Thus, $|E_f|\leq 2|E|$.</p>
<p><strong>Example:</strong> what’s the residual graph of the following graph?<br><img src="/img/algorithm_review/pic8.png" alt=""><br>Let f and f’ be any two flows in a network G. Define the function f+f’ by (f+f’)(u, v)=f(u, v)+f’(u, v) for all pairs of vertices u and v. Similarly, define the function f-f’ by (f-f’)(u, v)=f(u, v)-f’(u, v).<br><strong>Lemma</strong>: Let f be a flow in G and f’ the flow in the residual graph R for f. Then the function f+f’ is a flow in G of value |f|+|f’|.<br><strong>Lemma</strong>: Let f be any flow in G and f* a maximum flow in G. If R is the residual graph for f, then the value of a maximum flow in R is |f*|-|f|.</p>
<p>Given a flow f in G, an <strong>augmenting path</strong> p is a directed path from s to t in the residual graph R. The <strong>bottleneck capacity</strong> of p is the minimum residual capacity along p. The number of edges in p will be denoted by |p|.<br><strong>Theorem (max-flow min-cut theorem)</strong>: Let (G, s, t, c) be a network and f a flow in G. The following three statements are equivalent:<br>(a) There is a cut {S, T} with c(S, T)=|f|.<br>(b) f is a maximum flow in G.<br>(c) There is no augmenting path for f.</p>
<p><strong>The Ford-Fulkerson Method</strong>:<br>The previous theorem suggests a way to construct a maximum flow by iterative improvement<br>One keeps finding an augmenting path arbitrarily and increases the flow by its bottleneck capacity</p>
<p><strong>Input</strong>: A network (G, s, t, c);<br><strong>Output</strong>: A flow in G;<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Initialize the residual graph: R←G;</span><br><span class="line">for each edge (u, v)∈E</span><br><span class="line">    f(u, v)←0;</span><br><span class="line">end for;</span><br><span class="line">while there is an augmenting path p=s, …, t in R</span><br><span class="line">    Let ▲ be the bottleneck capacity of p;</span><br><span class="line">    for each edge (u, v) in p</span><br><span class="line">        f(u, v) ←f(u, v)+ ▲;</span><br><span class="line">    end for;</span><br><span class="line">  Update the residual graph R;</span><br><span class="line">end while;</span><br></pre></td></tr></table></figure><br>The time complexity of the Ford-Fulkerson Method is O(m|f*|), where m is the number of edges and |f*| is the value of the maximum flow, which is an integer.</p>
<h2 id="Shortest-Path-Augmentation"><a href="#Shortest-Path-Augmentation" class="headerlink" title="Shortest Path Augmentation:"></a>Shortest Path Augmentation:</h2><p>Here we consider another heuristic that puts some order on the selection of augmenting paths.<br>Definition: The level of a vertex v, denoted by level(v), is the least number of edges in a path from s to v. Given a directed graph G=(V, E), the level graph L is (V’, E’), where V’ is the set of vertices can be reached from s and </p>
<script type="math/tex; mode=display">
E'=\{(u, v) | level(v)=level(u)+1\ and\ (u, v)\in E\}.</script><p>Given a directed graph G and a source vertex s, its level graph L can easily be constructed using breadth-first search.</p>
<h2 id="MPLA-Minimum-path-length-augmentation"><a href="#MPLA-Minimum-path-length-augmentation" class="headerlink" title="MPLA | Minimum path length augmentation"></a>MPLA | Minimum path length augmentation</h2><p>Minimum path length augmentation (MPLA)<br>MPLA selects an augmenting path of minimum length and increases the current flow by an amount equal to the bottleneck capacity of the path. The algorithm starts by initializing the flow to the zero flow and setting the residual graph R to the original network. It then proceeds in phases. Each phase consists of the following two steps:<br>(1) Compute the level graph L from the residual graph R. If t is not in L, then halt; otherwise continue.<br>(2) As long as there is a path p from s to t in L, augment the current flow by p, remove saturated edges from L and R and update them accordingly.</p>
<p><strong>Input</strong>: A network (G, s, t, c);<br><strong>Output</strong>: The maximum flow in G;<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for each edge (u, v)∈ E</span><br><span class="line">    f(u, v)←0;</span><br><span class="line">end for;</span><br><span class="line">Initialize the residual graph: R ← G;</span><br><span class="line">Find the level graph L of R;</span><br><span class="line">while t is a vertex in L</span><br><span class="line">    while t is reachable from s in L</span><br><span class="line">        Let p be a path from s to t in L;</span><br><span class="line">        Let ▲ be the bottleneck capacity on p;</span><br><span class="line">      Augment the current flow f by ▲;</span><br><span class="line">      Update L and R along the path p;</span><br><span class="line">   end while;</span><br><span class="line">   Use the residual graph R to compute a new level graph L;</span><br><span class="line">end while;</span><br></pre></td></tr></table></figure></p>
<h1 id="匹配-Matching"><a href="#匹配-Matching" class="headerlink" title="匹配 Matching"></a>匹配 Matching</h1><p>Given an undirected graph G=(V, E), |V|=n, and |E|=m. A <strong>matching</strong> in G is a subset $M\subseteq E$ such that no two edges in M have a vertex in common.<br>An edge $e\in E$ is <strong>matched</strong> if it is in M, and <strong>unmatched</strong> or <strong>free</strong> otherwise.<br>A vertex $v\in E$ is <strong>matched</strong> if it is incident to a matched edge, and <strong>unmatched</strong> or <strong>free</strong> otherwise.<br>The size of a matching M, i.e. the number of matching edges in it, will be denoted by |M|.<br><strong>A maximum matching</strong> in a graph is a matching of maximum cardinality. A <strong>perfect matching</strong> is one in which every vertex in V is matched.</p>
<p>The maximum matching problem asks for a subset $M\subseteq E$ with the maximum number of nonoverlapping edges; that is, no two edges in M have a vertex in common.<br>This problem arises in many applications, particularly in the areas of communication and scheduling.</p>
<p>Given a matching M in an undirected graph G=(V, E), an <strong>alternating path</strong> p with respect to M is a simple path that consists of alternating matched and unmatched edges. The length of p is denoted by |p|.<br>If the two endpoints of an alternating path coincide, then it is called an <strong>alternating cycle</strong>.<br>An alternating path with respect to M is called an <strong>augmenting path</strong> with respect to M if all the matched edges in p are in M and its endpoints are free.</p>
<p>Let M1 and M2 be two matchings in a graph G. Then</p>
<script type="math/tex; mode=display">
\begin{equation} 
\begin{split} 
M1\oplus M2 
&= (M1\cup M2)-(M1\cap M2) \\
&= (M1 - M2)\cup (M2 - M1)
\end{split} 
\end{equation}</script><p>That is, $M1\oplus M2$ is the set of edges that are in M1 or in M2 but not in both.<br><strong>Lemma</strong>: Let M be a matching and p an augmenting path with respect to M, then $M\oplus p$ is a matching  of size $|M\oplus p|=|M|+1$.<br><strong>Corollary</strong>: A matching M in an undirected graph G is maximum if and only if G contains no augmenting paths with respect to M.</p>
<p><strong>Theorem</strong>: Let M1 and M2 be two matchings in an undirected G such that |M1|=r, |M2|=s and s&gt;r. Then, $M1\oplus M2$ contains at least k=s-r vertex-disjoint augmenting paths with respect to M1.</p>
<h2 id="二分图的匈牙利树方法-Hungarian-Tree-Method-for-Bipartite-Graphs"><a href="#二分图的匈牙利树方法-Hungarian-Tree-Method-for-Bipartite-Graphs" class="headerlink" title="二分图的匈牙利树方法 Hungarian Tree Method for Bipartite Graphs"></a>二分图的匈牙利树方法 Hungarian Tree Method for Bipartite Graphs</h2><p>The previous Lemma and Corollary suggest a procedure for finding a maximum matching in G:<br>Starting from an arbitrary (e.g. empty) matching, we find an augmenting path p in G, invert the roles of the edges in p (matched to unmatched and vice-versa), and repeat the process until there are no more augmenting paths.<br>However, to find an augmenting path efficiently in a general graph is not easy.</p>
<p>Given an undirected graph G=(V, E), if V can be divided into two disjoint subsets X and Y so that each edge in E has an end in X and an end in Y, then G is a <strong>bipartite graph.</strong><br>The most important feature of a bipartite graph is it contains no cycles of odd length.<br>Finding an augmenting path in the case of bipartite graphs is much easier than in the case of general graphs.</p>
<p>Let $G=(X\cup Y, E)$ be a bipartite graph with |X|+|Y|=n and |E|=m. Let M be a matching in G. We call a vertex in X an x-vertex. Similarly, a y-vertex denotes a vertex in Y.<br>First, we pick a free x-vertex, say r, and label it <strong>outer</strong>. From r, we grow an <strong>alternating path tree</strong>, i.e., a tree in which each path from the root r to a leaf is an alternating path. This tree, call it T, is constructed as follows:</p>
<p>Starting from r, add each unmatched edge (r, y) connecting r to the y-vertex y and label y inner.<br>For each y-vertex y adjacent to r, add the matched edge (y, z) to T if such a matched edge exists, and label z outer.<br>Repeat the above procedure and extend the tree until either a free y-vertex is encountered or the tree is blocked, i.e., cannot be extended any more (note that no vertex is added to the tree more than once).</p>
<p>If a free y-vertex is found, say v, then the alternating path from the root r to v is an augmenting path. On the other hand, if the tree is blocked, then in this case the tree is called a <strong>Hungarian tree.</strong><br>Next, we start from another free x-vertex, if any, and repeat the above procedure.</p>
<p><strong>Observation</strong><br>If T is a Hungarian tree, then it cannot be extended; each alternating path traced from the root is stopped at some outer vertex.<br>The only free vertex in T is its root. Notice that if (x, y) is an edge such that x is in T and y is not in T, then x must be labeled inner. Otherwise, x must be connected to a free vertex or T is extendable through x.</p>
<p>It follows that no vertex in a Hungarian tree can occur in an augmenting path.<br>Suppose that p is an alternating path that shares at least one vertex with T. If p “enters” T, then it must be through a vertex labeled inner. If it “leaves” T, then it must also be through a vertex labeled as inner. But, then, p is not an alternating path; a contradiction.</p>
<p>Therefore, if, in the process of searching for an augmenting path, a Hungarian tree is found, then it can be removed permanently without affecting the search.</p>
<p><strong>Input</strong>: A bipartite graph $G=(X\cup Y, E)$;<br><strong>Output</strong>: A maximum matching M in G;<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Initialize M to any arbitrary (possibly empty) matching;</span><br><span class="line">while (there exists a free x-vertex and a free y vertex)</span><br><span class="line">    Let r be a free x-vertex, and using breadth-first</span><br><span class="line">       search, grow an alternating path tree T  rooted at r;</span><br><span class="line">    if (T is a Hungarian tree) then let G←G-T;</span><br><span class="line">    else (find an augmenting path p in T  and let</span><br><span class="line">       M=M $\oplus$ p);</span><br><span class="line">end while.</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>编程</tag>
      </tags>
  </entry>
</search>
