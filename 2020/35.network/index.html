<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">

<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"skystarry.cn","root":"/","images":"/images","scheme":"Mist","version":"8.1.0","exturl":true,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta name="description" content="数据介绍本次练习所用的数据集有5000个训练样本，每个样本对应于20x20大小的灰度图像。这些训练样本包括了9-0共十个数字的手写图像。这些样本中每个像素都用浮点数表示。加载得到的数据中，每幅图像都被展开为一个400维的向量，构成了数据矩阵中的一行。完整的训练数据是一个5000x400的矩阵，其每一行为一个训练样本（数字的手写图像）。数据中，对应于数字”0”的图像被标记为”10”，而数字”1”到”">
<meta property="og:type" content="article">
<meta property="og:title" content="使用神经网络进行机器学习">
<meta property="og:url" content="https://skystarry.cn/2020/35.network/index.html">
<meta property="og:site_name" content="skystarry">
<meta property="og:description" content="数据介绍本次练习所用的数据集有5000个训练样本，每个样本对应于20x20大小的灰度图像。这些训练样本包括了9-0共十个数字的手写图像。这些样本中每个像素都用浮点数表示。加载得到的数据中，每幅图像都被展开为一个400维的向量，构成了数据矩阵中的一行。完整的训练数据是一个5000x400的矩阵，其每一行为一个训练样本（数字的手写图像）。数据中，对应于数字”0”的图像被标记为”10”，而数字”1”到”">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://skystarry.cn/img/PRML/data-array.png">
<meta property="og:image" content="https://skystarry.cn/img/PRML/nn-representation.png">
<meta property="og:image" content="https://skystarry.cn/img/PRML/nn-backpropagation.png">
<meta property="og:image" content="https://skystarry.cn/img/PRML/output_23_1.png">
<meta property="article:published_time" content="2020-04-13T11:09:17.000Z">
<meta property="article:modified_time" content="2020-12-27T00:35:27.144Z">
<meta property="article:author" content="skystarry">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://skystarry.cn/img/PRML/data-array.png">


<link rel="canonical" href="https://skystarry.cn/2020/35.network/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>使用神经网络进行机器学习 | skystarry</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/rss_skystarry.xml" title="skystarry" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">skystarry</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">我的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">数据介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA"><span class="nav-number">2.</span> <span class="nav-text">模型表示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">前向传播与代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">4.</span> <span class="nav-text">代价函数的正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E5%8F%8D%E4%BC%A0%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95-Backpropagation"><span class="nav-number">5.</span> <span class="nav-text">误差反传训练算法 (Backpropagation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">6.</span> <span class="nav-text">神经网络的正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E5%8F%8D%E4%BC%A0%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95"><span class="nav-number">7.</span> <span class="nav-text">误差反传训练算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sigmoid-%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E6%A2%AF%E5%BA%A6"><span class="nav-number">7.1.</span> <span class="nav-text">Sigmoid 函数及其梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">7.2.</span> <span class="nav-text">网络参数的随机初始化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number">8.</span> <span class="nav-text">检查梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">9.</span> <span class="nav-text">加载数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9D%83%E9%87%8D"><span class="nav-number">10.</span> <span class="nav-text">加载神经网络模型的权重</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">11.</span> <span class="nav-text">神经网络参数初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">12.</span> <span class="nav-text">训练神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="nav-number">13.</span> <span class="nav-text">模型预测</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="skystarry"
      src="/img/skystarry.jpg">
  <p class="site-author-name" itemprop="name">skystarry</p>
  <div class="site-description" itemprop="description">Seek only light, freedom, and you.</div>
</div>

   <div class="feed-link motion-element">
     <a href="/rss_skystarry.xml" rel="alternate">
       <i class="fa fa-rss"></i>
       RSS
     </a>
   </div>
 
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9za3lzdGFycnkuZ2l0ZWUuaW8v" title="Wiki → https:&#x2F;&#x2F;skystarry.gitee.io&#x2F;">Wiki</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmxpY2h1bnBlbmcyMjAwQG91dGxvb2suY29t" title="E-Mail → mailto:lichunpeng2200@outlook.com">E-Mail</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9saS1jaHVucGVuZw==" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;li-chunpeng">知乎</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vMTc5NDE2OTY1" title="B站 → https:&#x2F;&#x2F;space.bilibili.com&#x2F;179416965">B站</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cDovL211c2ljLjE2My5jb20vIy91c2VyL2hvbWU/aWQ9MjcyMTA3MDQw" title="网抑云 → http:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;272107040">网抑云</span>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL1B1bmtMaQ==" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://skystarry.cn/2020/35.network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/skystarry.jpg">
      <meta itemprop="name" content="skystarry">
      <meta itemprop="description" content="Seek only light, freedom, and you.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="skystarry">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          使用神经网络进行机器学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-04-13 19:09:17" itemprop="dateCreated datePublished" datetime="2020-04-13T19:09:17+08:00">2020-04-13</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2020-12-27 08:35:27" itemprop="dateModified" datetime="2020-12-27T08:35:27+08:00">2020-12-27</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="数据介绍"><a href="#数据介绍" class="headerlink" title="数据介绍"></a>数据介绍</h2><p>本次练习所用的数据集有5000个训练样本，每个样本对应于20x20大小的灰度图像。这些训练样本包括了9-0共十个数字的手写图像。这些样本中每个像素都用浮点数表示。加载得到的数据中，每幅图像都被展开为一个400维的向量，构成了数据矩阵中的一行。完整的训练数据是一个5000x400的矩阵，其每一行为一个训练样本（数字的手写图像）。数据中，对应于数字”0”的图像被标记为”10”，而数字”1”到”9”按照其自然顺序被分别标记为”1”到”9”。数据集保存在<code>NN_data.mat</code>.<br><img src="/img/PRML/data-array.png" alt="image"></p>
<a id="more"></a>
<h2 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h2><p>我们准备训练的神经网络是一个三层的结构，一个输入层，一个隐层以及一个输出层。由于我们训练样本（图像）是20x20的，所以输入层单元数为400（不考虑额外的偏置项，如果考虑单元个数需要+1）。在我们的程序中，数据会被加载到变量<span> $X$ </span> 和<span> $y$ </span>里。</p>
<p>本项练习提供了一组训练好的网络参数<span> $(\Theta^{(1)}, \Theta^{(2)})$ </span>。这些数据存储在数据文件 <code>NN_weights.mat</code>，在程序中被加载到变量 <code>Theta1</code> 与 <code>Theta2</code> 中。参数的维度对应于第二层有25个单元、10个输出单元（对应于10个数字 的类别）的网络。</p>
<p><img src="/img/PRML/nn-representation.png" alt="image"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> fmin_cg</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_data</span>(<span class="params">data, img_width=<span class="number">20</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将图像数据 data 按照矩阵形式显示出来&quot;&quot;&quot;</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    <span class="comment"># 计算数据尺寸相关数据</span></span><br><span class="line">    n_rows, n_cols = data.shape</span><br><span class="line">    img_height = n_cols // img_width</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算显示行数与列数</span></span><br><span class="line">    disp_rows = <span class="built_in">int</span>(np.sqrt(n_rows))</span><br><span class="line">    disp_cols = (n_rows + disp_rows - <span class="number">1</span>) // disp_rows</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 图像行与列之间的间隔</span></span><br><span class="line">    pad = <span class="number">1</span></span><br><span class="line">    disp_array = np.ones((pad + disp_rows*(img_height + pad),</span><br><span class="line">                          pad + disp_cols*(img_width + pad)))</span><br><span class="line"></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(disp_rows):</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(disp_cols):</span><br><span class="line">            <span class="keyword">if</span> idx &gt; m:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 复制图像块</span></span><br><span class="line">            rb = pad + row*(img_height + pad)</span><br><span class="line">            cb = pad + col*(img_width + pad)</span><br><span class="line">            disp_array[rb:rb+img_height, cb:cb+img_width] = data[idx].reshape((img_height, -<span class="number">1</span>), order=<span class="string">&#x27;F&#x27;</span>)</span><br><span class="line">            <span class="comment"># 获得图像块的最大值，对每个训练样本分别归一化</span></span><br><span class="line">            max_val = np.<span class="built_in">abs</span>(data[idx].<span class="built_in">max</span>())</span><br><span class="line">            disp_array[rb:rb+img_height, cb:cb+img_width] /= max_val</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    plt.imshow(disp_array)</span><br><span class="line"></span><br><span class="line">    plt.gray()</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;data-array.png&#x27;</span>, dpi=<span class="number">150</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="前向传播与代价函数"><a href="#前向传播与代价函数" class="headerlink" title="前向传播与代价函数"></a>前向传播与代价函数</h2><p>现在你需要实现神经网络的代价函数及其梯度。首先需要使得函数 <code>nn_cost_function</code> 能够返回正确的代价值。</p>
<p>神经网络的代价函数（不包括正则化项）的定义为：</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left[-y_k^{(i)} \log\left((h_{\theta}(x^{(i)}))_k\right) -(1 - y_k^{(i)}) \log\left(1 - (h_{\theta}(x^{(i)}))_k\right) \right]</script><p>其中<span> $h_{\theta}(x^{(i)})$ </span> 的计算如神经网络结构图所示，<span> $K=10$ </span>是 所有可能的类别数。这里的<span> $y$ </span>使用了one-hot 的表达方式。</p>
<p>运行程序，使用预先训练好的网络参数，确认你得到的代价函数是正确的。（正确的代价约为0.287629）。</p>
<h2 id="代价函数的正则化"><a href="#代价函数的正则化" class="headerlink" title="代价函数的正则化"></a>代价函数的正则化</h2><p>神经网络包括正则化项的代价函数为: &lt;/br&gt;</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{m}\sum_{i=1}^{m} \sum_{k=1}^{K} \left[-y_k^{(i)} \log\left((h_{\theta}(x^{(i)}))_k\right) -(1 - y_k^{(i)}) \log\left(1 - (h_{\theta}(x^{(i)}))_k\right) \right] + \frac{\lambda}{2m} \left[\sum_{j=1}^{25} \sum_{k=1}^{400} (\Theta_{j,k}^{(1)})^2 +\sum_{j=1}^{10} \sum_{k=1}^{25} (\Theta_{j,k}^{(2)})^2 \right]</script><p>注意在上面式子中，正则化项的加和形式与练习中设定的网络结构一致。但是你的代码实现要保证能够用于任意大小的神经网络。<br>此外，还需要注意，对应于偏置项的参数不能包括在正则化项中。对于矩阵 <code>Theta1</code> 与 <code>Theta2</code> 而言，这些项对应于矩阵的第一列。</p>
<p>运行程序，使用预先训练好的权重数据，设置正则化系数$\lambda=1$ (<code>lmb</code>) 确认你得到的代价函数是正确的。（正确的代价约为0.383770）。</p>
<p>此步练习需要你补充实现 <code>nn_cost_function</code> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_to_one_hot</span>(<span class="params">y,c</span>):</span></span><br><span class="line">    v = np.eye(c+<span class="number">1</span>)[y.reshape(-<span class="number">1</span>)].T  <span class="comment"># 11是因为有个非常讨厌的编码10出现</span></span><br><span class="line">    v = np.delete(v, <span class="number">0</span>, axis = <span class="number">0</span>)    <span class="comment"># 编码后是11位制的，第一行都是0删掉</span></span><br><span class="line">    <span class="keyword">return</span> v.T</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_cost_function</span>(<span class="params">nn_params, *args</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;神经网络的损失函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Unpack parameters from *args</span></span><br><span class="line">    input_layer_size, hidden_layer_size, num_labels, lmb, X, y = args</span><br><span class="line">    <span class="comment"># Unroll weights of neural networks from nn_params</span></span><br><span class="line">    Theta1 = nn_params[:hidden_layer_size*(input_layer_size + <span class="number">1</span>)]</span><br><span class="line">    Theta1 = Theta1.reshape((hidden_layer_size, input_layer_size + <span class="number">1</span>))</span><br><span class="line">    Theta2 = nn_params[hidden_layer_size*(input_layer_size + <span class="number">1</span>):]</span><br><span class="line">    Theta2 = Theta2.reshape((num_labels, hidden_layer_size + <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 设置变量</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># You need to return the following variable correctly</span></span><br><span class="line">    J = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># ====================== 你的代码 ======================</span></span><br><span class="line">    a_1 = np.hstack((np.ones((m, <span class="number">1</span>)), X))   <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_2 = np.dot(Theta1, a_1.T) </span><br><span class="line">    a_2 = sigmoid(Z_2)</span><br><span class="line">    a_2 = a_2.T</span><br><span class="line">    a_2 = np.hstack((np.ones((m, <span class="number">1</span>)), a_2)) <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_3 = np.dot(Theta2, a_2.T)</span><br><span class="line">    hypothesis = sigmoid(Z_3)</span><br><span class="line">   </span><br><span class="line">    one_hot_y = convert_to_one_hot(y, num_labels)  <span class="comment"># 这里需要对y进行one-hot编码</span></span><br><span class="line"></span><br><span class="line">    Theta1_without_bias = np.delete(Theta1, <span class="number">0</span>, axis = <span class="number">1</span>) <span class="comment"># 删除 input add bias as 1 在第一列</span></span><br><span class="line">    Theta2_without_bias = np.delete(Theta2, <span class="number">0</span>, axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    Regular = lmb / (<span class="number">2</span>*m) * (np.square(Theta1_without_bias).<span class="built_in">sum</span>() </span><br><span class="line">                           + np.square(Theta2_without_bias).<span class="built_in">sum</span>() ) <span class="comment"># 正则项, 偏置项不要正则</span></span><br><span class="line">    loss = <span class="number">0.0</span>    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m): <span class="comment"># 样本数量</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_labels): <span class="comment"># 标签类别</span></span><br><span class="line">            loss += (-one_hot_y[i,j] * np.log(hypothesis[j,i]) - (<span class="number">1</span> - one_hot_y[i,j]) * np.log(<span class="number">1</span> - hypothesis[j,i]))</span><br><span class="line"></span><br><span class="line">    J = <span class="number">1</span>/m * loss + Regular</span><br><span class="line">    <span class="comment">#print(&#x27;current cost: &#x27;, J)</span></span><br><span class="line">    <span class="comment"># ======================================================</span></span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="误差反传训练算法-Backpropagation"><a href="#误差反传训练算法-Backpropagation" class="headerlink" title="误差反传训练算法 (Backpropagation)"></a>误差反传训练算法 (Backpropagation)</h2><p><img src="/img/PRML/nn-backpropagation.png" alt="image"></p>
<p>现在你需要实现误差反传训练算法。误差反传算法的思想大致可以描述如下。对于一个训练样本<span> $(x^{(t)}, y^{(t)})$ </span>，我们首先使用前向传播计算网络中所有单元（神经元）的激活值（activation），包括假设输出<span> $h_{\Theta}(x)$ </span>。那么，对于第<span> $l$ </span>层的第<span> $j$ </span>个节点，我们期望计算出一个“误差项”<span> $\delta_{j}^{(l)}$ </span>用于衡量该节点对于输出的误差的“贡献”。</p>
<p>对于输出节点，我们可以直接计算网络的激活值与真实目标值之间的误差。对于我们所训练的第3层为输出层的网络，这个误差定义了<span> $\delta_{j}^{(3)}$ </span>。对于隐层单元，需要根据第<span> $l+1$ </span>层的节点的误差的加权平均来计算<span> $\delta_{j}^{(l)}$ </span>。</p>
<p>下面是误差反传训练算法的细节（如图3所示）。你需要在一个循环中实现步骤1至4。循环的每一步处理一个训练样本。第5步将累积的梯度除以<span> $m$ </span>以得到神经网络代价函数的梯度。</p>
<ol>
<li>设输入层的值<span> $a^{(1)}$ </span>为第<span> $t$ </span>个训练样本<span> $x^{(t)}$ </span>。执行前向传播，计算第2层与第3层各节点的激活值(<span> $z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)}$ </span>)。注意你需要在<span> $a^{(1)}$ </span>与<span> $a^{(2)}$ </span>增加一个全部为 +1 的向量，以确保包括了偏置项。在 <code>numpy</code> 中可以使用函数 <code>ones</code> ， <code>hstack</code>, <code>vstack</code> 等完成（向量化版本）。</li>
<li><p>对第3层中的每个输出单元<span> $k$ </span>，计算</p>
<script type="math/tex; mode=display">\delta_{k}^{(3)} = a_{k}^{(3)} - y_k</script><p>其中<span> $y_k \in \{0, 1\}$ </span>表示当前训练样本是否是第<span> $k$ </span>类。</p>
</li>
<li><p>对隐层<span> $l=2$ </span>, 计算</p>
<script type="math/tex; mode=display">\delta^{(2)} = \left( \Theta^{(2)} \right)^T \delta^{(3)} .* g^{\prime} (z^{(2)})</script><p>其中$g^{\prime}$ 表示 Sigmoid 函数的梯度， <code>.*</code> 在 <code>numpy</code> 中是通 常的逐个元素相乘的乘法，矩阵乘法应当使用 <code>numpy.dot</code> 函数。</p>
</li>
<li><p>使用下式将当前样本梯度进行累加：</p>
<script type="math/tex; mode=display">\Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T</script><p>在 <code>numpy</code> 中，数组可以使用 <code>+=</code> 运算。</p>
</li>
<li><p>计算神经网络代价函数的（未正则化的）梯度，</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)}</script></li>
</ol>
<p>这里，你需要（部分）完成函数 <code>nn_grad_function</code> 。程序将使用函数 <code>check_nn_gradients</code> 来检查你的实现是否正确。在使用循环的方式完成函数 <code>nn_grad_function</code> 后，建议尝试使用向量化的方式重新实现这个函数。</p>
<h2 id="神经网络的正则化"><a href="#神经网络的正则化" class="headerlink" title="神经网络的正则化"></a>神经网络的正则化</h2><p>你正确实现了误差反传训练算法之后，应当在梯度中加入正则化项。</p>
<p>假设你在误差反传算法中计算了<span> $\Delta_{ij}^{(l)}$ </span>，你需要增加的正则化项为</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)} \qquad \text{for } j = 0
\frac{\partial}{\partial \Theta_{ij}^{(l)}} J(\Theta) = D_{ij}^{(l)} = \frac{1}{m} \Delta_{ij}^{(l)} + \frac{\lambda}{m} \Theta_{ij}^{(l)} \qquad \text{for } j \geq 1</script><p>注意你不应该正则化<span> $\Theta^{(l)}$ </span>的第一列，因其对应于偏置项。</p>
<p>此步练习需要你补充实现函数 <code>nn_grad_function</code> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_grad_function</span>(<span class="params">nn_params, *args</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;神经网络的损失函数梯度计算 &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获得参数信息</span></span><br><span class="line">    input_layer_size, hidden_layer_size, num_labels, lmb, X, y = args</span><br><span class="line">    <span class="comment"># 得到各个参数的权重值</span></span><br><span class="line">    Theta1 = nn_params[:hidden_layer_size*(input_layer_size + <span class="number">1</span>)]</span><br><span class="line">    Theta1 = Theta1.reshape((hidden_layer_size, input_layer_size + <span class="number">1</span>))</span><br><span class="line">    Theta2 = nn_params[hidden_layer_size*(input_layer_size + <span class="number">1</span>):]</span><br><span class="line">    Theta2 = Theta2.reshape((num_labels, hidden_layer_size + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置变量</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ====================== 你的代码 =====================</span></span><br><span class="line">    a_1 = np.hstack((np.ones((m, <span class="number">1</span>)), X)) <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_2 = np.dot(Theta1, a_1.T)</span><br><span class="line">    a_2 = sigmoid(Z_2)</span><br><span class="line">    a_2 = a_2.T</span><br><span class="line">    a_2 = np.hstack((np.ones((m, <span class="number">1</span>)), a_2)) <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_3 = np.dot(Theta2, a_2.T)</span><br><span class="line">    a_3 = sigmoid(Z_3)</span><br><span class="line">    a_3 = a_3.T</span><br><span class="line"></span><br><span class="line">    one_hot_y = convert_to_one_hot(y, num_labels) <span class="comment"># 这里需要对y进行one-hot编码</span></span><br><span class="line"></span><br><span class="line">    Delta_2 = np.zeros_like(Theta2)</span><br><span class="line">    Delta_1 = np.zeros_like(Theta1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m): <span class="comment"># 对每一个样本</span></span><br><span class="line">        delta_3 = a_3[i] - one_hot_y[i]</span><br><span class="line">        grad = np.dot(Theta2.T, delta_3)</span><br><span class="line">        part_grad = np.delete(grad, <span class="number">0</span>)    <span class="comment"># 把偏置项删掉, 我不确定是否应该这样做</span></span><br><span class="line">        delta_2 = part_grad * sigmoid_gradient(Z_2[:,i])</span><br><span class="line">        Delta_2 = Delta_2 + np.dot(delta_3.reshape(-<span class="number">1</span>, <span class="number">1</span>), np.matrix(a_2[i]))</span><br><span class="line">        Delta_1 = Delta_1 + np.dot(delta_2.reshape(-<span class="number">1</span>, <span class="number">1</span>), np.matrix(a_1[i]))</span><br><span class="line">    </span><br><span class="line">    Theta2_grad = <span class="number">1</span>/m * Delta_2 + lmb/m * Theta2</span><br><span class="line">    Theta1_grad = <span class="number">1</span>/m * Delta_1 + lmb/m * Theta1</span><br><span class="line">    <span class="comment"># =====================================================</span></span><br><span class="line">    </span><br><span class="line">    grad = np.hstack((Theta1_grad.flatten(), Theta2_grad.flatten()))</span><br><span class="line">    grad = np.array(grad).flatten() <span class="comment"># 如果不加这一行，会在optimize.py等多处报错，例如deltak = numpy.dot(gfk, gfk)，error：(dim 1) != 1 (dim 0)</span></span><br><span class="line">    <span class="keyword">return</span> grad</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="误差反传训练算法"><a href="#误差反传训练算法" class="headerlink" title="误差反传训练算法"></a>误差反传训练算法</h2><h3 id="Sigmoid-函数及其梯度"><a href="#Sigmoid-函数及其梯度" class="headerlink" title="Sigmoid 函数及其梯度"></a><code>Sigmoid</code> 函数及其梯度</h3><p>Sigmoid 函数定义为</p>
<script type="math/tex; mode=display">\text{sigmoid}(z) = g(z) = \frac{1}{1+\exp(-z)}</script><p>Sigmoid 函数的梯度可以按照下式进行计算</p>
<script type="math/tex; mode=display">g^{\prime}(z) = \frac{d}{dz} g(z) = g(z)(1-g(z))</script><p>为验证你的实现是正确的，以下事实可供你参考。当<span> $z=0$ </span>是，梯度的精确值为 0.25 。当<span> $z$ </span>的值很大（可正可负）时，梯度值接近于0。</p>
<p>这里，你需要补充完成函数 <code>sigmoid</code> 与 <code>sigmoid_gradient</code> 。 你需要保证实现的函数的输入参数可以为矢量和矩阵( <code>numpy.ndarray</code>)。</p>
<h3 id="网络参数的随机初始化"><a href="#网络参数的随机初始化" class="headerlink" title="网络参数的随机初始化"></a>网络参数的随机初始化</h3><p>训练神经网络时，使用随机数初始化网络参数非常重要。一个非常有效的随机初始化策略为，在范围<span> $[ -\epsilon_{init}, \epsilon_{init} ]$ </span>内按照均匀分布随机选择参数<span> $\Theta^{(l)}$ </span>的初始值。这里你需要设置<span> $\epsilon_{init} = 0.12$ </span>。这个范围保证了参数较小且训练过程高效。</p>
<p>你需要补充实现函数 <code>rand_initialize_weigths</code> 。</p>
<p>对于一般的神经网络，如果第<span> $l$ </span>层的输入单元数为<span> $L_{in}$ </span>，输出单元数为<span> $L_{out}$ </span>，则<span> $\epsilon_{init} = {\sqrt{6}}/{\sqrt{L_{in} + L_{out}}}$ </span>可以做为有效的指导策略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Sigmoid 函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-np.asarray(z)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算Sigmoid 函数的梯度&quot;&quot;&quot;</span></span><br><span class="line">    g = np.zeros_like(z)</span><br><span class="line">    <span class="comment"># ======================　你的代码 ======================</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算Sigmoid 函数的梯度g的值</span></span><br><span class="line">    g = sigmoid(z)*(<span class="number">1.0</span>-sigmoid(z))</span><br><span class="line">    <span class="comment"># =======================================================</span></span><br><span class="line">    <span class="keyword">return</span> g</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rand_initialize_weights</span>(<span class="params">L_in, L_out</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; 初始化网络层权重参数&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    W = np.zeros((L_out, <span class="number">1</span> + L_in))</span><br><span class="line">    <span class="comment"># ====================== 你的代码 ======================</span></span><br><span class="line">    <span class="comment">#epsilon_init = 0.12</span></span><br><span class="line">    epsilon_init = np.sqrt(<span class="number">6.0</span>) / np.sqrt(L_in + L_out)</span><br><span class="line">    print(<span class="string">&#x27;epsilon_init: &#x27;</span>, epsilon_init)</span><br><span class="line">    <span class="comment">#初始化网络层的权重参数</span></span><br><span class="line">    M, N = W.shape</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            W[i,j] = np.random.uniform(-epsilon_init, epsilon_init)</span><br><span class="line">    <span class="comment">#print(W)</span></span><br><span class="line">    <span class="comment"># ======================================================</span></span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug_initialize_weights</span>(<span class="params">fan_out, fan_in</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Initalize the weights of a layer with</span></span><br><span class="line"><span class="string">    fan_in incoming connections and</span></span><br><span class="line"><span class="string">    fan_out outgoing connection using a fixed strategy.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    W = np.linspace(<span class="number">1</span>, fan_out*(fan_in+<span class="number">1</span>), fan_out*(fan_in+<span class="number">1</span>))</span><br><span class="line">    W = <span class="number">0.1</span>*np.sin(W).reshape(fan_out, fan_in + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> W</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_numerical_gradient</span>(<span class="params">cost_func, theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the numerical gradient of the given cost_func</span></span><br><span class="line"><span class="string">    at parameter theta&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    numgrad = np.zeros_like(theta)</span><br><span class="line">    perturb = np.zeros_like(theta)</span><br><span class="line">    eps = <span class="number">1.0e-4</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(theta)):</span><br><span class="line">        perturb[idx] = eps</span><br><span class="line">        loss1 = cost_func(theta - perturb)</span><br><span class="line">        loss2 = cost_func(theta + perturb)</span><br><span class="line">        numgrad[idx] = (loss2 - loss1)/(<span class="number">2</span>*eps)</span><br><span class="line">        perturb[idx] = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">return</span> numgrad</span><br></pre></td></tr></table></figure>
<h2 id="检查梯度"><a href="#检查梯度" class="headerlink" title="检查梯度"></a>检查梯度</h2><p>在神经网络中，需要最小化代价函数<span> $J(\Theta)$ </span>。为了检查梯度计算是否正确，考虑把参数<span> $\Theta^{(1)}$ </span>和<span> $\Theta^{(2)}$ </span>展开为一个长的向量<span> $\theta$ </span>。假设函数<span> $f_i(\theta)$ </span>表示<span> $\frac{\partial}{\partial \theta_i} J(\theta)$ </span>。</p>
<p>令</p>
<script type="math/tex; mode=display">\theta^{(i+)} = \theta + \begin{bmatrix} 0 \\ 0 \\ \vdots \\ \epsilon \\ \vdots \\ 0 \end{bmatrix} \qquad
  \theta^{(i-)} = \theta - \begin{bmatrix} 0 \\ 0 \\ \vdots \\ \epsilon \\ \vdots \\ 0 \end{bmatrix}</script><p>上式中，<span> $\theta^{(i+)}$ </span>除了第<span> $i$ </span>个元素增加了<span> $\epsilon$ </span>之 外，其他元素均与<span> $\theta$ </span>相同。类似的，<span> $\theta^{(i-)}$ </span>中仅第<span> $i$ </span>个元素减少了<span> $\epsilon$ </span>。可以使用数值近似验证<span> $f_i(\theta)$ </span>计算是否正确：</p>
<script type="math/tex; mode=display">f_i(\theta) \approx \frac{J(\theta^{(i+)}) - J(\theta^{(i-)})}{2\epsilon}</script><p>如果设<span> $\epsilon=10^{-4}$ </span>，通常上式左右两端的差异出现于第4位有效数字之后（经常会有更高的精度）。</p>
<p>在练习的程序代码中，函数 <code>compute_numerical_gradient</code> 已经实现，建议你认真阅读该函数并理解其实现原理与方案。</p>
<p>之后，程序将执行 <code>check_nn_gradients</code> 函数。该函数将创建一个较小的神经网络用于检测你的误差反传训练算法所计算得到的梯度是否正确。如果你的实现是正确的，你得到的 梯度与数值梯度之后的绝对误差（各分量的绝对值差之和）应当小于<span> $10^{-9}$ </span>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_nn_gradients</span>(<span class="params">lmb=<span class="number">0.0</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Creates a small neural network to check the backgropagation</span></span><br><span class="line"><span class="string">    gradients.&quot;&quot;&quot;</span></span><br><span class="line">    input_layer_size, hidden_layer_size = <span class="number">3</span>, <span class="number">5</span></span><br><span class="line">    num_labels, m = <span class="number">3</span>, <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    Theta1 = debug_initialize_weights(hidden_layer_size, input_layer_size)</span><br><span class="line">    Theta2 = debug_initialize_weights(num_labels, hidden_layer_size)</span><br><span class="line"></span><br><span class="line">    X = debug_initialize_weights(m, input_layer_size - <span class="number">1</span>)</span><br><span class="line">    y = np.array([<span class="number">1</span> + (t % num_labels) <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(m)])</span><br><span class="line">    nn_params = np.hstack((Theta1.flatten(), Theta2.flatten()))</span><br><span class="line"></span><br><span class="line">    cost_func = <span class="keyword">lambda</span> x: nn_cost_function(x,</span><br><span class="line">                                           input_layer_size,</span><br><span class="line">                                           hidden_layer_size,</span><br><span class="line">                                           num_labels, lmb, X, y)</span><br><span class="line">    grad = nn_grad_function(nn_params,</span><br><span class="line">                            input_layer_size, hidden_layer_size,</span><br><span class="line">                            num_labels, lmb, X, y)</span><br><span class="line">    numgrad = compute_numerical_gradient(cost_func, nn_params)</span><br><span class="line">    print(np.vstack((numgrad, grad)).T, np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(numgrad - grad)))</span><br><span class="line">    print(<span class="string">&#x27;The above two columns you get should be very similar.&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;(Left-Your Numerical Gradient, Right-Analytical Gradient)&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">Theta1, Theta2, X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;模型预测&quot;&quot;&quot;</span></span><br><span class="line">   </span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># num_labels = Theta2.shape[0]</span></span><br><span class="line"></span><br><span class="line">    p = np.zeros((m,<span class="number">1</span>), dtype=<span class="built_in">int</span>)</span><br><span class="line">    <span class="comment"># ====================== 你的代码============================</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 神经网络模型预测</span></span><br><span class="line">    </span><br><span class="line">    a_1 = np.hstack((np.ones((m, <span class="number">1</span>)), X))     <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_2 = np.dot(Theta1, a_1.T) </span><br><span class="line">    a_2 = sigmoid(Z_2)</span><br><span class="line">    a_2 = a_2.T</span><br><span class="line">    a_2 = np.hstack((np.ones((m, <span class="number">1</span>)), a_2)) <span class="comment"># Add bias as 1</span></span><br><span class="line"></span><br><span class="line">    Z_3 = np.dot(Theta2, a_2.T)</span><br><span class="line">    hypothesis = sigmoid(Z_3)</span><br><span class="line">    hypothesis = hypothesis.T</span><br><span class="line"></span><br><span class="line">    one_hot_to_val = np.argmax(hypothesis, axis=<span class="number">1</span>) + <span class="number">1.0</span></span><br><span class="line">    p = one_hot_to_val.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    ok = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">if</span> p[i] == y[i]:</span><br><span class="line">            ok = ok + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&quot;ok&quot;</span>,ok)</span><br><span class="line"></span><br><span class="line">    acc_rate = ok * <span class="number">1.0</span> / m</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ============================================================</span></span><br><span class="line">    <span class="keyword">return</span> acc_rate</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">input_layer_size = <span class="number">400</span>          <span class="comment"># 20x20 大小的输入图像，图像内容为手写数字</span></span><br><span class="line">hidden_layer_size = <span class="number">25</span>          <span class="comment"># 25 hidden units</span></span><br><span class="line">num_labels = <span class="number">10</span>                 <span class="comment"># 10 类标号 从1到10</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =========== 第一部分 ===============</span></span><br><span class="line"><span class="comment"># 加载训练数据</span></span><br><span class="line">print(<span class="string">&quot;Loading and Visualizing Data...&quot;</span>)</span><br><span class="line">data = sio.loadmat(<span class="string">&#x27;data/NN_data.mat&#x27;</span>)</span><br><span class="line">X, y = data[<span class="string">&#x27;X&#x27;</span>], data[<span class="string">&#x27;y&#x27;</span>]</span><br><span class="line"></span><br><span class="line">m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机选取100个数据显示</span></span><br><span class="line">rand_indices = np.array(<span class="built_in">range</span>(m))</span><br><span class="line">np.random.shuffle(rand_indices)</span><br><span class="line">X_sel = X[rand_indices[:<span class="number">100</span>]]</span><br><span class="line"></span><br><span class="line">display_data(X_sel)</span><br></pre></td></tr></table></figure>
<pre><code>Loading and Visualizing Data...
</code></pre><p><img src="/img/PRML/output_23_1.png" alt="png"></p>
<h2 id="加载神经网络模型的权重"><a href="#加载神经网络模型的权重" class="headerlink" title="加载神经网络模型的权重"></a>加载神经网络模型的权重</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =========== 第二部分 ===============</span></span><br><span class="line">print(<span class="string">&#x27;Loading Saved Neural Network Parameters ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the weights into variables Theta1 and Theta2</span></span><br><span class="line">data = sio.loadmat(<span class="string">&#x27;data/NN_weights.mat&#x27;</span>)</span><br><span class="line">Theta1, Theta2 = data[<span class="string">&#x27;Theta1&#x27;</span>], data[<span class="string">&#x27;Theta2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># print Theta1.shape, (hidden_layer_size, input_layer_size + 1)</span></span><br><span class="line"><span class="comment"># print Theta2.shape, (num_labels, hidden_layer_size + 1)</span></span><br></pre></td></tr></table></figure>
<pre><code>Loading Saved Neural Network Parameters ...
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ================ Part 3: Compute Cost (Feedforward) ================</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;\nFeedforward Using Neural Network ...&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight regularization parameter (we set this to 0 here).</span></span><br><span class="line">lmb = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">nn_params = np.hstack((Theta1.flatten(), Theta2.flatten()))</span><br><span class="line">J = nn_cost_function(nn_params,</span><br><span class="line">                     input_layer_size, hidden_layer_size,</span><br><span class="line">                     num_labels, lmb, X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Cost at parameters (loaded from PRML_NN_weights): %f &#x27;</span> % J)</span><br><span class="line">print(<span class="string">&#x27;(this value should be about 0.287629)&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Feedforward Using Neural Network ...
Cost at parameters (loaded from PRML_NN_weights): 0.287629 
(this value should be about 0.287629)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =============== Part 4: Implement Regularization ===============</span></span><br><span class="line">print(<span class="string">&#x27;Checking Cost Function (w/ Regularization) ... &#x27;</span>)</span><br><span class="line">lmb = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">J = nn_cost_function(nn_params,</span><br><span class="line">                     input_layer_size, hidden_layer_size,</span><br><span class="line">                     num_labels, lmb, X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Cost at parameters (loaded from PRML_NN_weights): %f &#x27;</span> % J)</span><br><span class="line">print(<span class="string">&#x27;(this value should be about 0.383770)&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from PRML_NN_weights): 0.383770 
(this value should be about 0.383770)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ================ Part 5: Sigmoid Gradient  ================</span></span><br><span class="line">print(<span class="string">&#x27;Evaluating sigmoid gradient...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">g = sigmoid_gradient([<span class="number">1</span>, -<span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">&#x27;Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:  &#x27;</span>, g)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:   [0.19661193 0.23500371 0.25       0.23500371 0.19661193]
</code></pre><h2 id="神经网络参数初始化"><a href="#神经网络参数初始化" class="headerlink" title="神经网络参数初始化"></a>神经网络参数初始化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  ================ Part 6: Initializing Pameters ================</span></span><br><span class="line">print(<span class="string">&#x27;Initializing Neural Network Parameters ...&#x27;</span>)</span><br><span class="line">initial_Theta1 = rand_initialize_weights(input_layer_size, hidden_layer_size)</span><br><span class="line">initial_Theta2 = rand_initialize_weights(hidden_layer_size, num_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Unroll parameters</span></span><br><span class="line">initial_nn_params = np.hstack((initial_Theta1.flatten(),</span><br><span class="line">                               initial_Theta2.flatten()))</span><br></pre></td></tr></table></figure>
<pre><code>Initializing Neural Network Parameters ...
epsilon_init:  0.1188177051572009
epsilon_init:  0.4140393356054125
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># =============== Part 7: Implement Backpropagation ===============</span></span><br><span class="line">print(<span class="string">&#x27;Checking Backpropagation... &#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check gradients by running checkNNGradients</span></span><br><span class="line">check_nn_gradients()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Checking Backpropagation... 
[[ 1.27220311e-02  1.27220311e-02]
 [ 1.58832809e-04  1.58832809e-04]
 [ 2.17690452e-04  2.17690455e-04]
 [ 7.64045027e-05  7.64045009e-05]
 [ 6.46352264e-03  6.46352265e-03]
 [ 2.34983744e-05  2.34983735e-05]
 [-3.74199116e-05 -3.74199098e-05]
 [-6.39345021e-05 -6.39345006e-05]
 [-5.74199923e-03 -5.74199923e-03]
 [-1.34052016e-04 -1.34052019e-04]
 [-2.59146269e-04 -2.59146269e-04]
 [-1.45982635e-04 -1.45982634e-04]
 [-1.26792390e-02 -1.26792390e-02]
 [-1.67913183e-04 -1.67913187e-04]
 [-2.41809017e-04 -2.41809017e-04]
 [-9.33867517e-05 -9.33867522e-05]
 [-7.94573534e-03 -7.94573535e-03]
 [-4.76254503e-05 -4.76254501e-05]
 [-2.64923861e-06 -2.64923844e-06]
 [ 4.47626736e-05  4.47626708e-05]
 [ 1.09347722e-01  1.09347722e-01]
 [ 5.67965185e-02  5.67965185e-02]
 [ 5.25298306e-02  5.25298306e-02]
 [ 5.53542907e-02  5.53542907e-02]
 [ 5.59290833e-02  5.59290833e-02]
 [ 5.23534682e-02  5.23534682e-02]
 [ 1.08133003e-01  1.08133003e-01]
 [ 5.67319602e-02  5.67319602e-02]
 [ 5.14442931e-02  5.14442931e-02]
 [ 5.48296085e-02  5.48296085e-02]
 [ 5.56926532e-02  5.56926532e-02]
 [ 5.11795651e-02  5.11795651e-02]
 [ 3.06270372e-01  3.06270372e-01]
 [ 1.59463135e-01  1.59463135e-01]
 [ 1.45570264e-01  1.45570264e-01]
 [ 1.56700533e-01  1.56700533e-01]
 [ 1.56043968e-01  1.56043968e-01]
 [ 1.45771544e-01  1.45771544e-01]] 9.96691174908528e-11
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =============== Part 8: Implement Regularization ===============</span></span><br><span class="line">print(<span class="string">&#x27;Checking Backpropagation (w/ Regularization) ... &#x27;</span>)</span><br><span class="line"><span class="comment"># Check gradients by running checkNNGradients</span></span><br><span class="line">lmb = <span class="number">3.0</span></span><br><span class="line">check_nn_gradients(lmb)</span><br></pre></td></tr></table></figure>
<pre><code>Checking Backpropagation (w/ Regularization) ... 
[[ 0.01272203  0.06321029]
 [ 0.05471668  0.05471668]
 [ 0.00868489  0.00868489]
 [-0.04533175 -0.04533175]
 [ 0.00646352 -0.05107193]
 [-0.01674143 -0.01674143]
 [ 0.03938178  0.03938178]
 [ 0.05929756  0.05929756]
 [-0.005742    0.01898511]
 [-0.03277532 -0.03277532]
 [-0.06025856 -0.06025856]
 [-0.03234036 -0.03234036]
 [-0.01267924  0.01253078]
 [ 0.05926853  0.05926853]
 [ 0.03877546  0.03877546]
 [-0.01736759 -0.01736759]
 [-0.00794574 -0.06562958]
 [-0.04510686 -0.04510686]
 [ 0.00898998  0.00898998]
 [ 0.05482148  0.05482148]
 [ 0.10934772  0.15983598]
 [ 0.11135436  0.11135436]
 [ 0.06099703  0.06099703]
 [ 0.00994614  0.00994614]
 [-0.00160637 -0.00160637]
 [ 0.03558854  0.03558854]
 [ 0.108133    0.1475522 ]
 [ 0.11609346  0.11609346]
 [ 0.0761714   0.0761714 ]
 [ 0.02218834  0.02218834]
 [-0.00430676 -0.00430676]
 [ 0.01898519  0.01898519]
 [ 0.30627037  0.33148039]
 [ 0.21889958  0.21889958]
 [ 0.18458753  0.18458753]
 [ 0.13942633  0.13942633]
 [ 0.09836012  0.09836012]
 [ 0.10071231  0.10071231]] 0.33076217369064975
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)
</code></pre><h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># =================== Part 8: Training NN ===================</span></span><br><span class="line">print(<span class="string">&#x27;Training Neural Network...&#x27;</span>)</span><br><span class="line"></span><br><span class="line">lmb, maxiter = <span class="number">1.0</span>, <span class="number">50</span></span><br><span class="line">args = (input_layer_size, hidden_layer_size, num_labels, lmb, X, y)</span><br><span class="line">nn_params, cost_min, _, _, _ = fmin_cg(nn_cost_function,</span><br><span class="line">                                       initial_nn_params,</span><br><span class="line">                                       fprime=nn_grad_function,</span><br><span class="line">                                       args=args,</span><br><span class="line">                                       maxiter=maxiter,</span><br><span class="line">                                       full_output=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">Theta1 = nn_params[:hidden_layer_size*(input_layer_size + <span class="number">1</span>)]</span><br><span class="line">Theta1 = Theta1.reshape((hidden_layer_size, input_layer_size + <span class="number">1</span>))</span><br><span class="line">Theta2 = nn_params[hidden_layer_size*(input_layer_size + <span class="number">1</span>):]</span><br><span class="line">Theta2 = Theta2.reshape((num_labels, hidden_layer_size + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Training Neural Network...
Warning: Maximum number of iterations has been exceeded.
         Current function value: 0.449704
         Iterations: 50
         Function evaluations: 99
         Gradient evaluations: 99
</code></pre><h2 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ================= Part 9: Implement Predict =================</span></span><br><span class="line"></span><br><span class="line">pred = predict(Theta1, Theta2, X)</span><br><span class="line"><span class="comment"># print(pred.shape, y.shape)</span></span><br><span class="line"><span class="comment"># print(np.hstack((pred, y)))</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Training Set Accuracy:&#x27;</span>, pred)</span><br></pre></td></tr></table></figure>
<pre><code>ok 4796
Training Set Accuracy: 0.9592
</code></pre>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/34.logistic/" rel="prev" title="逻辑回归">
                  <i class="fa fa-chevron-left"></i> 逻辑回归
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fab fa-github"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">朋克李PunkLi</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">163k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:28</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  

<script src="/js/local-search.js"></script>






  





  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
